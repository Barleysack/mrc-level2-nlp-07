{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# Passage Retrieval 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm9gQPjOWKki"
   },
   "source": [
    "이번 과제에서는 3강에서 배운 **Sparse Passage Retrieval** 과 4강에서 배운 **Dense Passage Retrieval (DPR)** 을 구현해봅니다. \n",
    "\n",
    "Passage Retrieval 을 다시 복습해보면,\n",
    "1. Query와 Passage 를 임베딩 시킨 후\n",
    "2. 임베딩된 벡터들에 각각 dot product를 수행하여 유사도를 구한 후에\n",
    "3. 유사도가 가장 높은 passage 들을 검색 대상으로 합니다.   \n",
    "\n",
    "이 때 임베딩 시키는 방법에서 Sparse 와 Dense 가 나누어진 점, 다들 기억하시죠?\n",
    "차근차근 구현해본 후, 전체 Wikipedia 에 대해서도 작업해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ug7dthenVCR"
   },
   "source": [
    "```\n",
    "🛠 Setup을 하는 부분입니다. 이전 과제에서 반복되는 부분이기 때문에 무지성 실행 하셔도 좋습니다.\n",
    "💻 실습 코드입니다. 따라가면서 코드를 이해해보세요.\n",
    "❓ 과제입니다. 주어진 질문과 요구사항에 맞춰서 직접 코드를 짜보세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKIvUE3FcdK6"
   },
   "source": [
    "## 🛠 초기설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### 🛠 Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "0331be65-340d-4196-ebcd-a804705740d9"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm==4.48.0 -q\n",
    "!pip install datasets==1.4.1 -q\n",
    "!pip install transformers==4.5.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHf4ReA9dzyp"
   },
   "source": [
    "### 🛠 난수 고정 및 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:13.948236Z",
     "start_time": "2021-09-15T08:06:12.672812Z"
    },
    "id": "fNEhsdR6hM-6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:13.978267Z",
     "start_time": "2021-09-15T08:06:13.951238Z"
    },
    "id": "sSyEXp19d0L1"
   },
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:14.023236Z",
     "start_time": "2021-09-15T08:06:13.979238Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsayd5Ite-KY",
    "outputId": "0a958d6e-91a9-4699-a3c6-1de4706615fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"% (torch.__version__))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "### 🛠 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD 의 train 데이터를 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:46.968793Z",
     "start_time": "2021-09-15T06:52:28.974382Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "ddb9baa5e79c4e5dbfc4cc2a23e79987",
      "7b64d95da47f46c4800e0bba13283693",
      "cd22331b8ece475080b0b102be3522eb",
      "9a28b6e8626841f2a23fd6f31160a2f7",
      "d863b911b833435dbb32db1761884539",
      "5357ac4465374fc79ec8dd5a9c532a49",
      "b973d350bcfd4d75b3c4235b1624f798",
      "0f36e44064244c2daf1a9fdeee5e7fe4",
      "69b6b6a01c034bf78478509a09ac3513",
      "9da62ff244874ecb94b771df40dd7b64",
      "cc2727ff9c2d401a8008e617ba9ad747",
      "fb079b9a51a248d9ad0de418ab0476c8",
      "cd1c037bd10a4894a4fc213294dc3a13",
      "ff74f9f5d22b498aaf8f6c3d41c5270c",
      "9abf651d6f80423ea448ab06acfac005",
      "b17c04a5d62542f1b51ed9a0b32e77bf",
      "2553f14880194fe3b1a4a5c611de59f3",
      "9d5de35252fd4404a474054f999e950b",
      "925386f42c2b40a09990ade0181559ba",
      "03d20af78d7c44159abca802ac082d1a",
      "e9e10b47884943488ce8fdbaf2f1e9b9",
      "46d61012f6ac4ca189d8890826a27fe5",
      "1f2a869e4b404e68b954eb384ef62d68",
      "0d9e100657ac4554974e990951e43387"
     ]
    },
    "id": "4IUxepuj_VCv",
    "outputId": "ee0defb1-06a9-4666-f3e6-ef728a63a04d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 9606개의 지문이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_kor_v1\")\n",
    "corpus = list(set([example[\"context\"] for example in dataset[\"train\"]]))\n",
    "print(f\"총 {len(corpus)}개의 지문이 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "### 🛠 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT 를 encoder 로 사용하므로, KLUE에서 제공하는 `klue/bert-base` tokenizer 를 활용해봅시다. 다른 pretrained 모델을 사용하고 싶으시다면, `model_checkpoint`를 바꿔보세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.152836Z",
     "start_time": "2021-09-15T06:52:46.970795Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213,
     "referenced_widgets": [
      "697798699f354b3bb87a4265f545dcbf",
      "9ebc6cb8cbdc4675869035f04166b9d7",
      "f85b679b2add466ba5230481ffcfa3ab",
      "4c832fc7a4ca4965bd37b68a1ae2c736",
      "48f64c7b8d164ce7ac7efb8ae7aaaffe",
      "1111e96e637447439c90016ef4184ba7",
      "49e6ffd8000644eda4157dcb48829984",
      "0be86d05d64240b887902fb7624c474d",
      "05a482882c4f4e1b9b85150a3eed2293",
      "5bfdfadc746c4a2ca44af7da3bfc4ab9",
      "ec3ad8119f13412f8a7b8b00bf275b8b",
      "a2daef726e6e48219e7c0d9c62ac962c",
      "7260a5df06ff44a18e4890109b9b0be9",
      "493174df299e451c95241a6fa0f21b59",
      "113b4c8b0e264313a5abfc5bc3fe85df",
      "3d4b7f7914fb42f39c364a92538eea2e"
     ]
    },
    "id": "AoB8BHGDmVIK",
    "outputId": "f088e681-4a67-489e-e052-597640fffe86"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X48czRQwcxPu"
   },
   "source": [
    "불러온 Tokenzier가 잘 작동하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.182781Z",
     "start_time": "2021-09-15T06:52:56.153749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0U7sn3jsu44O",
    "outputId": "ac54c22a-518c-4536-ef7c-176ca02c260c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. '\n",
      " '이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 [UNK] 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 '\n",
      " '심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 '\n",
      " '받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 '\n",
      " '라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 '\n",
      " '받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 '\n",
      " '완성과 동시에 그는 이 서곡 ( 1악장 ) 을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. '\n",
      " '결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 '\n",
      " '방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 '\n",
      " '의견도 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(\n",
    "    dataset[\"train\"][0][\"context\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "pprint(tokenizer.decode(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGPpy1Hpd_7m"
   },
   "source": [
    "## 💻 Ⅰ. Sparse Retriever 실습\n",
    "첫 번째로 TF-IDF 를 통해 임베딩 벡터를 만들어봅시다. 이 모듈은 직접 구현할 필요 없이 `sklearn.feature_extract.text` 에서 구현된 것을 사용합시다!\n",
    "\n",
    "더 간단하게 임베딩 벡터를 구할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCeRbHPvh8UB"
   },
   "source": [
    "### 💻 1. TF-IDF 학습하기\n",
    "TF-IDF 사용법은 sklearn 홈페이지에서 확인할 수 있습니다. 제공된 링크를 읽어보시면, 아래에 작성된 코드를 쉽게 이해하실 수 있을 거에요.\n",
    "\n",
    "\n",
    "*   [TF-IDF 공식 홈페이지](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "*   [TF-IDF 사용 예시](https://wikidocs.net/31698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.197781Z",
     "start_time": "2021-09-15T06:52:56.183749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSQWisY2fUgV",
    "outputId": "74446c3a-eb8e-464b-888b-8c0590e0b400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ 기존 문장 ------------------------------\n",
      "('초대 교회 시절에 새 로마 주교는 로마 관구에 속한 다른 주교들이 참석한 가운데 로마 교구 성직자들과 신자들에 의해 특정인을 투표로 '\n",
      " '선출하거나 지명하는 방식으로 착좌하였다. 이는 당시 다른 교구들에서도 자신들의 새 주교를 뽑을 때 관례적으로 사용한 방식이었다. 작은 '\n",
      " '공동체에 불과했던 교회가 박해 시대에서 살아남는데는 이러한 단순한 방법이 잘 통하였지만, 점차 규모가 커져감에 따라 새 로마 주교를 뽑는 '\n",
      " '거수 투표 방식은 경쟁상대 간의 극한 대립을 초래하여 파벌이 생기는 사태를 초래하였으며, 이는 다른 주교 선거에도 영향을 끼쳤다. 더불어 '\n",
      " '4세기에는 새로 선출된 로마 주교들이 자신의 주교 착좌 승인을 받기 위해 로마 황제들에게 찾아갔는데, 이는 이따금씩 교회 안 문제를 '\n",
      " '국가가 간섭하는 결과를 낳기도 하였다.')\n",
      "\n",
      "------------------------------ Tokenize 된 문장 ------------------------------\n",
      "('초대 교회 시절 ##에 새 로마 주교 ##는 로마 관 ##구 ##에 속한 다른 주교 ##들이 참석 ##한 가운데 로마 교구 성직자 ##들 '\n",
      " '##과 신자 ##들 ##에 의해 특정인 ##을 투표 ##로 선출 ##하 ##거나 지명 ##하 ##는 방식 ##으로 착 ##좌 ##하 '\n",
      " '##였 ##다 . 이 ##는 당시 다른 교구 ##들 ##에서 ##도 자신 ##들 ##의 새 주교 ##를 뽑 ##을 때 관례 ##적으로 '\n",
      " '사용 ##한 방식 ##이 ##었 ##다 . 작 ##은 공동체 ##에 불과 ##했 ##던 교회 ##가 박해 시대 ##에서 살아남 ##는데 '\n",
      " '##는 이러 ##한 단순 ##한 방법 ##이 잘 통하 ##였 ##지만 , 점차 규모 ##가 커져 ##감 ##에 따라 새 로마 주교 ##를 '\n",
      " '뽑 ##는 거 ##수 투표 방식 ##은 경쟁 ##상 ##대 간 ##의 극한 대립 ##을 초래 ##하여 파벌 ##이 생기 ##는 사태 '\n",
      " '##를 초래 ##하 ##였 ##으며 , 이 ##는 다른 주교 선거 ##에도 영향 ##을 끼쳤 ##다 . 더불 ##어 4 ##세기 ##에 '\n",
      " '##는 새로 선출 ##된 로마 주교 ##들이 자신 ##의 주교 착 ##좌 승인 ##을 받 ##기 위해 로마 황제 ##들 ##에 ##게 '\n",
      " '찾아갔 ##는데 , 이 ##는 이따금 ##씩 교회 안 문제 ##를 국가 ##가 간섭 ##하 ##는 결과 ##를 낳 ##기 ##도 하 '\n",
      " '##였 ##다 .')\n"
     ]
    }
   ],
   "source": [
    "# Huggingface의 Tokenizer를 사용하셔도 좋고\n",
    "tokenizer_func = lambda x: tokenizer.tokenize(x)\n",
    "\n",
    "# 혹은 단순 띄어쓰기 기준으로 Tokenize 하셔도 좋습니다.\n",
    "# tokenizer_func = lambda x: x.split(' ')\n",
    "\n",
    "# 어떻게 Tokenize 되었는지 확인해봅시다.\n",
    "print(f\"{'-'*30} 기존 문장 {'-'*30}\")\n",
    "pprint(corpus[20])\n",
    "print(f\"\\n{'-'*30} Tokenize 된 문장 {'-'*30}\")\n",
    "pprint(\" \".join(tokenizer_func(corpus[20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LZR8COph2g9"
   },
   "source": [
    "모듈을 활용해서 `fit` 해봅시다. 과정은 어렵지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:20.560497Z",
     "start_time": "2021-09-15T06:52:56.198781Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyQ290Cdd64F",
    "outputId": "eaea66d8-78d9-4186-8e62-2782b03bf210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenizer_func,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "vectorizer.fit(corpus)\n",
    "sp_matrix = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4sPODsJ76xm"
   },
   "source": [
    "`Vectorizer`로 임베딩을 시켜주면 (문서의 개수, 단어의 개수) 꼴의 행렬로 변환이 됩니다. 기본적으로 단어의 개수는 지정해주지 않으면 전체 단어의 개수만큼 차원이 지정됩니다. 사용되는 단어의 개수가 너무 많아서 지나치게 행렬이 희소해지면 사용이 불편하기 때문에, 필요하다면 벡터 임베딩 사이즈 또한 지정해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4nh6x8o9KCP"
   },
   "source": [
    "참고로 결과물이 희소행렬이기 때문에 평소에 사용되는 `numpy.ndarray`가 아닙니다. Scipy 모듈의 csr_matrix를 이용하는데 이는 아래 링크를 참고해주세요\n",
    "+ [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "+ [Scipy sparse matrix handling](https://lovit.github.io/nlp/machine%20learning/2018/04/09/sparse_mtarix_handling/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBuC_c-e9PHF",
    "outputId": "ff929cb4-9595-4cb4-fef0-43bfe60a77d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:20.575499Z",
     "start_time": "2021-09-15T06:53:20.562498Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxHJP5HHfxFm",
    "outputId": "366403a1-2cab-4b88-eb1d-b780d99e352f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9606, 684272)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_matrix.shape # (num_passage, num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxSvTRPFiH0Z"
   },
   "source": [
    "첫 번째 문장의 TF-IDF 벡터를 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.609083Z",
     "start_time": "2021-09-15T06:53:20.577499Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nf0fNy11fzXY",
    "outputId": "0ed450ad-e87e-43de-cda3-0e812a7dd863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF-IDF\n",
      "그        0.153808\n",
      "고비       0.118545\n",
      "##어 .    0.118545\n",
      "##째 고비   0.105799\n",
      "구조 대원    0.105799\n",
      "##였 ##어  0.105799\n",
      "응급 구조    0.105799\n",
      "그 ##는    0.102659\n",
      "그 때      0.102603\n",
      "고비 ##는   0.101273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    sp_matrix[0].T.todense(),\n",
    "    index=vectorizer.get_feature_names(),\n",
    "    columns=[\"TF-IDF\"]\n",
    ")\n",
    "df = df.sort_values(\"TF-IDF\", ascending=False)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgjN_SF7iM_N"
   },
   "source": [
    "### 💻 2. Query 임베딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEdiT1W78X5y"
   },
   "source": [
    "이제 Query 를 임베딩해봅시다. 아까 사용한 `vectorizer`를 이용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.624082Z",
     "start_time": "2021-09-15T06:53:21.611084Z"
    },
    "id": "sXSOfYisf5ad"
   },
   "outputs": [],
   "source": [
    "sample_idx = random.choice(range(len(dataset[\"train\"])))\n",
    "\n",
    "query = dataset[\"train\"][sample_idx][\"question\"]\n",
    "ground_truth = dataset[\"train\"][sample_idx][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.654082Z",
     "start_time": "2021-09-15T06:53:21.626084Z"
    },
    "id": "dw0T5Drrf658"
   },
   "outputs": [],
   "source": [
    "query_vec = vectorizer.transform([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXpNKTYSia9D"
   },
   "source": [
    "### 💻 3. Dot Product 를 통해 유사도 구하기\n",
    "내적을 통해 주어진 Query 와 전체 Passage 사이의 유사도를 구해봅시다.\n",
    "그리고 값을 내림차순으로 나열하여 높은 점수를 가진 Passage 들을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:25.265288Z",
     "start_time": "2021-09-15T06:54:25.046289Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLsck2Lyf_Vm",
    "outputId": "3e940842-bc2e-4023-aaa1-cc6645d58468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9606)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = query_vec * sp_matrix.T\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:26.190604Z",
     "start_time": "2021-09-15T06:54:26.172592Z"
    },
    "id": "kLRrldUef8Cw"
   },
   "outputs": [],
   "source": [
    "sorted_result = np.argsort(-result.data)\n",
    "doc_scores = result.data[sorted_result]\n",
    "doc_ids = result.indices[sorted_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:26.555129Z",
     "start_time": "2021-09-15T06:54:26.541098Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDYgaL3wgBC6",
    "outputId": "bc810b50-e352-4391-da83-cc29b7b4dc88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.21208908, 0.07221365, 0.0677442 , 0.05575468, 0.05302485]),\n",
       " array([6654, 1344, 2489, 3216,  863], dtype=int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "doc_scores[:k], doc_ids[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pin40NMV91-l"
   },
   "source": [
    "잘 뽑았는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:27.343209Z",
     "start_time": "2021-09-15T06:54:27.333696Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e-OSr2tgCGt",
    "outputId": "b018af21-6206-4404-ed18-867515da9686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 구식 군인들의 월급인 쌀에 모래와 돌멩이가 들어가있던 사건을 말미암아 일어난 사태의 이름은? \n",
      "\n",
      "[Ground truth passage]\n",
      "1882년 6월 민영익의 귀국 권고로 일시 귀국했다가 다시 되돌아갔다. 7월 23일 한성부에서 구식 군인들의 월급으로 주는 쌀에 모래와 돌멩이 및 썩은 쌀을 주자 여기에 반발한 구식 군인들에 의해 임오군란이 일어났는데, 그는 임오군란 사태 당시 행동을 삼가고 사태의 추이를 예의주시하고 있었다. 그러다가 그해 10월 13일 박영효, 김옥균 일행이 수신사(修信使) 겸 사죄사(辭罪使)로 하는 사절단(使節團)이 파견되자, 그는 박영효와 김옥균 일행이 도쿄에 방문했을 때 사절단의 통역을 맡아보았다. 수신사는 3개월간 일본의 각 기관을 시찰하고 일본의 여·야당 정치 지도자들과 만나 면담하고 각국 사절과도 폭넓게 접촉하여 의견을 교환했다. 한학을 배워 중국어와 일본어의 기초 실력이 있었던 그는 사절단의 통역을 맡아 활약했으며, 박영효 등이 귀국할 때 일본 유학을 마치고 박영효 일행과 함께 귀국했다. \n",
      "\n",
      "Top-1 passage with score 0.2121\n",
      "1882년 6월 민영익의 귀국 권고로 일시 귀국했다가 다시 되돌아갔다. 7월 23일 한성부에서 구식 군인들의 월급으로 주는 쌀에 모래와 돌멩이 및 썩은 쌀을 주자 여기에 반발한 구식 군인들에 의해 임오군란이 일어났는데, 그는 임오군란 사태 당시 행동을 삼가고 사태의 추이를 예의주시하고 있었다. 그러다가 그해 10월 13일 박영효, 김옥균 일행이 수신사(修信使) 겸 사죄사(辭罪使)로 하는 사절단(使節團)이 파견되자, 그는 박영효와 김옥균 일행이 도쿄에 방문했을 때 사절단의 통역을 맡아보았다. 수신사는 3개월간 일본의 각 기관을 시찰하고 일본의 여·야당 정치 지도자들과 만나 면담하고 각국 사절과도 폭넓게 접촉하여 의견을 교환했다. 한학을 배워 중국어와 일본어의 기초 실력이 있었던 그는 사절단의 통역을 맡아 활약했으며, 박영효 등이 귀국할 때 일본 유학을 마치고 박영효 일행과 함께 귀국했다. \n",
      "\n",
      "Top-2 passage with score 0.0722\n",
      "당시 백신시장에 개인용 백신은 무료배포가 많았던 시기라 개인사용자들에 한해서 1989년 부터 도스용 백신 소프트웨어인 V3+ 네오라는 백신을 무료로 제공하였다. 이후 V3+ 네오는 시그니처 수의 증가로 인해 당시 가장 보편적인 저장매체인 3.5인치 디스켓 2장이 필요하는 등 실제 이용이 매우 어렵고 제한되게 되었다. 결국 한동안 무료백신에 크게 신경을 쓰지 않던 안연구소는 알약 등의 경쟁 무료 제품의 확산을 막기 위해 빛자루 제품을 유료에서 무료로 바꾸고 V3 라이트라는 무료 제품을 연이어 출시한다. V3+ 네오는 V3 라이트 출시 이후 단종되었다. 안철수는 사업에 대해서 잘 모르는 상태에서 시작했기 때문에 처음 4년 간은 많은 고생을 했다. 당시 안철수연구소의 월급날은 매월 25일이었는데 월초부터 직원들의 월급 걱정을 해야 하는 지경이었고 자신이 월급을 받지 않고 직원들의 월급을 줄 때도 있었다. \n",
      "\n",
      "Top-3 passage with score 0.0677\n",
      "사구(砂丘)는 바람에 의하여 모래가 이동하여 퇴적된 언덕이나 둑 모양의 모래 언덕이다. 내륙 사구는 고비 사막이나 사하라 사막과 같이 대륙 내부의 사막에 흔히 이루어진다. 사구는 한 장소에 고정되지 않고 독특한 모양을 유지하면서 바람이 부는 쪽으로 이동하는 경우가 많다. 사구 사이사이에는 기반암이나 자갈층이 드러난 경우도 있고, 넓은 지면이 모두 사구로 덮인 경우도 있다. 장애물이 바람에 가로놓여 있으면 바람그늘 쪽에는 풍속이 줄어들어 모래가 잘 쌓인다. 모래알이 장애물의 바람그늘 쪽에 쌓인 모래 위로 떨어지면 이동 속도가 줄어들어 모래가 계속 해서 집적하게 된다. 모래 더미가 원래의 장애물에 비하여 너무 크게 성장하면 다시 천천히 움직이면서 이동성 사구로 발전한다.한편, 해안 사구는 바닷물의 물결을 따라 바닷가에 밀려온 모래가 사빈으로 퇴적되었다가 다시 바다로부터 불어오는 바람에 실려가 사빈의 뒷쪽에 쌓여 생긴 것으로, 대개 해안선과 나란히 생긴다. 모래가 육지 쪽으로 너무 많이 날려가면 농경지가 묻히기 때문에, 이와 같은 해안의 주민들은 방풍림이나 방사림을 조성하여 모래의 이동을 막고 있다. \n",
      "\n",
      "Top-4 passage with score 0.0558\n",
      "그리고 이러한 만민공동회 활동과 관련하여, 독립협회에서 만민공동회에 사주하여 결의한 헌의 6조에 배치되는 활동이 자주 나타났고, 그에 따라 황실에서는 독립협회를 탄압하려고 했다. 이러한 독립협회의 활동이 일본이나 미국에 유리한 조건을 조성해 주었고, 오히려 황실에서 추진하는 광무개혁이 일본이나 미국이 추진하는 경제적 침탈에 불리하게 작용하자, 그들은 황실의 만민공동회 탄압 계획을 번번이 저지하였다. 그들은 또한 독립협회 활동의 폭력화로 말미암아 무정부상태가 되는 일도 바라지 않았다. 그들이 생각하는 최악의 사태는 무정부상태로 말미암아 고종이 제2의 아관파천을 하는 일이었다. 미국과 일본은 그것을 막기 위해 독립협회에 대한 탄압은 억제하면서, 박영효 세력을 저지하는 한편 무력 진압을 양해하였다. 그 결과 독립협회의 활동으로 말미암아 오히려 외세 의존성을 심화하게 된다. \n",
      "\n",
      "Top-5 passage with score 0.0530\n",
      "16개의 보 중에서 총 14개의 보를 2017년 11월 13일 수문을 개방했다. 이후 약 6개월이 지난 2018년 5월 4일 금강 세종보 상류는 수위가 낮아지면서 모래와 자갈로 이뤄진 작은 섬들이 드넓게 드러나기 시작했으며 좌안에는 모래가 30cm 이상 쌓이고 강물의 유속도 빨라지는 등 변화가 감지되었다. 현장 조사를 시행한 오준오 박사는 \"보 개방으로 실트층이 하류로 씻겨 내려가면서 자갈이 다시 드러났고, 모래가 쌓였다\"고 설명했다. 하지만 금강 하류의 백제보가 수문을 개방하지 않아 물빛이 세종보보다 탁했고 모래 대신 뻘만 가득하여 악취가 심한 등 이전과 차이가 없었다. 이는 낙동강도 마찬가지였다. 환경부 관계자는 \"보의 개방은 수질과 구조적 건강성, 생물학적 건강성 등 세 가지 측면에서 모두 긍정적 결과가 나타났다. 올해 말까지 개방한 보들에 대해 평가하고 처리 방안을 결정할 계획\"이라고 밝혔다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Top-{i + 1} passage with score {doc_scores[i]:.4f}\")\n",
    "    doc_id = doc_ids[i]\n",
    "    print(corpus[doc_id], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DirOwPFOixwg"
   },
   "source": [
    "Ground Truth 와 동일한 Passage 가 잘 나왔나요? 제법 성능이 나쁘지 않은걸 확인할 수 있네요.   \n",
    "### ❓ 제시된 두 Tokenizer 를 모두 활용해서 결과를 비교해볼까요?\n",
    "위에서 제시된 Hugginface Tokenizer 와 단순 띄어쓰기로 구분된 Tokenizer 는 많은 차이를 불러올까요? 직접 코드로 구현해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl9vDzADcnPu"
   },
   "source": [
    "### ❓ SparseRetrieval 코드를 class로 합쳐봅시다.\n",
    "Sparse Retrieval 을 잘 구현하셨나요? 하지만 코드가 너무 흩어져있어서 다시 재사용하기 많이 어렵겠네요. 이 코드들을 다른 사람들도 잘 활용할 수 있도록 모듈화를 진행해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34DcdPHglRIr"
   },
   "source": [
    "❗ _Hint_   \n",
    "Scratch 부터 짜는게 많이 어려우신가요?\n",
    "\n",
    "만들어야하는 기능들의 파이프라인을 나열하고 하나씩 모듈화하는 습관을 들이는 것이 좋습니다. 순서대로 생각해볼까요?\n",
    "\n",
    "1. 데이터를 불러옵니다.\n",
    "2. TF-IDF 를 Fitting 한 후 Passage 를 Transform 합니다.\n",
    "3. Query 를 Fitting 한 TF-IDF 를 통해 Transform 합니다.\n",
    "4. Inner dot product 를 이용해 유사도를 구하고, 내림차순을 통해 유사한 Passage 를 Retrieval 합니다.\n",
    "\n",
    "위의 단계를 하나씩 함수로 구현한 후 class로 합치면 훨씬 쉬울 겁니다! 반드시 이 과정을 따르지 않으셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Jr-IgCXWGysH"
   },
   "outputs": [],
   "source": [
    "# 화이팅\n",
    "class TfIdfRetrieval:\n",
    "\n",
    "    def __init__(self, tokenize_fn, data_path):\n",
    "\n",
    "        \"\"\"\n",
    "        1. 여기서 Data를 불러올까요.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "        texts = [wiki[str(i)] for i in range(len(wiki))]\n",
    "        self.corpus = list(set([v[\"text\"] for v in texts]))\n",
    "        self.sp_matrix = None\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn,\n",
    "            ngram_range=(1,2)\n",
    "            )\n",
    "\n",
    "    def get_sparse_embedding(self):\n",
    "\n",
    "        \"\"\"\n",
    "        2. 여기서 주어진 전체 passage들에 대해 TF-IDF를 .fit 해줍시다.\n",
    "        \"\"\"\n",
    "        self.vectorizer.fit(self.corpus)\n",
    "        self.sp_matrix = self.vectorizer.transform(self.corpus)\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1):\n",
    "\n",
    "        \"\"\"\n",
    "        여기서\n",
    "            3. Query를 받아서 TF-IDF에 Transform 시켜줍니다.\n",
    "            4. 전체 Passage에 대한 유사도를 구한 후 상위 k개의 Passage를 반환합니다.\n",
    "        \"\"\"\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        result = query_vec * self.sp_matrix.T\n",
    "        sorted_result = np.argsort(-result.data)[:k]\n",
    "        doc_scores = result.data[sorted_result]\n",
    "        doc_ids = result.indices[sorted_result]\n",
    "        return doc_scores, corpus[doc_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_2Vvx8BdzXw"
   },
   "source": [
    "잘 구현하셨나요? 여러분들이 만든 클래스를 아래와 같이 활용하는데 성공하셨다면 마무리된 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:39.464565Z",
     "start_time": "2021-09-15T06:54:38.837869Z"
    },
    "id": "-W8Tl7osdyf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse starts!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4e8b19934e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfIdfRetrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../data/wikipedia_documents.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sparse_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# query = \"미국의 대통령은 누구인가?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# doc_score, doc_indices = retriever.get_relevant_doc(query, k=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-07d208a32563>\u001b[0m in \u001b[0;36mget_sparse_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \"\"\"\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse starts!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-4e8b19934e92>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfIdfRetrieval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../data/wikipedia_documents.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sparse_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# query = \"미국의 대통령은 누구인가?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     def set_truncation_and_padding(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m         )\n\u001b[1;32m   2343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2344\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2345\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenize_fn = lambda x: tokenizer.tokenize(x)\n",
    "retriever = TfIdfRetrieval(tokenize_fn=tokenize_fn, data_path=\"../../data/wikipedia_documents.json\")\n",
    "\n",
    "retriever.get_sparse_embedding()\n",
    "# query = \"미국의 대통령은 누구인가?\"\n",
    "# doc_score, doc_indices = retriever.get_relevant_doc(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60613"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../data/wikipedia_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)\n",
    "len(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXorfdV1hHGH"
   },
   "outputs": [],
   "source": [
    "print(f\"[Search Query] {query}\")\n",
    "\n",
    "for i, idx in enumerate(doc_indices):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.contexts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHOE4jZqhXRs"
   },
   "source": [
    "직접 만든 코드에 다음과 같은 추가 사항들을 고려해봅시다.   \n",
    "#### ➕추가 과제: 다양한 기능 추가하기\n",
    "+ 현재 코드가 **어떻게 동작하고 있는지** 궁금하지 않으신가요? 가령, 코드가 돌고는 있는데 대체 어떤 메소드를 수행 중인지, 아니면 시간이 얼마나 걸리는지 등 추가적인 정보를 Logger 에 기록하거나 Prompt 에 찍는 방향으로 더 추가해보세요 !\n",
    "+ 위에서 시간을 출력하는 코드를 짜보셨다면, 이제 fitting 시간이 적지 않음을 확인할 수 있었습니다. 그리고 passage 가 자주 변경되는 것이 아니라면, 같은 기법을 사용해서 **매번 fitting 을 하는 것은 아주 비효율적**입니다. 임베딩된 passage를 따로 **저장해두고 불러와서 사용하면** 더 효율적이지 않을까요? 이 임베딩들을 `.bin` 파일로 저장하고, 만약 `.bin` 파일이 있다면 TfidfVectorizer 가 fitting 하지 않고, 이 `.bin` 파일을 불러오는 방향으로 코드를 추가해봅시다.\n",
    "+ 예외 케이스가 있지 않을까요? **query가 `str` 1개가 아니라 2개 이상인 `List[str]`로 주어진 경우에는 어떻게 해야할까요?** 이 상황은 멀리 있지 않고 여러분들이 대회에서 inference 할 때도 필요한 코드입니다. 단순히 반복문을 돌릴 수 있지만, 우리는 행렬곱을 이용하고 있기 때문에 만약 query 가 여러 개면 이 행렬곱의 장점을 이용할 수 있을 것 같습니다.\n",
    "    + 한 개의 query(`str`) 에 대해 유사한 passage 를 구하는 함수와\n",
    "    + 여러 개의 query(`list`) 에 대해 유사한 passage 를 구하는 함수\n",
    "\n",
    "  두 가지를 만들어봅시다. 그리고 `retrieve` 라는 함수를 만들어서 query 가 한 개인지 다수인지 체크한 후 각각의 함수를 사용해서 유사한 passage 를 찾도록 수정해봅시다.\n",
    "+ 또 다른 예외케이스는 `TfidfVectorizer` 에서 생길 수 있습니다. 가령 Passage 에서 한 번도 보지 못한 단어로만 구성된 query 를 입력했다면, TF-IDF 의 특성상 이 단어는 0 으로만 임베딩될 것입니다. 전체 벡터가 0이 되면 어떻게 처리해야할까요? `assert` 를 활용해보세요!\n",
    "+ 이 메소드는 어떤 parameters를 받아오고 어떤 기능을 하는지 docstring을 추가해봅시다.\n",
    "\n",
    "\n",
    "요구사항이 너무 많나요? 원래 인생이 그렇습니다.\n",
    "\n",
    "_Sparse Retrieval 과제는 두 가지 (기존과 추가 과제) 모두 예시 코드가 금요일 (10/15)제공됩니다._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## 💻 Ⅱ. Dense Retriever (BERT) 학습 시키기\n",
    "\n",
    "이번에는 BERT 를 불러오고 학습시키는 과정을 거쳐봅시다. 시작하기 전에 어떤 과정을 거쳐야할 지 생각해봅시다.\n",
    "1. 데이터셋 준비\n",
    "2. 모델 준비\n",
    "3. 학습하기 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmAIDfEbeENm"
   },
   "source": [
    "### 💻 1. 데이터셋 준비하기\n",
    "위에서 불러온 `datasets`를 이용해 데이터를 불러왔습니다. 하지만 이번 과제에서 passage encoder 가 학습되는 방식은 주어진 query/question에 적합한 passage 를 찾아오는 과정이 필요한데, 이 때 올바른 데이터만 이용해서 학습하는 것이 아니라, 올바르지 않은 passage 또한 학습해보는 과정을 거쳐야합니다. 이를 in-batch negative 라고 부르는데 관련된 내용은 기계독해 강의와 아래 논문을 참고해보세요.\n",
    "+ [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "#### 💻 Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "실습 시간을 단축시키기 위해 일부 데이터만 활용해봅시다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.543616Z",
     "start_time": "2021-09-15T06:56:49.543616Z"
    },
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "sample_idx = np.random.choice(range(len(dataset[\"train\"])), 20)\n",
    "training_dataset = dataset[\"train\"][sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALNnnBJTxeU4"
   },
   "source": [
    "#### 💻 Negative sampling 을 위한 negative sample 들을 샘플링\n",
    "\n",
    "주어진 query/question 에 해당하지 않는 지문들을 뽑아서 훈련데이터로 넣어줍시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.545616Z",
     "start_time": "2021-09-15T06:56:49.545616Z"
    },
    "id": "LPomXJ1Afc6l"
   },
   "outputs": [],
   "source": [
    "# In-batch Negatvie로 사용할 데이터 생성\n",
    "num_neg = 2\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "p_with_neg = []\n",
    "\n",
    "for c in training_dataset[\"context\"]:\n",
    "    \n",
    "    while True:\n",
    "        neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "        if not c in corpus[neg_idxs]:\n",
    "            p_neg = corpus[neg_idxs]\n",
    "\n",
    "            p_with_neg.append(c)\n",
    "            p_with_neg.extend(p_neg)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir_hYkQ5fBro"
   },
   "source": [
    "주어진 질문에 알맞는 지문과 올바르지 않는 지문을 모두 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.546616Z",
     "start_time": "2021-09-15T06:56:49.546616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbGW7PRJ7Yv5",
    "outputId": "7bc3b3ac-5f88-4c0f-fa07-76bf21d9b148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query Given]\n",
      "'문제는 나랏법 자체보다는 법을 적용하고 옹호하는 데 있었다고 묘사한 사학자는?'\n",
      "\n",
      "[Positive context]\n",
      "('전원 공동체, 특히 그 중에서도 잉글랜드 동남부는 농노제의 작동과 지역 장원 재판소의 세금 징수를 불만스러워했다. 특히 재판소를 운영하는 '\n",
      " '영주들이 곧 원성을 사는 노동 조례나 왕실에서 제정한 법의 현장 집행자로 기능하는 경우가 잦았기 때문에 불만은 가중되었다. 촌락의 엘리트 '\n",
      " '계층 다수는 지방정부의 역할을 맡는 것을 거부했고, 재판소의 활동을 방해하기 시작했다. 재판소에 의해 몰수된 가축들은 그 주인들에 의해 '\n",
      " '‘해방’되었고, 법관들은 폭행을 당했다. 일부는 전통적인 법을 존중하나 런던에서 내려오는 증오받는 중앙 법률에서는 분리된, 독립적인 촌락 '\n",
      " '공동체의 탄생을 지지하기 시작했다. 사학자 미리 루빈은 이 상황을, “문제는 나랏법 그 자체라기보다, 그 법을 적용하고 옹호하는 데 '\n",
      " '있었다”고 묘사한다.')\n",
      "\n",
      "[Negative context]\n",
      "'9월 26일 바예지드는 전투 직전 라호보의 오스만 군 포로들을 프랑스인들이 죽인것에 대한 보복으로 3,000명에서 10,000명에 달하는 포로들을 죽일 것을 명한다. 그는 또 그가 압도적인 승리를 거두었음에도 불구하고 전투 초반에 많은 병사들을 잃은 것에 대하여 분노하였다. 그는 어린 포로들을 자신의 군대에 편입시켰다. 고향으로 돌아가는 와중에 많은 고생을 하지만 결국 돌아간 이들도 있었다. 지기스문트는 니콜라 고르잔스키와 실리의 헤르만의 도움으로 도망칠 수 있었다. 그는 베네치아 배를 타 흑해, 에게 해, 그리고 지중해를 통해 해로로 고향으로 돌아가는 길을 택했고 이 와중에 왈라키아인들의 믿을 수 없는 성품을 알아차린다. 샤를 6세에게 패배소식이 크리스마스에 알려졌다. 서유럽의 기사들은 곧 십자군에 대한 열정을 잃어버렸다. 싸움은 스페인과 지중해, 그리고 북유럽의 이교도에 대하여 계속되고 있었다. 그러나 이 전투 이후에 발칸 반도에 대한 오스만 투르크의 전진을 막기 위해 서유럽에서 구성된 원정은 더 이상 없었다. 르네상스시대가 다가올 때까지였다.'\n",
      "'가스라이팅(gaslighting)은 상황 조작을 통해 타인의 마음에 스스로에 대한 의심을 불러일으켜 현실감과 판단력을 잃게 만듦으로써 그 사람을 정신적으로 황폐화시키고 그 사람에게 지배력을 행사하여 결국 그 사람을 파국으로 몰아가는 것을 의미하는 심리학 용어이다. 주로 친밀한 관계에서 이루어진다. 하지만 정치계나 연예계에서도 구사될 수 있다. 가스라이팅 구사자들은 상황 조작을 통해 상대방의 자아를 흔들어서 자신의 영향력을 증폭시킨다. 이를 통해 상대방을 자유자재로 가지고 놀 수 있고 그 사람이 가진 재산 등을 탈취할 수도 있다. 가스라이팅 피해자는 자신에 대한 신뢰감을 잃어가게 되고 종국에는 자존감이 없어진다. 가해자들은 상대방의 공감능력을 이용해서 상대방을 통제한다. 동정심을 이용해서 타인을 조종하는 소시오패스가 예가 될 수 있다. 이런 심리술을 이론화한 로빈 스턴은 미국에서 20여 년간 심리상담가, 교사, 우드헐리더십연구원으로 일하면서 수많은 상담을 진행해온 리더십 강사 및 컨설턴트였다.'\n"
     ]
    }
   ],
   "source": [
    "print(\"[Query Given]\")\n",
    "pprint(training_dataset[\"question\"][0])\n",
    "\n",
    "print(\"\\n[Positive context]\")\n",
    "pprint(p_with_neg[0])\n",
    "\n",
    "print(\"\\n[Negative context]\")\n",
    "pprint(p_with_neg[1])\n",
    "pprint(p_with_neg[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jW0gXtJ-_nf"
   },
   "source": [
    "처리한 데이터를 `torch`가 처리할 수 있게 `DataLoader`로 넘겨주는 작업을 해봅시다. 기본적으로 Huggingface Pretrained 모델이 `input_ids`, `attention_mask`, `token_type_ids`를 받아주니, 이 3가지를 넣어주도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiacVxXFeBbK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "\n",
    "q_seqs = tokenizer(\n",
    "    training_dataset[\"question\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "p_seqs = tokenizer(\n",
    "    p_with_neg,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvzIayy79Mdy",
    "outputId": "02e08862-b73f-4bd3-c629-534c8ff88298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg + 1, max_len)\n",
    "p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg + 1, max_len)\n",
    "p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg + 1, max_len)\n",
    "\n",
    "print(p_seqs[\"input_ids\"].size())  # (num_example, pos + neg, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "    q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "### 💻 2. BERT encoder 학습시키기\n",
    "1. BERT 모델을 구성한 후\n",
    "2. Passage 를 임베딩하는 `p_encoder`와 Query/Question 을 임베딩하는 `q_encoder` 를 각각 선언해줍니다.\n",
    "3. 두 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:32.340866Z",
     "start_time": "2021-09-15T08:06:32.327858Z"
    },
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "      \n",
    "    def forward(self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnO1b30SomBP",
    "outputId": "635dec4b-33c2-4026-8fef-a6fa8f150829"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-train된 모델을 사용해줍니다. 위에서 사용한 `model_checkpoint`를 재활용합니다.\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "`train` 함수를 정의한 후 `p_encoder`과 `q_encoder`를 학습시켜봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAb7NpUc8YRo"
   },
   "outputs": [],
   "source": [
    "def train(args, num_neg, dataset, p_encoder, q_encoder):\n",
    "    batch_size = args.per_device_train_batch_size\n",
    "  \n",
    "    # Dataloader\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Optimizer\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "        {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "        {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "        {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        eps=args.adam_epsilon\n",
    "    )\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Start training!\n",
    "    global_step = 0\n",
    "\n",
    "    p_encoder.zero_grad()\n",
    "    q_encoder.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    for _ in train_iterator:\n",
    "\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "\n",
    "                p_encoder.train()\n",
    "                q_encoder.train()\n",
    "        \n",
    "                targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                targets = targets.to(args.device)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                    \"attention_mask\": batch[1].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                    \"token_type_ids\": batch[2].view(batch_size * (num_neg + 1), -1).to(args.device)\n",
    "                }\n",
    "        \n",
    "                q_inputs = {\n",
    "                    \"input_ids\": batch[3].to(args.device),\n",
    "                    \"attention_mask\": batch[4].to(args.device),\n",
    "                    \"token_type_ids\": batch[5].to(args.device)\n",
    "                }\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                # (batch_size * (num_neg + 1), emb_dim)\n",
    "                p_outputs = p_encoder(**p_inputs)\n",
    "                # (batch_size, emb_dim)  \n",
    "                q_outputs = q_encoder(**q_inputs)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                p_outputs = p_outputs.view(batch_size, -1, num_neg + 1)\n",
    "                q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                # (batch_size, num_neg + 1)\n",
    "                sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  \n",
    "                sim_scores = sim_scores.view(batch_size, -1)\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                q_encoder.zero_grad()\n",
    "                p_encoder.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "    return p_encoder, q_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2, # 아슬아슬합니다. 작게 쓰세요 !\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5h7_-aRA9dm"
   },
   "source": [
    "_혹시 CUDA 메모리 에러가 뜬다면 passage encoder 와 query encoder 를 나누지 말고 하나로 학습해보세요. `batch_size`도 변경해보세요._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "fbbd90b3-b569-450c-8054-692333666768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:12<00:00, 13.29s/batch, loss=0.0]\n",
      "100%|██████████| 10/10 [02:28<00:00, 14.84s/batch, loss=0.0]\n",
      "Epoch: 100%|██████████| 2/2 [04:41<00:00, 140.66s/it]\n"
     ]
    }
   ],
   "source": [
    "p_encoder, q_encoder = train(args, num_neg, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "### 💻 3. 검증셋에 있는 Query 에 대해 passage-retrieval 해보기\n",
    "이제 처음 보는 검증셋에 있는 Query 와 passage 에 대해서도 잘 작동하는지 확인해봅시다. 아래와 같은 순서로 작동해야겠죠?\n",
    "1. 검증셋을 불러온다.\n",
    "2. 검증셋의 query 와 passage 를 임베딩시킨다.\n",
    "3. 유사도를 통해 유사한 주어진 query 에 맞는 passage 를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZA5V6tEpuUV"
   },
   "source": [
    "#### 💻 검증셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NouB9uBcTaws",
    "outputId": "d20d885c-d7cf-481f-f5fa-66b3952236b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- [Selected Query] -----\n",
      "'SBS 연기대상에서 김태희는 무슨 상을 받았나?'\n",
      "\n",
      "----- [Ground Truth] -----\n",
      "('2001년 영화 《선물》에 이영애의 아역으로 출연하면서 연기자로 입문하였고, 2002년 독립 영화 《신도시인》에 출연했다. 이후 '\n",
      " '시트콤《레츠고》(2002), 《스크린》, 《흥부네 박터졌네》(2003)와 같은 텔레비전 드라마에 출연했다. 2003년에는 《천국의 '\n",
      " '계단》에서 악역 한유리 역할로 얼굴을 알리며 인기를 얻기 시작했고, 이 드라마로 SBS 연기대상에서 뉴스타상을 수상했다. 2004년에는 '\n",
      " '드라마 《구미호 외전》과 《러브스토리 인 하버드》에 출연했다. 하지만 이후 영화 《중천》(2006), 《싸움》(2007)에 출연했으나, '\n",
      " '모두 흥행에 실패했다. 2009년에는 시청률 30%대를 기록한 드라마 《아이리스》에서 최승희 역할을 연기해 KBS 연기대상에서 '\n",
      " '우수연기상을 수상하였다. 이후 드라마 《마이프린세스》(2011), 《나와 스타의 99일》(2012), 《장옥정 사랑에 '\n",
      " '살다》(2013)에서 주연으로 출연했고, 2015년 드라마 《용팔이》로 SBS 연기대상 최우수연기상을 수상했다.')\n"
     ]
    }
   ],
   "source": [
    "valid_corpus = list(set([example[\"context\"] for example in dataset[\"validation\"]]))[:10]\n",
    "\n",
    "sample_idx = random.choice(range(len(dataset[\"validation\"])))\n",
    "query = dataset[\"validation\"][sample_idx][\"question\"]\n",
    "ground_truth = dataset[\"validation\"][sample_idx][\"context\"]\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "    valid_corpus.append(ground_truth)\n",
    "\n",
    "print(f\"[Selected Query]\")\n",
    "pprint(query)\n",
    "\n",
    "print(f\"[Ground Truth]\")\n",
    "pprint(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "#### 💻 앞서 학습한 passage encoder, question encoder 을 이용해 dense embedding 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YufA_ayPJBRg",
    "outputId": "cd5720b6-46c9-4da6-d5dc-d480868348e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "    q_encoder.eval()\n",
    "\n",
    "    q_seqs_val = tokenizer(\n",
    "        [query],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    q_emb = q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query, emb_dim)\n",
    "\n",
    "    p_embs = []\n",
    "    for p in valid_corpus:\n",
    "        p = tokenizer(\n",
    "            p,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        p_emb = p_encoder(**p).to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "#### 💻 Dot product를 통해 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "84d37434-bfe5-4a6f-d160-d0ca72d65bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "tensor([[176.3418, 147.5652, 165.1897, 177.7192, 148.4269, 156.9671, 166.3144,\n",
      "         155.8632, 150.4935, 141.3773, 166.0702]])\n",
      "tensor([ 3,  0,  6, 10,  2,  5,  7,  8,  4,  1,  9])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "print(dot_prod_scores)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "#### 💻 Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "9d93531f-8e65-43c7-dfdc-a1184963362f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " SBS 연기대상에서 김태희는 무슨 상을 받았나? \n",
      "\n",
      "[Ground truth passage]\n",
      "2001년 영화 《선물》에 이영애의 아역으로 출연하면서 연기자로 입문하였고, 2002년 독립 영화 《신도시인》에 출연했다. 이후 시트콤《레츠고》(2002), 《스크린》, 《흥부네 박터졌네》(2003)와 같은 텔레비전 드라마에 출연했다. 2003년에는 《천국의 계단》에서 악역 한유리 역할로 얼굴을 알리며 인기를 얻기 시작했고, 이 드라마로 SBS 연기대상에서 뉴스타상을 수상했다. 2004년에는 드라마 《구미호 외전》과 《러브스토리 인 하버드》에 출연했다. 하지만 이후 영화 《중천》(2006), 《싸움》(2007)에 출연했으나, 모두 흥행에 실패했다. 2009년에는 시청률 30%대를 기록한 드라마 《아이리스》에서 최승희 역할을 연기해 KBS 연기대상에서 우수연기상을 수상하였다. 이후 드라마 《마이프린세스》(2011), 《나와 스타의 99일》(2012), 《장옥정 사랑에 살다》(2013)에서 주연으로 출연했고, 2015년 드라마 《용팔이》로 SBS 연기대상 최우수연기상을 수상했다. \n",
      "\n",
      "Top-1 passage with score 177.71917724609375:.4f\n",
      "('2013년 12월부터는 SBS 드라마 《별에서 온 그대》에 한류 스타 천송이 역으로 출연했다. 이는 1999년 《해피투게더》 이후 14년 '\n",
      " '만의 드라마 작품으로, 이 전에 《도둑들》에서 함께 했던 김수현과 주연을 맡았다. 《별에서 온 그대》는 최고 시청률 28.1%, 평균 '\n",
      " '시청률 22.6%를 기록하며 단 한 번도 동시간대 시청률 1위를 놓친 적이 없다. 이 드라마로 2014년 5월 27일 열린 제50회 '\n",
      " '백상예술대상에서 TV 대상을 포함해 InStyle상까지 2관왕에 올랐다. 같은 해 《별에서 온 그대》로 2014년 SBS 연기대상에서 '\n",
      " '최고상인 대상과 베스트 커플상, 10대 스타상, 프로듀서상을 수상하여 4관왕에 올랐다.')\n",
      "Top-2 passage with score 176.34176635742188:.4f\n",
      "('1976년 10월 공군에 입대해 사병으로 복무 중인, 1978년 12월에 독일 분데스리가 SV 다름슈타트 98로 이적하였지만 병역 관련 '\n",
      " '문제로 계약이 파기되었다. (당시 차범근은 공군 팀 전력 강화를 꾀하던 참모총장의 권한으로 2년 뒤 전역을 약속받고, 공군에 입대하였다. '\n",
      " '약속된 1978년 12월 복무 기간을 마쳤다고 생각하고, 특별 휴가를 받아 독일로 떠나 SV 다름슈타트 98 입단 계약을 체결하였고 '\n",
      " '12월 30일 VfL 보훔과의 리그 경기에 출전해서 77분을 소화했고, 키커 평점 3점을 받는 등 좋은 활약을 펼치며 분데스리가 데뷔전을 '\n",
      " '성공적으로 끝마쳤다. 그러나 공군의 입장 변화로 1979년 1월 5일 다시 귀국한 후, 복귀해서 독일로 다시 나가지 못하고 1979년 '\n",
      " '5월 31일 만기 전역하였다.')\n",
      "Top-3 passage with score 166.31443786621094:.4f\n",
      "('국토교통부는 2015년까지 100년 주택인 장수명 아파트 인증제 도입을 추진한다고 보도했다. 유럽이나 미국의 경우, 100년된 주택을 '\n",
      " '쉽게 찿을수 있지만 고도의 성장에 있어 40년 넘은 주택조차 찿기 어려운것에 대한 조치이다. 서울시는 최근 근현대 문물 보존 프로잭트인 '\n",
      " '미래문화유산화의 일부보존 대상으로 아파트 10군데를 후보로 지정하였다. 가장오래된 충정아파트(1933), 동대문아파트 (1965)을 '\n",
      " '비롯해 정동아파트 (1965), 힐탑아파트 (1967), 회현제2시민아파트 (1970), 여의도시범아파트 (1971), 성요셉아파트 '\n",
      " '(1971), 서소문아파트 (1972), 반포본동아파트 (1973), 개포주공1단지아파트 (1981)가 선정되었으며 소유자의 동의를 받고 '\n",
      " '지정될 예정이다.')\n",
      "Top-4 passage with score 166.0701904296875:.4f\n",
      "('2001년 영화 《선물》에 이영애의 아역으로 출연하면서 연기자로 입문하였고, 2002년 독립 영화 《신도시인》에 출연했다. 이후 '\n",
      " '시트콤《레츠고》(2002), 《스크린》, 《흥부네 박터졌네》(2003)와 같은 텔레비전 드라마에 출연했다. 2003년에는 《천국의 '\n",
      " '계단》에서 악역 한유리 역할로 얼굴을 알리며 인기를 얻기 시작했고, 이 드라마로 SBS 연기대상에서 뉴스타상을 수상했다. 2004년에는 '\n",
      " '드라마 《구미호 외전》과 《러브스토리 인 하버드》에 출연했다. 하지만 이후 영화 《중천》(2006), 《싸움》(2007)에 출연했으나, '\n",
      " '모두 흥행에 실패했다. 2009년에는 시청률 30%대를 기록한 드라마 《아이리스》에서 최승희 역할을 연기해 KBS 연기대상에서 '\n",
      " '우수연기상을 수상하였다. 이후 드라마 《마이프린세스》(2011), 《나와 스타의 99일》(2012), 《장옥정 사랑에 '\n",
      " '살다》(2013)에서 주연으로 출연했고, 2015년 드라마 《용팔이》로 SBS 연기대상 최우수연기상을 수상했다.')\n",
      "Top-5 passage with score 165.18966674804688:.4f\n",
      "('1953년 3월 24일 전라북도 고창군에서 태어난 김이수는 고수국민학교, 광주서중학교, 전남고등학교와 서울대학교 법학과를 졸업했다. '\n",
      " '대학교 3학년인 1974년 민청학련 사건에 연루돼 64일간 구금됐다가 석방되었다. 1977년 제19회 사법시험에 합격해 사법연수원 9기를 '\n",
      " '수료하고 1979년 12월 31일에 사단 군검찰관으로 임관해 5개월 후인 1980년 5월 법무사 군 판사로서 5·18 항쟁 시민군을 태운 '\n",
      " '버스 운전사에게 사형을 선고했다. 이를 지적하는 새누리당 함진규 의원의 지적에 \"안 맡았으면 좋았을 재판이라고 지금도 생각한다\"며 \"광주 '\n",
      " '사람으로서 광주항쟁에 참여해야할 입장이었는데 재판을 맡게 됐다. 아주 복잡한 입장이었다\"고 말했으며 \"시민군 가담 여고생에게 징역 1년을 '\n",
      " '선고하고 50살 농민을 구금하고 징역 2년 집행유예 3년 선고했다. 군의 살상행위를 알린 현직 이장에게 유언비어 유포죄로 징역 1년 집유 '\n",
      " '2년을 선고했다\"는 새누리당 의원의 지적에는 \"기록을 검토해봐야 한다. 판결문만 보면 모순이 있는 것 같다\"고 했다. 5·16 '\n",
      " '군사쿠데타에 관해서 \"권력을 잡는 방법은 비정상적이었지만 권력을 잡은 측면과 전체를 한꺼번에 봐야한다\"며 \"10월 유신까지 가는 부분에는 '\n",
      " '공과가 있다\"고 했다.')\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(f\"Top-{i + 1} passage with score {dot_prod_scores.squeeze()[rank[i]]}:.4f\")\n",
    "  pprint(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cicYTii7jZZC"
   },
   "source": [
    "Ground Truth와 똑같거나 비슷한 Passage가 출력됐나요? 역시 Pretrained는 강력하군요.\n",
    "### ❓ Dense Retrieval 코드를 class로 합쳐봅시다.\n",
    "현재 구현된 모델은 굉장히 단순히 짜여졌습니다. 이미 Pretrain된 BERT 모델을 활용했는데, 다른 PLM은 어떨까요? 이를 다시 시도해보자니 코드가 너무 셀로 흩어져있어서 재사용이 어렵네요. 우선 클래스화를 진햏한 후에, `BERT`가 아닌 다른 PLM도 활용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on_Ay5_nbqEc"
   },
   "source": [
    "❗ _Hint_   \n",
    "Scratch부터 짜는게 많이 어려우신가요?\n",
    "\n",
    "만들어야하는 기능들의 파이프라인을 나열하고 하나씩 모듈화하는 습관을 들이는 것이 좋습니다. 순서대로 생각해볼까요?\n",
    "\n",
    "1. Setup   \n",
    "    Naive한 `Dataset`을 받아서 이를 In-Batch Negative를 활용한 후 Dataloader로 변경해주는 코드가 있어야겠죠? 클래스 내에서 활용할 수 있도록 속성(attribute)으로 만들어줍시다. 이 코드를 위에서 활용한 `train` 함수에서 조금 차용해볼까요?\n",
    "2. PLM을 주어진 Passage 와 In-batch negative 기법을 활용해서 훈련합니다.   \n",
    "    이는 위에서 만든 `train` 함수를 약간 응용해서 재활용합시다.\n",
    "3. 훈련한 PLM 을 통해 Query 를 Transform 합니다.\n",
    "4. 내적을 통해 유사도를 구하고, 내림차순을 통해 유사한 Passage 를 Retrieval 합니다.\n",
    "\n",
    "위 4단계를 하나씩 함수로 구현한 후 Class로 합치면 훨씬 쉬울 겁니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:47.410939Z",
     "start_time": "2021-09-15T08:06:46.140856Z"
    },
    "id": "994wDYAujsLr"
   },
   "outputs": [],
   "source": [
    "# 코드가 많아보이지만 주석이 더 많지롱\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        num_neg,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            args (Huggingface Arguments):\n",
    "                세팅과 학습에 필요한 설정값을 받습니다.\n",
    "            dataset (datasets.Dataset):\n",
    "                Huggingface의 Dataset을 받아옵니다.\n",
    "            num_neg (int):\n",
    "                In-batch negative 수행시 사용할 negative sample의 수를 받아옵니다.\n",
    "            tokenizer (Callable):\n",
    "                Tokenize할 함수를 받아옵니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "            p_encoder (torch.nn.Module):\n",
    "                Passage를 Dense Representation으로 임베딩시킬 모델입니다.\n",
    "            q_encoder (torhc.nn.Module):\n",
    "                Query를 Dense Representation으로 임베딩시킬 모델입니다.\n",
    "\n",
    "        Summary:\n",
    "            학습과 추론에 필요한 객체들을 받아서 속성으로 저장합니다.\n",
    "            객체가 instantiate될 때 in-batch negative가 생긴 데이터를 만들도록 함수를 수행합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder.to(args.device)\n",
    "        self.q_encoder = q_encoder.to(args.device)\n",
    "\n",
    "        self.prepare_in_batch_negative(num_neg=num_neg)\n",
    "\n",
    "\n",
    "    def prepare_in_batch_negative(self,\n",
    "        dataset=None,\n",
    "        num_neg=2,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dataset (datasets.Dataset, default=None):\n",
    "                Huggingface의 Dataset을 받아오면,\n",
    "                in-batch negative를 추가해서 Dataloader를 만들어주세요.\n",
    "            num_neg (int, default=2):\n",
    "                In-batch negative 수행시 사용할 negative sample의 수를 받아옵니다.\n",
    "            tokenizer (Callable, default=None):\n",
    "                Tokenize할 함수를 받아옵니다.\n",
    "                별도로 받아오지 않으면 속성으로 저장된 Tokenizer를 불러올 수 있게 짜주세요.\n",
    "\n",
    "        Note:\n",
    "            모든 Arguments는 사실 이 클래스의 속성으로 보관되어 있기 때문에\n",
    "            별도로 Argument를 직접 받지 않아도 수행할 수 있게 만들어주세요.\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "        args=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            train을 합니다. 위에 과제에서 이용한 코드를 활용합시다.\n",
    "            encoder들과 dataloader가 속성으로 저장되어있는 점에 유의해주세요.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_relevant_doc(self,\n",
    "        query,\n",
    "        k=1,\n",
    "        args=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str)\n",
    "                문자열로 주어진 질문입니다.\n",
    "            k (int, default=1)\n",
    "                상위 몇 개의 유사한 passage를 뽑을 것인지 결정합니다.\n",
    "            args (Huggingface Arguments, default=None)\n",
    "                Configuration을 필요한 경우 넣어줍니다.\n",
    "                만약 None이 들어오면 self.args를 쓰도록 짜면 좋을 것 같습니다.\n",
    "\n",
    "        Summary:\n",
    "            1. query를 받아서 embedding을 하고\n",
    "            2. 전체 passage와의 유사도를 구한 후\n",
    "            3. 상위 k개의 문서 index를 반환합니다.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.333519Z",
     "start_time": "2021-09-15T08:06:47.419925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1b16d8e4c2d74aab9fdda9ae60c21441",
      "58ca9dbe05d149fcb73677736541c1ce",
      "92cbaf4e42694aca93e327941b1c9422",
      "e4c5ce6351014015aa41d8a43fdca7d8",
      "9407250e75c14524be3fa222b02c4f74",
      "e38d794604bb409bb4d41a5bfef5eb61",
      "0a78e445530c4d93a2ccce5bcda351da",
      "3fe1fbff472c435c94c25c1736081373",
      "430873d31e5c4770afd48183c1a8f1de",
      "3ad885d798e64e35ae6705412494a52e",
      "be78ee5fbd55487fb3dd0da23adbc154"
     ]
    },
    "id": "cBr57rLvkM68",
    "outputId": "1bcc0a3e-90b7-44cb-c772-91a88df16f93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b16d8e4c2d74aab9fdda9ae60c21441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋과 모델은 아래와 같이 불러옵니다.\n",
    "train_dataset = load_dataset(\"squad_kor_v1\")[\"train\"]\n",
    "\n",
    "# 메모리가 부족한 경우 일부만 사용하세요 !\n",
    "num_sample = 1500\n",
    "sample_idx = np.random.choice(range(len(train_dataset)), num_sample)\n",
    "train_dataset = train_dataset[sample_idx]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.335488Z",
     "start_time": "2021-09-15T08:06:50.335488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aUOE8VEyZ1v",
    "outputId": "1b78b56c-a84c-4e5f-8d4a-43afe8dd0c2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 750/750 [01:34<00:00,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    num_neg=2,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")\n",
    "\n",
    "retriever.train()\n",
    "\n",
    "query = \"유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은?\"\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKKMfYhREui4",
    "outputId": "ef512c8c-f8d6-4b6f-d124-24c438529faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은? \n",
      "\n",
      "[Predicted Passage]\n",
      "('한편 2009년 헐리우드 메이저 영화 단독 주연으로 \"닌자 어쌔신\" 개봉하였고 2010년 6월 6일 아시아 한국인 최초로 미국 LA에서 '\n",
      " '열린 제 19회 \"2010 엠티비 무비 어워드(MTV Movie Awards)에서 최고의 액션스타상(Biggest Badass '\n",
      " 'Star)을 수상하였다. 아시아 최초 첫 단독 주연 액션스타상은 비가 최초이다. 또한 미국의 배우 안젤리나 졸리는 한 언론과의 인터뷰에서 '\n",
      " '비는 정말 대단하다며 극찬을 펼치기도 했었다. 2011년 아시아 연예인 최초로 미국 타임 세계에서 가장 영향력 있는 100인에 2회 '\n",
      " '선정되었다. 미국 타임 100인 선정은 타임 심사 위원들의 심사를 거쳐 선정하는데 가능하면 과거에 뽑힌 인물은 중복해서 선정하지 않는 게 '\n",
      " '원칙이고 인터넷 투표는 참고사항일뿐이라고 타임지는 밝혔습니다. 비는 지금까지 온라인 인기투표 리스트에 총 6회 이름이 올려줬다.[1]')\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Search Query] {query}\")\n",
    "\n",
    "indices = result[1]\n",
    "for i, idx in enumerate(indices):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.contexts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwE-zCr2CIiq"
   },
   "source": [
    "#### ➕추가 과제: 다양한 기능 추가하기\n",
    "현재 코드에서 불편한 부분이 있죠. 일단 너무 많은 기능을 한 class에 넣은 것도 문제, 메소드도 한 번에 너무 다양한 기능을 하는 것이 문제입니다. 메소드 내의 기능을 쪼개서 여러 메소드로 나누거나 데이터셋을 만드는 부분 같이 큰 기능을 다른 class로 분리해봅시다. (DPR 추가 과제는 예시 코드가 제공되지 않습니다. 다양하게 활용해보세요 !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU8A2bpXnIOL"
   },
   "source": [
    "### ❓ Sparse Retrieval과 Dense Retrieval을 둘 다 시도해보았습니다. 어떤 차이가 있을까요? 팀원들과 논의해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8KO4AmLhr_G"
   },
   "source": [
    "## ❓ 과제: Wikipedia documents에 대해 Passage Retrieval 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnvVrapPmcCO"
   },
   "source": [
    "위에서 배운 Passage Retrieval을 Wikipedia에 있는 문서들로 진행해봅시다. 사실 위에서 두 클래스를 성공적으로 만들었다면, 데이터를 불러온 후 적용만 시키면 끝이 납니다.   \n",
    "***별도의 예시코드가 제공되지 않습니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC2AVU0m6OoV"
   },
   "outputs": [],
   "source": [
    "# 츄라이 츄라이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74EO3anZnbga"
   },
   "source": [
    "만족스러운 결과가 나왔나요? 그렇지 않았다면 모델의 구조를 바꾸거나, 더 많은 training set으로 학습 시켜보세요 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3j0VhH-kU9e"
   },
   "source": [
    "### ❓ Take-home Question\n",
    "이 QA 모델을 배포한다고 생각해봅시다. 성능이 좋아서 점점 이용자가 많아지는 상황입니다. 만약 동시에 1000명의 이용자가 Query 를 보내면 어떻게 될까요? 전체 Passage가 100,000 개라면, 이 유사도를 계산하고 비교하는 횟수는 ... 점점 많아질 것 같네요. 이 문제는 어떻게 해결하면 좋을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCWS-F5haN6_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1012_MRC Mission 3 - Sparse & Dense Passage Retrieval.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e6a65b5bf72b320117a5cfd505c5a1e2290f30898d5727c674807c535145cbf6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "439.071px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03d20af78d7c44159abca802ac082d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d9e100657ac4554974e990951e43387",
      "placeholder": "​",
      "style": "IPY_MODEL_1f2a869e4b404e68b954eb384ef62d68",
      "value": " 38.5M/? [00:01&lt;00:00, 31.1MB/s]"
     }
    },
    "05a482882c4f4e1b9b85150a3eed2293": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec3ad8119f13412f8a7b8b00bf275b8b",
       "IPY_MODEL_a2daef726e6e48219e7c0d9c62ac962c"
      ],
      "layout": "IPY_MODEL_5bfdfadc746c4a2ca44af7da3bfc4ab9"
     }
    },
    "0a78e445530c4d93a2ccce5bcda351da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0be86d05d64240b887902fb7624c474d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d9e100657ac4554974e990951e43387": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f36e44064244c2daf1a9fdeee5e7fe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1111e96e637447439c90016ef4184ba7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "113b4c8b0e264313a5abfc5bc3fe85df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b16d8e4c2d74aab9fdda9ae60c21441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92cbaf4e42694aca93e327941b1c9422",
       "IPY_MODEL_e4c5ce6351014015aa41d8a43fdca7d8",
       "IPY_MODEL_9407250e75c14524be3fa222b02c4f74"
      ],
      "layout": "IPY_MODEL_58ca9dbe05d149fcb73677736541c1ce"
     }
    },
    "1f2a869e4b404e68b954eb384ef62d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2553f14880194fe3b1a4a5c611de59f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_925386f42c2b40a09990ade0181559ba",
       "IPY_MODEL_03d20af78d7c44159abca802ac082d1a"
      ],
      "layout": "IPY_MODEL_9d5de35252fd4404a474054f999e950b"
     }
    },
    "3ad885d798e64e35ae6705412494a52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d4b7f7914fb42f39c364a92538eea2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fe1fbff472c435c94c25c1736081373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "430873d31e5c4770afd48183c1a8f1de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46d61012f6ac4ca189d8890826a27fe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48f64c7b8d164ce7ac7efb8ae7aaaffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "493174df299e451c95241a6fa0f21b59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49e6ffd8000644eda4157dcb48829984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c832fc7a4ca4965bd37b68a1ae2c736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be86d05d64240b887902fb7624c474d",
      "placeholder": "​",
      "style": "IPY_MODEL_49e6ffd8000644eda4157dcb48829984",
      "value": " 428/428 [00:01&lt;00:00, 320B/s]"
     }
    },
    "5357ac4465374fc79ec8dd5a9c532a49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58ca9dbe05d149fcb73677736541c1ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bfdfadc746c4a2ca44af7da3bfc4ab9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "697798699f354b3bb87a4265f545dcbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f85b679b2add466ba5230481ffcfa3ab",
       "IPY_MODEL_4c832fc7a4ca4965bd37b68a1ae2c736"
      ],
      "layout": "IPY_MODEL_9ebc6cb8cbdc4675869035f04166b9d7"
     }
    },
    "69b6b6a01c034bf78478509a09ac3513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc2727ff9c2d401a8008e617ba9ad747",
       "IPY_MODEL_fb079b9a51a248d9ad0de418ab0476c8"
      ],
      "layout": "IPY_MODEL_9da62ff244874ecb94b771df40dd7b64"
     }
    },
    "7260a5df06ff44a18e4890109b9b0be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7b64d95da47f46c4800e0bba13283693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "925386f42c2b40a09990ade0181559ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46d61012f6ac4ca189d8890826a27fe5",
      "max": 7568316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9e10b47884943488ce8fdbaf2f1e9b9",
      "value": 7568316
     }
    },
    "92cbaf4e42694aca93e327941b1c9422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a78e445530c4d93a2ccce5bcda351da",
      "placeholder": "​",
      "style": "IPY_MODEL_e38d794604bb409bb4d41a5bfef5eb61",
      "value": "100%"
     }
    },
    "9407250e75c14524be3fa222b02c4f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be78ee5fbd55487fb3dd0da23adbc154",
      "placeholder": "​",
      "style": "IPY_MODEL_3ad885d798e64e35ae6705412494a52e",
      "value": " 2/2 [00:00&lt;00:00, 36.73it/s]"
     }
    },
    "9a28b6e8626841f2a23fd6f31160a2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f36e44064244c2daf1a9fdeee5e7fe4",
      "placeholder": "​",
      "style": "IPY_MODEL_b973d350bcfd4d75b3c4235b1624f798",
      "value": " 4.56k/? [00:01&lt;00:00, 2.72kB/s]"
     }
    },
    "9abf651d6f80423ea448ab06acfac005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d5de35252fd4404a474054f999e950b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da62ff244874ecb94b771df40dd7b64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebc6cb8cbdc4675869035f04166b9d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2daef726e6e48219e7c0d9c62ac962c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d4b7f7914fb42f39c364a92538eea2e",
      "placeholder": "​",
      "style": "IPY_MODEL_113b4c8b0e264313a5abfc5bc3fe85df",
      "value": " 248k/248k [00:00&lt;00:00, 252kB/s]"
     }
    },
    "b17c04a5d62542f1b51ed9a0b32e77bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b973d350bcfd4d75b3c4235b1624f798": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be78ee5fbd55487fb3dd0da23adbc154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc2727ff9c2d401a8008e617ba9ad747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff74f9f5d22b498aaf8f6c3d41c5270c",
      "max": 962,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd1c037bd10a4894a4fc213294dc3a13",
      "value": 962
     }
    },
    "cd1c037bd10a4894a4fc213294dc3a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cd22331b8ece475080b0b102be3522eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5357ac4465374fc79ec8dd5a9c532a49",
      "max": 1745,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d863b911b833435dbb32db1761884539",
      "value": 1745
     }
    },
    "d863b911b833435dbb32db1761884539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ddb9baa5e79c4e5dbfc4cc2a23e79987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd22331b8ece475080b0b102be3522eb",
       "IPY_MODEL_9a28b6e8626841f2a23fd6f31160a2f7"
      ],
      "layout": "IPY_MODEL_7b64d95da47f46c4800e0bba13283693"
     }
    },
    "e38d794604bb409bb4d41a5bfef5eb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4c5ce6351014015aa41d8a43fdca7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430873d31e5c4770afd48183c1a8f1de",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fe1fbff472c435c94c25c1736081373",
      "value": 2
     }
    },
    "e9e10b47884943488ce8fdbaf2f1e9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec3ad8119f13412f8a7b8b00bf275b8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_493174df299e451c95241a6fa0f21b59",
      "max": 248477,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7260a5df06ff44a18e4890109b9b0be9",
      "value": 248477
     }
    },
    "f85b679b2add466ba5230481ffcfa3ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1111e96e637447439c90016ef4184ba7",
      "max": 428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48f64c7b8d164ce7ac7efb8ae7aaaffe",
      "value": 428
     }
    },
    "fb079b9a51a248d9ad0de418ab0476c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b17c04a5d62542f1b51ed9a0b32e77bf",
      "placeholder": "​",
      "style": "IPY_MODEL_9abf651d6f80423ea448ab06acfac005",
      "value": " 2.24k/? [00:00&lt;00:00, 28.6kB/s]"
     }
    },
    "ff74f9f5d22b498aaf8f6c3d41c5270c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
