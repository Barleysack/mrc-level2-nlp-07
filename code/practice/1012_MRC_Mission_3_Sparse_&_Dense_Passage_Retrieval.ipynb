{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDdziEN_VCt"
   },
   "source": [
    "# Passage Retrieval 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm9gQPjOWKki"
   },
   "source": [
    "이번 과제에서는 3강에서 배운 **Sparse Passage Retrieval** 과 4강에서 배운 **Dense Passage Retrieval (DPR)** 을 구현해봅니다. \n",
    "\n",
    "Passage Retrieval 을 다시 복습해보면,\n",
    "1. Query와 Passage 를 임베딩 시킨 후\n",
    "2. 임베딩된 벡터들에 각각 dot product를 수행하여 유사도를 구한 후에\n",
    "3. 유사도가 가장 높은 passage 들을 검색 대상으로 합니다.   \n",
    "\n",
    "이 때 임베딩 시키는 방법에서 Sparse 와 Dense 가 나누어진 점, 다들 기억하시죠?\n",
    "차근차근 구현해본 후, 전체 Wikipedia 에 대해서도 작업해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ug7dthenVCR"
   },
   "source": [
    "```\n",
    "🛠 Setup을 하는 부분입니다. 이전 과제에서 반복되는 부분이기 때문에 무지성 실행 하셔도 좋습니다.\n",
    "💻 실습 코드입니다. 따라가면서 코드를 이해해보세요.\n",
    "❓ 과제입니다. 주어진 질문과 요구사항에 맞춰서 직접 코드를 짜보세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKIvUE3FcdK6"
   },
   "source": [
    "## 🛠 초기설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NWluWk3_VCu"
   },
   "source": [
    "### 🛠 Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGqFS4EEBF_Z",
    "outputId": "0331be65-340d-4196-ebcd-a804705740d9"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm==4.48.0 -q\n",
    "!pip install datasets==1.4.1 -q\n",
    "!pip install transformers==4.5.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHf4ReA9dzyp"
   },
   "source": [
    "### 🛠 난수 고정 및 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:13.948236Z",
     "start_time": "2021-09-15T08:06:12.672812Z"
    },
    "id": "fNEhsdR6hM-6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:13.978267Z",
     "start_time": "2021-09-15T08:06:13.951238Z"
    },
    "id": "sSyEXp19d0L1"
   },
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:14.023236Z",
     "start_time": "2021-09-15T08:06:13.979238Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsayd5Ite-KY",
    "outputId": "0a958d6e-91a9-4699-a3c6-1de4706615fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"% (torch.__version__))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYUkp06Y_VCv"
   },
   "source": [
    "### 🛠 데이터셋 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMrZa4uql_nx"
   },
   "source": [
    "KorQuAD 의 train 데이터를 학습 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:46.968793Z",
     "start_time": "2021-09-15T06:52:28.974382Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "ddb9baa5e79c4e5dbfc4cc2a23e79987",
      "7b64d95da47f46c4800e0bba13283693",
      "cd22331b8ece475080b0b102be3522eb",
      "9a28b6e8626841f2a23fd6f31160a2f7",
      "d863b911b833435dbb32db1761884539",
      "5357ac4465374fc79ec8dd5a9c532a49",
      "b973d350bcfd4d75b3c4235b1624f798",
      "0f36e44064244c2daf1a9fdeee5e7fe4",
      "69b6b6a01c034bf78478509a09ac3513",
      "9da62ff244874ecb94b771df40dd7b64",
      "cc2727ff9c2d401a8008e617ba9ad747",
      "fb079b9a51a248d9ad0de418ab0476c8",
      "cd1c037bd10a4894a4fc213294dc3a13",
      "ff74f9f5d22b498aaf8f6c3d41c5270c",
      "9abf651d6f80423ea448ab06acfac005",
      "b17c04a5d62542f1b51ed9a0b32e77bf",
      "2553f14880194fe3b1a4a5c611de59f3",
      "9d5de35252fd4404a474054f999e950b",
      "925386f42c2b40a09990ade0181559ba",
      "03d20af78d7c44159abca802ac082d1a",
      "e9e10b47884943488ce8fdbaf2f1e9b9",
      "46d61012f6ac4ca189d8890826a27fe5",
      "1f2a869e4b404e68b954eb384ef62d68",
      "0d9e100657ac4554974e990951e43387"
     ]
    },
    "id": "4IUxepuj_VCv",
    "outputId": "ee0defb1-06a9-4666-f3e6-ef728a63a04d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 9606개의 지문이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_kor_v1\")\n",
    "corpus = list(set([example[\"context\"] for example in dataset[\"train\"]]))\n",
    "print(f\"총 {len(corpus)}개의 지문이 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJtECqpB_VCx"
   },
   "source": [
    "### 🛠 토크나이저 준비 - Huggingface 제공 tokenizer 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Fu2WaqpUB8"
   },
   "source": [
    "BERT 를 encoder 로 사용하므로, KLUE에서 제공하는 `klue/bert-base` tokenizer 를 활용해봅시다. 다른 pretrained 모델을 사용하고 싶으시다면, `model_checkpoint`를 바꿔보세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.152836Z",
     "start_time": "2021-09-15T06:52:46.970795Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213,
     "referenced_widgets": [
      "697798699f354b3bb87a4265f545dcbf",
      "9ebc6cb8cbdc4675869035f04166b9d7",
      "f85b679b2add466ba5230481ffcfa3ab",
      "4c832fc7a4ca4965bd37b68a1ae2c736",
      "48f64c7b8d164ce7ac7efb8ae7aaaffe",
      "1111e96e637447439c90016ef4184ba7",
      "49e6ffd8000644eda4157dcb48829984",
      "0be86d05d64240b887902fb7624c474d",
      "05a482882c4f4e1b9b85150a3eed2293",
      "5bfdfadc746c4a2ca44af7da3bfc4ab9",
      "ec3ad8119f13412f8a7b8b00bf275b8b",
      "a2daef726e6e48219e7c0d9c62ac962c",
      "7260a5df06ff44a18e4890109b9b0be9",
      "493174df299e451c95241a6fa0f21b59",
      "113b4c8b0e264313a5abfc5bc3fe85df",
      "3d4b7f7914fb42f39c364a92538eea2e"
     ]
    },
    "id": "AoB8BHGDmVIK",
    "outputId": "f088e681-4a67-489e-e052-597640fffe86"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X48czRQwcxPu"
   },
   "source": [
    "불러온 Tokenzier가 잘 작동하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.182781Z",
     "start_time": "2021-09-15T06:52:56.153749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0U7sn3jsu44O",
    "outputId": "ac54c22a-518c-4536-ef7c-176ca02c260c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. '\n",
      " '이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 [UNK] 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 '\n",
      " '심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 '\n",
      " '받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 '\n",
      " '라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 '\n",
      " '받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 '\n",
      " '완성과 동시에 그는 이 서곡 ( 1악장 ) 을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. '\n",
      " '결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 '\n",
      " '방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 '\n",
      " '의견도 있다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(\n",
    "    dataset[\"train\"][0][\"context\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "pprint(tokenizer.decode(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGPpy1Hpd_7m"
   },
   "source": [
    "## 💻 Ⅰ. Sparse Retriever 실습\n",
    "첫 번째로 TF-IDF 를 통해 임베딩 벡터를 만들어봅시다. 이 모듈은 직접 구현할 필요 없이 `sklearn.feature_extract.text` 에서 구현된 것을 사용합시다!\n",
    "\n",
    "더 간단하게 임베딩 벡터를 구할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCeRbHPvh8UB"
   },
   "source": [
    "### 💻 1. TF-IDF 학습하기\n",
    "TF-IDF 사용법은 sklearn 홈페이지에서 확인할 수 있습니다. 제공된 링크를 읽어보시면, 아래에 작성된 코드를 쉽게 이해하실 수 있을 거에요.\n",
    "\n",
    "\n",
    "*   [TF-IDF 공식 홈페이지](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "*   [TF-IDF 사용 예시](https://wikidocs.net/31698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:52:56.197781Z",
     "start_time": "2021-09-15T06:52:56.183749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WSQWisY2fUgV",
    "outputId": "74446c3a-eb8e-464b-888b-8c0590e0b400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ 기존 문장 ------------------------------\n",
      "('일본에서는 구미권의 크리스트교에 의한 사체의 부활신앙이나 그에 따라 존재한 화장의 금기·저항감과 같은 개념은 부족한 경향이 있다. 또 '\n",
      " '에도기에는 마차가 존재하지 않았고, 만약 만일 여행지나 먼 봉사할 곳에서 급사자가 나오고, 그 사체를 원격지에 반송한다면 실질적으로는 '\n",
      " '나가모치 등을 이용해 인력에 의지하지 않았고, 일반 서민의 레벨에서는 사체를 그대로의 모습으로 장거리 수송한다는 생각도 선택사항도 '\n",
      " '존재하지 않았다. 이 생각은 구미인에 의해서 마차와 견인용의 중종마가 반입된 에도막부 말기부터 메이지기, 그리고 동력 근대화가 진행된 '\n",
      " '메이지 후기 이후도 본질적으로는 너무 변함없이, 전시중도 전사자는 현지에서 화장되어 전후도 또 오랫동안, 다수의 사망자가 발생한 재해나 '\n",
      " '사고에서는 현지에서 화장허가를 얻어 서서히 사체를 화장함에 교부하고 유골을 가지고 돌아간다는 형태가 일반적이었다. 오랫동안 매장습관이 '\n",
      " '남아 있던 지역도 많지만, 이것들에서도 화장도 완전히는 부정되지 않고, 화장의 기술의 진보나 시설의 도입에 의해서 근현대에 급속히 매장이 '\n",
      " '쇠퇴했다. 그 같은 일로부터, 일본에서는 구미권과 같은 유체보존기술의 습관이 퍼지지 않았다.')\n",
      "\n",
      "------------------------------ Tokenize 된 문장 ------------------------------\n",
      "('일본 ##에서 ##는 구미 ##권 ##의 크리스 ##트 ##교 ##에 의한 사체 ##의 부활 ##신 ##앙 ##이나 그 ##에 따라 존재 '\n",
      " '##한 화장 ##의 금기 · 저항 ##감 ##과 같 ##은 개념 ##은 부족 ##한 경향 ##이 있 ##다 . 또 에도 ##기에 ##는 '\n",
      " '마차 ##가 존재 ##하 ##지 않 ##았 ##고 , 만약 만일 여행지 ##나 먼 봉사 ##할 곳 ##에서 급사 ##자 ##가 나오 '\n",
      " '##고 , 그 사체 ##를 원격 ##지 ##에 반 ##송 ##한다 ##면 실질 ##적으로 ##는 나가 ##모 ##치 등 ##을 이용해 '\n",
      " '인력 ##에 의지 ##하 ##지 않 ##았 ##고 , 일반 서민 ##의 레벨 ##에서 ##는 사체 ##를 그대로 ##의 모습 ##으로 '\n",
      " '장거리 수송 ##한다 ##는 생각 ##도 선택 ##사항 ##도 존재 ##하 ##지 않 ##았 ##다 . 이 생각 ##은 구미 ##인 '\n",
      " '##에 의해서 마차 ##와 견인 ##용 ##의 중종 ##마 ##가 반입 ##된 에도 ##막 ##부 말기 ##부터 메이지 ##기 , 그리고 '\n",
      " '동력 근대 ##화가 진행 ##된 메이지 후기 이후 ##도 본질 ##적으로 ##는 너무 변함없이 , 전시 ##중 ##도 전사자 ##는 현지 '\n",
      " '##에서 화장 ##되 ##어 전후 ##도 또 오랫동안 , 다수 ##의 사망자 ##가 발생 ##한 재해 ##나 사고 ##에서 ##는 현지 '\n",
      " '##에서 화장 ##허가 ##를 얻어 서서히 사체 ##를 화장 ##함 ##에 교부 ##하고 유골 ##을 가지 ##고 돌아간다 ##는 형태 '\n",
      " '##가 일반 ##적 ##이 ##었 ##다 . 오랫동안 매장 ##습관 ##이 남아 있 ##던 지역 ##도 많 ##지만 , 이것 ##들 '\n",
      " '##에서 ##도 화장 ##도 완전히 ##는 부정 ##되 ##지 않 ##고 , 화장 ##의 기술 ##의 진보 ##나 시설 ##의 도입 '\n",
      " '##에 의해서 근현 ##대 ##에 급속히 매장 ##이 쇠퇴 ##했 ##다 . 그 같 ##은 일 ##로 ##부터 , 일본 ##에서 ##는 '\n",
      " '구미 ##권 ##과 같 ##은 유체 ##보 ##존 ##기술 ##의 습관 ##이 퍼지 ##지 않 ##았 ##다 .')\n"
     ]
    }
   ],
   "source": [
    "# Huggingface의 Tokenizer를 사용하셔도 좋고\n",
    "tokenizer_func = lambda x: tokenizer.tokenize(x)\n",
    "\n",
    "# 혹은 단순 띄어쓰기 기준으로 Tokenize 하셔도 좋습니다.\n",
    "# tokenizer_func = lambda x: x.split(' ')\n",
    "\n",
    "# 어떻게 Tokenize 되었는지 확인해봅시다.\n",
    "print(f\"{'-'*30} 기존 문장 {'-'*30}\")\n",
    "pprint(corpus[20])\n",
    "print(f\"\\n{'-'*30} Tokenize 된 문장 {'-'*30}\")\n",
    "pprint(\" \".join(tokenizer_func(corpus[20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LZR8COph2g9"
   },
   "source": [
    "모듈을 활용해서 `fit` 해봅시다. 과정은 어렵지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:20.560497Z",
     "start_time": "2021-09-15T06:52:56.198781Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyQ290Cdd64F",
    "outputId": "eaea66d8-78d9-4186-8e62-2782b03bf210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenizer_func,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "vectorizer.fit(corpus)\n",
    "sp_matrix = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4sPODsJ76xm"
   },
   "source": [
    "`Vectorizer`로 임베딩을 시켜주면 (문서의 개수, 단어의 개수) 꼴의 행렬로 변환이 됩니다. 기본적으로 단어의 개수는 지정해주지 않으면 전체 단어의 개수만큼 차원이 지정됩니다. 사용되는 단어의 개수가 너무 많아서 지나치게 행렬이 희소해지면 사용이 불편하기 때문에, 필요하다면 벡터 임베딩 사이즈 또한 지정해줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4nh6x8o9KCP"
   },
   "source": [
    "참고로 결과물이 희소행렬이기 때문에 평소에 사용되는 `numpy.ndarray`가 아닙니다. Scipy 모듈의 csr_matrix를 이용하는데 이는 아래 링크를 참고해주세요\n",
    "+ [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "+ [Scipy sparse matrix handling](https://lovit.github.io/nlp/machine%20learning/2018/04/09/sparse_mtarix_handling/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBuC_c-e9PHF",
    "outputId": "ff929cb4-9595-4cb4-fef0-43bfe60a77d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sp_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:20.575499Z",
     "start_time": "2021-09-15T06:53:20.562498Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxHJP5HHfxFm",
    "outputId": "366403a1-2cab-4b88-eb1d-b780d99e352f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9606, 684272)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_matrix.shape # (num_passage, num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxSvTRPFiH0Z"
   },
   "source": [
    "첫 번째 문장의 TF-IDF 벡터를 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.609083Z",
     "start_time": "2021-09-15T06:53:20.577499Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nf0fNy11fzXY",
    "outputId": "0ed450ad-e87e-43de-cda3-0e812a7dd863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF-IDF\n",
      "그        0.153808\n",
      "고비       0.118545\n",
      "##어 .    0.118545\n",
      "##째 고비   0.105799\n",
      "구조 대원    0.105799\n",
      "##였 ##어  0.105799\n",
      "응급 구조    0.105799\n",
      "그 ##는    0.102659\n",
      "그 때      0.102603\n",
      "고비 ##는   0.101273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    sp_matrix[0].T.todense(),\n",
    "    index=vectorizer.get_feature_names(),\n",
    "    columns=[\"TF-IDF\"]\n",
    ")\n",
    "df = df.sort_values(\"TF-IDF\", ascending=False)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgjN_SF7iM_N"
   },
   "source": [
    "### 💻 2. Query 임베딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEdiT1W78X5y"
   },
   "source": [
    "이제 Query 를 임베딩해봅시다. 아까 사용한 `vectorizer`를 이용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.624082Z",
     "start_time": "2021-09-15T06:53:21.611084Z"
    },
    "id": "sXSOfYisf5ad"
   },
   "outputs": [],
   "source": [
    "sample_idx = random.choice(range(len(dataset[\"train\"])))\n",
    "\n",
    "query = dataset[\"train\"][sample_idx][\"question\"]\n",
    "ground_truth = dataset[\"train\"][sample_idx][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:53:21.654082Z",
     "start_time": "2021-09-15T06:53:21.626084Z"
    },
    "id": "dw0T5Drrf658"
   },
   "outputs": [],
   "source": [
    "query_vec = vectorizer.transform([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXpNKTYSia9D"
   },
   "source": [
    "### 💻 3. Dot Product 를 통해 유사도 구하기\n",
    "내적을 통해 주어진 Query 와 전체 Passage 사이의 유사도를 구해봅시다.\n",
    "그리고 값을 내림차순으로 나열하여 높은 점수를 가진 Passage 들을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:25.265288Z",
     "start_time": "2021-09-15T06:54:25.046289Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLsck2Lyf_Vm",
    "outputId": "3e940842-bc2e-4023-aaa1-cc6645d58468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9606)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = query_vec * sp_matrix.T\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:26.190604Z",
     "start_time": "2021-09-15T06:54:26.172592Z"
    },
    "id": "kLRrldUef8Cw"
   },
   "outputs": [],
   "source": [
    "sorted_result = np.argsort(-result.data)\n",
    "doc_scores = result.data[sorted_result]\n",
    "doc_ids = result.indices[sorted_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:26.555129Z",
     "start_time": "2021-09-15T06:54:26.541098Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDYgaL3wgBC6",
    "outputId": "bc810b50-e352-4391-da83-cc29b7b4dc88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.21208908, 0.07221365, 0.0677442 , 0.05575468, 0.05302485]),\n",
       " array([6654, 1344, 2489, 3216,  863], dtype=int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "doc_scores[:k], doc_ids[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pin40NMV91-l"
   },
   "source": [
    "잘 뽑았는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:27.343209Z",
     "start_time": "2021-09-15T06:54:27.333696Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e-OSr2tgCGt",
    "outputId": "b018af21-6206-4404-ed18-867515da9686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " 구식 군인들의 월급인 쌀에 모래와 돌멩이가 들어가있던 사건을 말미암아 일어난 사태의 이름은? \n",
      "\n",
      "[Ground truth passage]\n",
      "1882년 6월 민영익의 귀국 권고로 일시 귀국했다가 다시 되돌아갔다. 7월 23일 한성부에서 구식 군인들의 월급으로 주는 쌀에 모래와 돌멩이 및 썩은 쌀을 주자 여기에 반발한 구식 군인들에 의해 임오군란이 일어났는데, 그는 임오군란 사태 당시 행동을 삼가고 사태의 추이를 예의주시하고 있었다. 그러다가 그해 10월 13일 박영효, 김옥균 일행이 수신사(修信使) 겸 사죄사(辭罪使)로 하는 사절단(使節團)이 파견되자, 그는 박영효와 김옥균 일행이 도쿄에 방문했을 때 사절단의 통역을 맡아보았다. 수신사는 3개월간 일본의 각 기관을 시찰하고 일본의 여·야당 정치 지도자들과 만나 면담하고 각국 사절과도 폭넓게 접촉하여 의견을 교환했다. 한학을 배워 중국어와 일본어의 기초 실력이 있었던 그는 사절단의 통역을 맡아 활약했으며, 박영효 등이 귀국할 때 일본 유학을 마치고 박영효 일행과 함께 귀국했다. \n",
      "\n",
      "Top-1 passage with score 0.2121\n",
      "1882년 6월 민영익의 귀국 권고로 일시 귀국했다가 다시 되돌아갔다. 7월 23일 한성부에서 구식 군인들의 월급으로 주는 쌀에 모래와 돌멩이 및 썩은 쌀을 주자 여기에 반발한 구식 군인들에 의해 임오군란이 일어났는데, 그는 임오군란 사태 당시 행동을 삼가고 사태의 추이를 예의주시하고 있었다. 그러다가 그해 10월 13일 박영효, 김옥균 일행이 수신사(修信使) 겸 사죄사(辭罪使)로 하는 사절단(使節團)이 파견되자, 그는 박영효와 김옥균 일행이 도쿄에 방문했을 때 사절단의 통역을 맡아보았다. 수신사는 3개월간 일본의 각 기관을 시찰하고 일본의 여·야당 정치 지도자들과 만나 면담하고 각국 사절과도 폭넓게 접촉하여 의견을 교환했다. 한학을 배워 중국어와 일본어의 기초 실력이 있었던 그는 사절단의 통역을 맡아 활약했으며, 박영효 등이 귀국할 때 일본 유학을 마치고 박영효 일행과 함께 귀국했다. \n",
      "\n",
      "Top-2 passage with score 0.0722\n",
      "당시 백신시장에 개인용 백신은 무료배포가 많았던 시기라 개인사용자들에 한해서 1989년 부터 도스용 백신 소프트웨어인 V3+ 네오라는 백신을 무료로 제공하였다. 이후 V3+ 네오는 시그니처 수의 증가로 인해 당시 가장 보편적인 저장매체인 3.5인치 디스켓 2장이 필요하는 등 실제 이용이 매우 어렵고 제한되게 되었다. 결국 한동안 무료백신에 크게 신경을 쓰지 않던 안연구소는 알약 등의 경쟁 무료 제품의 확산을 막기 위해 빛자루 제품을 유료에서 무료로 바꾸고 V3 라이트라는 무료 제품을 연이어 출시한다. V3+ 네오는 V3 라이트 출시 이후 단종되었다. 안철수는 사업에 대해서 잘 모르는 상태에서 시작했기 때문에 처음 4년 간은 많은 고생을 했다. 당시 안철수연구소의 월급날은 매월 25일이었는데 월초부터 직원들의 월급 걱정을 해야 하는 지경이었고 자신이 월급을 받지 않고 직원들의 월급을 줄 때도 있었다. \n",
      "\n",
      "Top-3 passage with score 0.0677\n",
      "사구(砂丘)는 바람에 의하여 모래가 이동하여 퇴적된 언덕이나 둑 모양의 모래 언덕이다. 내륙 사구는 고비 사막이나 사하라 사막과 같이 대륙 내부의 사막에 흔히 이루어진다. 사구는 한 장소에 고정되지 않고 독특한 모양을 유지하면서 바람이 부는 쪽으로 이동하는 경우가 많다. 사구 사이사이에는 기반암이나 자갈층이 드러난 경우도 있고, 넓은 지면이 모두 사구로 덮인 경우도 있다. 장애물이 바람에 가로놓여 있으면 바람그늘 쪽에는 풍속이 줄어들어 모래가 잘 쌓인다. 모래알이 장애물의 바람그늘 쪽에 쌓인 모래 위로 떨어지면 이동 속도가 줄어들어 모래가 계속 해서 집적하게 된다. 모래 더미가 원래의 장애물에 비하여 너무 크게 성장하면 다시 천천히 움직이면서 이동성 사구로 발전한다.한편, 해안 사구는 바닷물의 물결을 따라 바닷가에 밀려온 모래가 사빈으로 퇴적되었다가 다시 바다로부터 불어오는 바람에 실려가 사빈의 뒷쪽에 쌓여 생긴 것으로, 대개 해안선과 나란히 생긴다. 모래가 육지 쪽으로 너무 많이 날려가면 농경지가 묻히기 때문에, 이와 같은 해안의 주민들은 방풍림이나 방사림을 조성하여 모래의 이동을 막고 있다. \n",
      "\n",
      "Top-4 passage with score 0.0558\n",
      "그리고 이러한 만민공동회 활동과 관련하여, 독립협회에서 만민공동회에 사주하여 결의한 헌의 6조에 배치되는 활동이 자주 나타났고, 그에 따라 황실에서는 독립협회를 탄압하려고 했다. 이러한 독립협회의 활동이 일본이나 미국에 유리한 조건을 조성해 주었고, 오히려 황실에서 추진하는 광무개혁이 일본이나 미국이 추진하는 경제적 침탈에 불리하게 작용하자, 그들은 황실의 만민공동회 탄압 계획을 번번이 저지하였다. 그들은 또한 독립협회 활동의 폭력화로 말미암아 무정부상태가 되는 일도 바라지 않았다. 그들이 생각하는 최악의 사태는 무정부상태로 말미암아 고종이 제2의 아관파천을 하는 일이었다. 미국과 일본은 그것을 막기 위해 독립협회에 대한 탄압은 억제하면서, 박영효 세력을 저지하는 한편 무력 진압을 양해하였다. 그 결과 독립협회의 활동으로 말미암아 오히려 외세 의존성을 심화하게 된다. \n",
      "\n",
      "Top-5 passage with score 0.0530\n",
      "16개의 보 중에서 총 14개의 보를 2017년 11월 13일 수문을 개방했다. 이후 약 6개월이 지난 2018년 5월 4일 금강 세종보 상류는 수위가 낮아지면서 모래와 자갈로 이뤄진 작은 섬들이 드넓게 드러나기 시작했으며 좌안에는 모래가 30cm 이상 쌓이고 강물의 유속도 빨라지는 등 변화가 감지되었다. 현장 조사를 시행한 오준오 박사는 \"보 개방으로 실트층이 하류로 씻겨 내려가면서 자갈이 다시 드러났고, 모래가 쌓였다\"고 설명했다. 하지만 금강 하류의 백제보가 수문을 개방하지 않아 물빛이 세종보보다 탁했고 모래 대신 뻘만 가득하여 악취가 심한 등 이전과 차이가 없었다. 이는 낙동강도 마찬가지였다. 환경부 관계자는 \"보의 개방은 수질과 구조적 건강성, 생물학적 건강성 등 세 가지 측면에서 모두 긍정적 결과가 나타났다. 올해 말까지 개방한 보들에 대해 평가하고 처리 방안을 결정할 계획\"이라고 밝혔다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Top-{i + 1} passage with score {doc_scores[i]:.4f}\")\n",
    "    doc_id = doc_ids[i]\n",
    "    print(corpus[doc_id], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DirOwPFOixwg"
   },
   "source": [
    "Ground Truth 와 동일한 Passage 가 잘 나왔나요? 제법 성능이 나쁘지 않은걸 확인할 수 있네요.   \n",
    "### ❓ 제시된 두 Tokenizer 를 모두 활용해서 결과를 비교해볼까요?\n",
    "위에서 제시된 Hugginface Tokenizer 와 단순 띄어쓰기로 구분된 Tokenizer 는 많은 차이를 불러올까요? 직접 코드로 구현해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl9vDzADcnPu"
   },
   "source": [
    "### ❓ SparseRetrieval 코드를 class로 합쳐봅시다.\n",
    "Sparse Retrieval 을 잘 구현하셨나요? 하지만 코드가 너무 흩어져있어서 다시 재사용하기 많이 어렵겠네요. 이 코드들을 다른 사람들도 잘 활용할 수 있도록 모듈화를 진행해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34DcdPHglRIr"
   },
   "source": [
    "❗ _Hint_   \n",
    "Scratch 부터 짜는게 많이 어려우신가요?\n",
    "\n",
    "만들어야하는 기능들의 파이프라인을 나열하고 하나씩 모듈화하는 습관을 들이는 것이 좋습니다. 순서대로 생각해볼까요?\n",
    "\n",
    "1. 데이터를 불러옵니다.\n",
    "2. TF-IDF 를 Fitting 한 후 Passage 를 Transform 합니다.\n",
    "3. Query 를 Fitting 한 TF-IDF 를 통해 Transform 합니다.\n",
    "4. Inner dot product 를 이용해 유사도를 구하고, 내림차순을 통해 유사한 Passage 를 Retrieval 합니다.\n",
    "\n",
    "위의 단계를 하나씩 함수로 구현한 후 class로 합치면 훨씬 쉬울 겁니다! 반드시 이 과정을 따르지 않으셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Jr-IgCXWGysH"
   },
   "outputs": [],
   "source": [
    "# 화이팅\n",
    "class TfIdfRetrieval:\n",
    "\n",
    "    def __init__(self, tokenize_fn, data_path):\n",
    "\n",
    "        \"\"\"\n",
    "        1. 여기서 Data를 불러올까요.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "        texts = [wiki[str(i)] for i in range(len(wiki))]\n",
    "        self.contexts = list(set([v[\"text\"] for v in texts]))\n",
    "        self.sp_matrix = None\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn,\n",
    "            ngram_range=(1,2)\n",
    "            )\n",
    "\n",
    "    def get_sparse_embedding(self):\n",
    "\n",
    "        \"\"\"\n",
    "        2. 여기서 주어진 전체 passage들에 대해 TF-IDF를 .fit 해줍시다.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vectorizer.fit(tqdm(self.contexts))\n",
    "        self.sp_matrix = self.vectorizer.transform(tqdm(self.contexts))\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1):\n",
    "\n",
    "        \"\"\"\n",
    "        여기서\n",
    "            3. Query를 받아서 TF-IDF에 Transform 시켜줍니다.\n",
    "            4. 전체 Passage에 대한 유사도를 구한 후 상위 k개의 Passage를 반환합니다.\n",
    "        \"\"\"\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        result = query_vec * self.sp_matrix.T\n",
    "        sorted_result = np.argsort(-result.data)\n",
    "        doc_scores = result.data[sorted_result]\n",
    "        doc_ids = result.indices[sorted_result]\n",
    "        return doc_scores[:k], doc_ids[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_2Vvx8BdzXw"
   },
   "source": [
    "잘 구현하셨나요? 여러분들이 만든 클래스를 아래와 같이 활용하는데 성공하셨다면 마무리된 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:54:39.464565Z",
     "start_time": "2021-09-15T06:54:38.837869Z"
    },
    "id": "-W8Tl7osdyf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56737/56737 [02:06<00:00, 449.96it/s]\n",
      "100%|██████████| 56737/56737 [02:06<00:00, 447.88it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_fn = lambda x: tokenizer.tokenize(x)\n",
    "retriever = TfIdfRetrieval(tokenize_fn=tokenize_fn, data_path=\"../../data/wikipedia_documents.json\")\n",
    "\n",
    "retriever.get_sparse_embedding()\n",
    "query = \"미국의 대통령은 누구인가?\"\n",
    "doc_score, doc_indices = retriever.get_relevant_doc(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "IXorfdV1hHGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search Query] 미국의 대통령은 누구인가?\n",
      "Top-1th Passage (Index 50000)\n",
      "('2000년 개헌에 따라 핀란드 대통령은 권한은 많이 축소됐다. 핀란드 대통령은 내각과 함께 대외 정책을 이끈하다. 하지만 유럽정책은 '\n",
      " '총리의 소관이다. 대통령은 국내 문제에 대한 권한이 거의 없다. 대통령에게 의회 해산권이 있지만, 총리의 요청에 따라 해산할 수 있다. '\n",
      " '또 대통령은 법안에 대한 인준을 결정할 수 없다. 단지 대통령은 법안을 의회로 다시 돌려보낼 수 있고, 의회는 재의결을 통해 대통령의 '\n",
      " '서명없이 법률을 공표할 수 있다. 대통령이 임명할 수 있는 공직의 수가 줄어 들기는 했지만, 군장성과 판사의 임명권은 여전히 대통령에게 '\n",
      " '있다. 대통령은 핀란드 방위군 통수권자이며, 사면권을 가지고 있다.\\n'\n",
      " '    \\n'\n",
      " '\\n'\n",
      " '2000년 개헌에 따른 권한 축소 이후, 대통령은 상징적 지도자로 받아들여지고 있다.')\n",
      "Top-2th Passage (Index 14332)\n",
      "('미국의 대통령 가족(First Family of the United States)은 미국의 국가 원수이자 정부 수반인 미국의 대통령 '\n",
      " '가족을 일컫는 비공식적인 명칭이다. 대통령 가족은 대통령과 미국의 대통령 부인, 그리고 그들의 자녀들로 구성된다. 그러나, 대통령과 '\n",
      " '대통령 부인의 가까운 친척들인 부모, 손자, 의붓자식 등이 백악관 단지의 대통령 관저에 거주할 경우에는 그들도 대통령 가족에 포함될 수 '\n",
      " '있다.\\n'\n",
      " '\\n'\n",
      " '미국에서 대중 매체나 특히, 백악관 기자단에서 가장 자주 사용하는 \"대통령 가족\"(First Family)이라는 용어는 일반적으로 '\n",
      " '대통령의 직계 가족을 가리키는 말이다. 개별적으로, 대통령 가족의 각 구성원은 미국 비밀 경호국에서 비밀 경호 코드네임으로 지정된다. '\n",
      " '특수 요원들이 사용하는 이러한 코드네임은 간결성, 명확성, 전통 등을 위할 뿐만 아니라 대통령 가족의 지속적인 보호를 위해 특별하게 '\n",
      " '식별된다.\\n'\n",
      " '\\n'\n",
      " '수 대에 걸쳐 대통령 가족을 배출한 가문은 미국의 명문 대통령 가문으로 불린다.')\n",
      "Top-3th Passage (Index 43467)\n",
      "('R. J. 셰퍼는 목격자의 증언을 검증하는 점검 목록을 제공한다. \\n'\n",
      " '\\n'\n",
      " '# 저술이 실제로 의미하는 것이 문자 그대로의 의미와 다른 것인가 ? 언어는 현재 사용되는 의미와 다른 것인가 ? 문장이 풍자적이지는 '\n",
      " '않은가 ? (즉, 말하는 것과 다른 의미를 갖고 있는 것은 아닌가 ?)\\n'\n",
      " '# 저자는 보고하는 사항을 어떻게 관찰하였나 ? 저자가 느끼는 것과 관찰 대상은 같은 것인가 ? 저자가 보고, 듣고, 만져보기에 적당한 '\n",
      " '위치에 있었나 ? 저자는 적절한 사회적 관찰 능력을 갖고 있는가 ? 즉, 그 언어를 이해할 수 있는가 ? 그 외에 전문적인 지식이 필요한 '\n",
      " '것은 아닐까 ? (예를 들면 법률이나 군사) 저자는 배우자나 비밀경찰에 위협받고 있지 않았나 ?\\n'\n",
      " '# 저자는 어떻게 기록하였나 ? 저자의 기록 능력은 어떠한가 ?\\n'\n",
      " '## 기록 능력과 관련하여, 저자는 편견을 갖고 있지는 않은가 ? 저자는 기록 작성을 위해 충분한 시간이 있었나 ? 기록에 적합한 장소가 '\n",
      " '있었나 ? 적절한 기록 용구가 있었나 ?\\n'\n",
      " '## 저자가 관찰했을 때부터 기록했을 때까지의 시간은 ? 꽤 시간이 지나지는 않았나 ?\\n'\n",
      " '## 저자가 보고하려는 의도는 ? 누구를 위해 보고하였나 ? 그 때에 주변의 인물이 왜곡을 요구하거나 권장하지는 않았는가 ?\\n'\n",
      " '## 의도하였던 진실성에 외부의 관여가 없었나 ? 보고 사항에 무관심하여 의도하지 않은 왜곡의 가능성은 없는가 ? 저자 자신이 손해가 '\n",
      " '되는 내용을 적게 되어 왜곡할 가능성이 있지는 않았나 ? 저자는 우연으로 또는 항상 정보가 주어져 의도적으로 오류를 유발할 가능성은 '\n",
      " '없었는가 ?\\n'\n",
      " '# 저자의 언급은 본질적으로 현실성이 결여되어 있지는 않나 ? 즉, 인간성에 반하거나 일반 상식에 배치(背馳)되지 않는가 ?\\n'\n",
      " '# 정보의 유형에 따라 관찰과 보고가 쉬운 경우가 있음을 명심한다.\\n'\n",
      " '# 글 내부적으로 모순이 있지 않은가 ?\\n'\n",
      " '\\n'\n",
      " \"루이스 곳샬크는 추가적으로 생각할 사항으로, '해당 사실이 널리 알려지지 않았더라도, 사안에 따라서는 흔히 일어나거나 충분히 가능한 \"\n",
      " \"일이어서 오류나 거짓이 있을 것 같지 않은 경우도 있다'는 것을 지적하였다. \\n\"\n",
      " '\\n'\n",
      " \"갸라한은 대부분의 정보가 '간접적인 목격자'로부터 나옴을 지적하였다. 이들은 해당 장소에 있지 않았으나 다른 이들로부터 그 내용을 \"\n",
      " \"전해들은 것이다.  곳샬크는, '역사가는 이따금 소문에 의한 증거를 사용할 수 있음'을 지적한다. 어쨌거나, 2차 목격자의 정보를 \"\n",
      " '사용하는 경우 이들을 전적으로 신용할 수는 없으며, 오히려 1) 목격자는 어떠한 1차 증언을 근거로 하고 있는지, 2) 2차 목격자가 '\n",
      " '1차 증언을 대체적으로 정확히 보고하는지, 3) 그렇지 못하다면, 목격자는 1차 증언을 얼마나 상세하게 보고하는지 물어보아야 한다는 '\n",
      " '것이다. 그는 2)번과 3)번 질문에서 만족할 만한 대답을 얻었다면 역사가는 1차 증언의 전체 또는 요점을 제공받은 것이며, 이러한 '\n",
      " '경우에는 목격자의 정보에 대하여 1차 증거와 같이 테스트를 한다고 설명하였다.')\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Search Query] {query}\")\n",
    "\n",
    "for i, idx in enumerate(doc_indices):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.contexts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHOE4jZqhXRs"
   },
   "source": [
    "직접 만든 코드에 다음과 같은 추가 사항들을 고려해봅시다.   \n",
    "#### ➕추가 과제: 다양한 기능 추가하기\n",
    "+ 현재 코드가 **어떻게 동작하고 있는지** 궁금하지 않으신가요? 가령, 코드가 돌고는 있는데 대체 어떤 메소드를 수행 중인지, 아니면 시간이 얼마나 걸리는지 등 추가적인 정보를 Logger 에 기록하거나 Prompt 에 찍는 방향으로 더 추가해보세요 !\n",
    "+ 위에서 시간을 출력하는 코드를 짜보셨다면, 이제 fitting 시간이 적지 않음을 확인할 수 있었습니다. 그리고 passage 가 자주 변경되는 것이 아니라면, 같은 기법을 사용해서 **매번 fitting 을 하는 것은 아주 비효율적**입니다. 임베딩된 passage를 따로 **저장해두고 불러와서 사용하면** 더 효율적이지 않을까요? 이 임베딩들을 `.bin` 파일로 저장하고, 만약 `.bin` 파일이 있다면 TfidfVectorizer 가 fitting 하지 않고, 이 `.bin` 파일을 불러오는 방향으로 코드를 추가해봅시다.\n",
    "+ 예외 케이스가 있지 않을까요? **query가 `str` 1개가 아니라 2개 이상인 `List[str]`로 주어진 경우에는 어떻게 해야할까요?** 이 상황은 멀리 있지 않고 여러분들이 대회에서 inference 할 때도 필요한 코드입니다. 단순히 반복문을 돌릴 수 있지만, 우리는 행렬곱을 이용하고 있기 때문에 만약 query 가 여러 개면 이 행렬곱의 장점을 이용할 수 있을 것 같습니다.\n",
    "    + 한 개의 query(`str`) 에 대해 유사한 passage 를 구하는 함수와\n",
    "    + 여러 개의 query(`list`) 에 대해 유사한 passage 를 구하는 함수\n",
    "\n",
    "  두 가지를 만들어봅시다. 그리고 `retrieve` 라는 함수를 만들어서 query 가 한 개인지 다수인지 체크한 후 각각의 함수를 사용해서 유사한 passage 를 찾도록 수정해봅시다.\n",
    "+ 또 다른 예외케이스는 `TfidfVectorizer` 에서 생길 수 있습니다. 가령 Passage 에서 한 번도 보지 못한 단어로만 구성된 query 를 입력했다면, TF-IDF 의 특성상 이 단어는 0 으로만 임베딩될 것입니다. 전체 벡터가 0이 되면 어떻게 처리해야할까요? `assert` 를 활용해보세요!\n",
    "+ 이 메소드는 어떤 parameters를 받아오고 어떤 기능을 하는지 docstring을 추가해봅시다.\n",
    "\n",
    "\n",
    "요구사항이 너무 많나요? 원래 인생이 그렇습니다.\n",
    "\n",
    "_Sparse Retrieval 과제는 두 가지 (기존과 추가 과제) 모두 예시 코드가 금요일 (10/15)제공됩니다._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpoTleVJjp5x"
   },
   "source": [
    "## 💻 Ⅱ. Dense Retriever (BERT) 학습 시키기\n",
    "\n",
    "이번에는 BERT 를 불러오고 학습시키는 과정을 거쳐봅시다. 시작하기 전에 어떤 과정을 거쳐야할 지 생각해봅시다.\n",
    "1. 데이터셋 준비\n",
    "2. 모델 준비\n",
    "3. 학습하기 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmAIDfEbeENm"
   },
   "source": [
    "### 💻 1. 데이터셋 준비하기\n",
    "위에서 불러온 `datasets`를 이용해 데이터를 불러왔습니다. 하지만 이번 과제에서 passage encoder 가 학습되는 방식은 주어진 query/question에 적합한 passage 를 찾아오는 과정이 필요한데, 이 때 올바른 데이터만 이용해서 학습하는 것이 아니라, 올바르지 않은 passage 또한 학습해보는 과정을 거쳐야합니다. 이를 in-batch negative 라고 부르는데 관련된 내용은 기계독해 강의와 아래 논문을 참고해보세요.\n",
    "+ [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-bKwkxTpoje"
   },
   "source": [
    "#### 💻 Training Dataset 준비하기 (question, passage pairs)\n",
    "\n",
    "실습 시간을 단축시키기 위해 일부 데이터만 활용해봅시다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.543616Z",
     "start_time": "2021-09-15T06:56:49.543616Z"
    },
    "id": "E_FQ1kcazxge"
   },
   "outputs": [],
   "source": [
    "sample_idx = np.random.choice(range(len(dataset[\"train\"])), 20)\n",
    "training_dataset = dataset[\"train\"][sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALNnnBJTxeU4"
   },
   "source": [
    "#### 💻 Negative sampling 을 위한 negative sample 들을 샘플링\n",
    "\n",
    "주어진 query/question 에 해당하지 않는 지문들을 뽑아서 훈련데이터로 넣어줍시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.545616Z",
     "start_time": "2021-09-15T06:56:49.545616Z"
    },
    "id": "LPomXJ1Afc6l"
   },
   "outputs": [],
   "source": [
    "# In-batch Negatvie로 사용할 데이터 생성\n",
    "num_neg = 2\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "p_with_neg = []\n",
    "\n",
    "for c in training_dataset[\"context\"]:\n",
    "    \n",
    "    while True:\n",
    "        neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "        if not c in corpus[neg_idxs]:\n",
    "            p_neg = corpus[neg_idxs]\n",
    "\n",
    "            p_with_neg.append(c)\n",
    "            p_with_neg.extend(p_neg)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir_hYkQ5fBro"
   },
   "source": [
    "주어진 질문에 알맞는 지문과 올바르지 않는 지문을 모두 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T06:56:49.546616Z",
     "start_time": "2021-09-15T06:56:49.546616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbGW7PRJ7Yv5",
    "outputId": "7bc3b3ac-5f88-4c0f-fa07-76bf21d9b148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query Given]\n",
      "'문제는 나랏법 자체보다는 법을 적용하고 옹호하는 데 있었다고 묘사한 사학자는?'\n",
      "\n",
      "[Positive context]\n",
      "('전원 공동체, 특히 그 중에서도 잉글랜드 동남부는 농노제의 작동과 지역 장원 재판소의 세금 징수를 불만스러워했다. 특히 재판소를 운영하는 '\n",
      " '영주들이 곧 원성을 사는 노동 조례나 왕실에서 제정한 법의 현장 집행자로 기능하는 경우가 잦았기 때문에 불만은 가중되었다. 촌락의 엘리트 '\n",
      " '계층 다수는 지방정부의 역할을 맡는 것을 거부했고, 재판소의 활동을 방해하기 시작했다. 재판소에 의해 몰수된 가축들은 그 주인들에 의해 '\n",
      " '‘해방’되었고, 법관들은 폭행을 당했다. 일부는 전통적인 법을 존중하나 런던에서 내려오는 증오받는 중앙 법률에서는 분리된, 독립적인 촌락 '\n",
      " '공동체의 탄생을 지지하기 시작했다. 사학자 미리 루빈은 이 상황을, “문제는 나랏법 그 자체라기보다, 그 법을 적용하고 옹호하는 데 '\n",
      " '있었다”고 묘사한다.')\n",
      "\n",
      "[Negative context]\n",
      "'유럽찌르레기 떼는 해충 구제에 도움이 되어 농업에 유익하다. 그러나 유럽찌르레기가 열매나 싹을 뜯어먹으면서 오히려 해수가 될 수도 있다. 유럽찌르레기는 도시 지역에서는 수탉처럼 횃소리를 내는데 이것이 매우 시끄럽고 어지러워서 사람을 성가시게 만든다. 외래종으로 침투된 지역들에서는 도태 작업을 비롯한 각종 활동을 통해 개체수를 조절하려 하고 있으나, 웨스턴오스트레일리아를 제외한 다른 지역들에서는 그러한 시도들은 거의 성공하지 못했다. 1980년대 이후 북유럽과 서유럽에서는 그 개체수가 감소 추세에 있다. 성장기 병아리의 먹이로 삼아야 하는 초지의 무척추동물이 드물어졌기 때문이다. 그럼에도 불구하고 전세계적인 개체수는 그다지 크게 줄어들지 않고 있으며, 국제자연보전연맹은 유럽찌르레기를 관심 불필요(least concern) 종으로 분류했다.'\n",
      "\"그의 영화 활동은 '주드' (1996), '엘리자베스' (1998), '엑시스텐즈' (1999), '식스티 세컨즈' (2000), '디 아더스' (2001), '24시간 파티하는 사람들' (2002), '28일 후' (2002) 의 주제역을 포함하여 세간의 이목을 끄는 다양한 역할에 발탁되면서 시작되었다. 그는 토마스 미들턴의 동명의 연극을 각색한 2002년 영화 '리벤저스 트레지디'의 주인공 역으로써 주연을 맡았다. 그는 독립 영화 '루비보다 값진 것' (1998) 과 '인비저블' (2001) 의 주역을 맡았다. 그는 자동차 도둑 영화 '식스티 세컨즈'의 주연을 맡았지만 2004년 1월까지 운전시험을 받지 않았다. 그는 BBC '탑 기어'에서 그의 면허가 자동변속장치를 이용한 교통수단으로 제한된다고 말했다.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"[Query Given]\")\n",
    "pprint(training_dataset[\"question\"][0])\n",
    "\n",
    "print(\"\\n[Positive context]\")\n",
    "pprint(p_with_neg[0])\n",
    "\n",
    "print(\"\\n[Negative context]\")\n",
    "pprint(p_with_neg[1])\n",
    "pprint(p_with_neg[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jW0gXtJ-_nf"
   },
   "source": [
    "처리한 데이터를 `torch`가 처리할 수 있게 `DataLoader`로 넘겨주는 작업을 해봅시다. 기본적으로 Huggingface Pretrained 모델이 `input_ids`, `attention_mask`, `token_type_ids`를 받아주니, 이 3가지를 넣어주도록 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jiacVxXFeBbK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "\n",
    "q_seqs = tokenizer(\n",
    "    training_dataset[\"question\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "p_seqs = tokenizer(\n",
    "    p_with_neg,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvzIayy79Mdy",
    "outputId": "02e08862-b73f-4bd3-c629-534c8ff88298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg + 1, max_len)\n",
    "p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg + 1, max_len)\n",
    "p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg + 1, max_len)\n",
    "\n",
    "print(p_seqs[\"input_ids\"].size())  # (num_example, pos + neg, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bAplp66Pkayy"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "    q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwMvVH1e3h99"
   },
   "source": [
    "### 💻 2. BERT encoder 학습시키기\n",
    "1. BERT 모델을 구성한 후\n",
    "2. Passage 를 임베딩하는 `p_encoder`와 Query/Question 을 임베딩하는 `q_encoder` 를 각각 선언해줍니다.\n",
    "3. 두 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:32.340866Z",
     "start_time": "2021-09-15T08:06:32.327858Z"
    },
    "id": "oKKkTlh_l5VL"
   },
   "outputs": [],
   "source": [
    "class BertEncoder(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "      \n",
    "    def forward(self,\n",
    "            input_ids, \n",
    "            attention_mask=None,\n",
    "            token_type_ids=None\n",
    "        ): \n",
    "  \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnO1b30SomBP",
    "outputId": "635dec4b-33c2-4026-8fef-a6fa8f150829"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-train된 모델을 사용해줍니다. 위에서 사용한 `model_checkpoint`를 재활용합니다.\n",
    "p_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = BertEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3Dgo8U997HD"
   },
   "source": [
    "`train` 함수를 정의한 후 `p_encoder`과 `q_encoder`를 학습시켜봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VAb7NpUc8YRo"
   },
   "outputs": [],
   "source": [
    "def train(args, num_neg, dataset, p_encoder, q_encoder):\n",
    "    batch_size = args.per_device_train_batch_size\n",
    "  \n",
    "    # Dataloader\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Optimizer\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "        {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "        {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "        {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        eps=args.adam_epsilon\n",
    "    )\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args.warmup_steps,\n",
    "        num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Start training!\n",
    "    global_step = 0\n",
    "\n",
    "    p_encoder.zero_grad()\n",
    "    q_encoder.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    for _ in train_iterator:\n",
    "\n",
    "        # epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "\n",
    "                p_encoder.train()\n",
    "                q_encoder.train()\n",
    "        \n",
    "                targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                targets = targets.to(args.device)\n",
    "\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                    \"attention_mask\": batch[1].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                    \"token_type_ids\": batch[2].view(batch_size * (num_neg + 1), -1).to(args.device)\n",
    "                }\n",
    "        \n",
    "                q_inputs = {\n",
    "                    \"input_ids\": batch[3].to(args.device),\n",
    "                    \"attention_mask\": batch[4].to(args.device),\n",
    "                    \"token_type_ids\": batch[5].to(args.device)\n",
    "                }\n",
    "\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                # (batch_size * (num_neg + 1), emb_dim)\n",
    "                p_outputs = p_encoder(**p_inputs)\n",
    "                # (batch_size, emb_dim)  \n",
    "                q_outputs = q_encoder(**q_inputs)\n",
    "\n",
    "                # Calculate similarity score & loss\n",
    "                p_outputs = p_outputs.view(batch_size, -1, num_neg + 1)\n",
    "                q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                # (batch_size, num_neg + 1)\n",
    "                sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  \n",
    "                sim_scores = sim_scores.view(batch_size, -1)\n",
    "                sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                loss = F.nll_loss(sim_scores, targets)\n",
    "                tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                q_encoder.zero_grad()\n",
    "                p_encoder.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                del p_inputs, q_inputs\n",
    "\n",
    "    return p_encoder, q_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ICSJoJrUDGZ5"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2, # 아슬아슬합니다. 작게 쓰세요 !\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5h7_-aRA9dm"
   },
   "source": [
    "_혹시 CUDA 메모리 에러가 뜬다면 passage encoder 와 query encoder 를 나누지 말고 하나로 학습해보세요. `batch_size`도 변경해보세요._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8a7ww3WgsaZ",
    "outputId": "fbbd90b3-b569-450c-8054-692333666768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s, loss=3.1829447746276855]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:07,  1.21batch/s, loss=3.1829447746276855]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:07,  1.21batch/s, loss=0.0]               \u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.41batch/s, loss=0.0]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.41batch/s, loss=0.0]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.59batch/s, loss=0.0]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.59batch/s, loss=0.0]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.74batch/s, loss=0.0]\u001b[A\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.74batch/s, loss=0.0]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.86batch/s, loss=0.0]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  1.86batch/s, loss=0.0]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.96batch/s, loss=0.0]\u001b[A\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.96batch/s, loss=0.0]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  2.04batch/s, loss=0.0]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  2.04batch/s, loss=0.0]\u001b[A\n",
      " 80%|████████  | 8/10 [00:03<00:00,  2.09batch/s, loss=0.0]\u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:00,  2.09batch/s, loss=0.0]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  2.13batch/s, loss=0.0]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  2.13batch/s, loss=0.0]\u001b[A\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.07batch/s, loss=0.0]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [00:04<00:04,  4.84s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?batch/s, loss=0.0]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.25batch/s, loss=0.0]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 60%|██████    | 6/10 [00:02<00:01,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 60%|██████    | 6/10 [00:02<00:01,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 80%|████████  | 8/10 [00:03<00:00,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 80%|████████  | 8/10 [00:03<00:00,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  2.24batch/s, loss=0.0]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  2.24batch/s, loss=0.0]\u001b[A\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.24batch/s, loss=0.0]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "p_encoder, q_encoder = train(args, num_neg, train_dataset, p_encoder, q_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGOw-k7Ln85t"
   },
   "source": [
    "### 💻 3. 검증셋에 있는 Query 에 대해 passage-retrieval 해보기\n",
    "이제 처음 보는 검증셋에 있는 Query 와 passage 에 대해서도 잘 작동하는지 확인해봅시다. 아래와 같은 순서로 작동해야겠죠?\n",
    "1. 검증셋을 불러온다.\n",
    "2. 검증셋의 query 와 passage 를 임베딩시킨다.\n",
    "3. 유사도를 통해 유사한 주어진 query 에 맞는 passage 를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZA5V6tEpuUV"
   },
   "source": [
    "#### 💻 검증셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NouB9uBcTaws",
    "outputId": "d20d885c-d7cf-481f-f5fa-66b3952236b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Selected Query]\n",
      "'SKY가 휴대전화 CM에서 푸시캣 돌스의 노래와 함꼐 춘 춤은?'\n",
      "[Ground Truth]\n",
      "('2005년 9월 13일 푸시캣 돌스의 첫 번째 정규 음반 PCD가 발매됐다. 이 앨범은 그들이 활동했던 댄스팀 스타일의 댄스팝 음악들과, '\n",
      " '트리뷰트, 커버곡들로 구성되어 있다. 이 앨범은 뉴질랜드에서 1위를 차지했고, 캐나다, 네덜란드, 미국에서 TOP 5에 이름을 올렸으며, '\n",
      " '영국, 오스트리아, 독일, 스위스, 아일랜드에서 10위까지 올라갔다. PCD는 전 세계적으로 900만 장 이상의 판매고를 올렸다. 첫 '\n",
      " \"싱글 'Don't Cha'는 영국, 오스트레일리아, 캐나다 등의 나라에서 1위에 올랐고, 빌보드 핫 100 차트 2위에 올랐다. 또한 이 \"\n",
      " \"노래는 한국의 휴대전화 기기 제조사 SKY의 휴대전화 CM송으로 쓰여, CM 속 모델들이 추는 춤 이름인 '맷돌춤'의 배경음악으로 \"\n",
      " '유행하기도 하였다.')\n"
     ]
    }
   ],
   "source": [
    "valid_corpus = list(set([example[\"context\"] for example in dataset[\"validation\"]]))[:10]\n",
    "\n",
    "sample_idx = random.choice(range(len(dataset[\"validation\"])))\n",
    "query = dataset[\"validation\"][sample_idx][\"question\"]\n",
    "ground_truth = dataset[\"validation\"][sample_idx][\"context\"]\n",
    "\n",
    "if not ground_truth in valid_corpus:\n",
    "    valid_corpus.append(ground_truth)\n",
    "\n",
    "print(f\"[Selected Query]\")\n",
    "pprint(query)\n",
    "\n",
    "print(f\"[Ground Truth]\")\n",
    "pprint(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05D8GzFrJhHO"
   },
   "source": [
    "#### 💻 앞서 학습한 passage encoder, question encoder 을 이용해 dense embedding 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YufA_ayPJBRg",
    "outputId": "cd5720b6-46c9-4da6-d5dc-d480868348e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    p_encoder.eval()\n",
    "    q_encoder.eval()\n",
    "\n",
    "    q_seqs_val = tokenizer(\n",
    "        [query],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    q_emb = q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query, emb_dim)\n",
    "\n",
    "    p_embs = []\n",
    "    for p in valid_corpus:\n",
    "        p = tokenizer(\n",
    "            p,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        p_emb = p_encoder(**p).to(\"cpu\").numpy()\n",
    "        p_embs.append(p_emb)\n",
    "\n",
    "p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "print(p_embs.size(), q_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOHHak7WS1ko"
   },
   "source": [
    "#### 💻 Dot product를 통해 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xn5Cx5JkKZJB",
    "outputId": "84d37434-bfe5-4a6f-d160-d0ca72d65bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "tensor([[182.7851, 183.2831, 172.0922, 193.5603, 172.2709, 169.0991, 178.8234,\n",
      "         189.1181, 156.6556, 168.1709, 188.9190]])\n",
      "tensor([ 3,  7, 10,  1,  0,  6,  4,  2,  5,  9,  8])\n"
     ]
    }
   ],
   "source": [
    "dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "print(dot_prod_scores.size())\n",
    "\n",
    "rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "print(dot_prod_scores)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq2Oiv8MKVS6"
   },
   "source": [
    "#### 💻 Top-5개의 passage를 retrieve 하고 ground truth와 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaStRXYdJ-wI",
    "outputId": "9d93531f-8e65-43c7-dfdc-a1184963362f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Search query]\n",
      " SKY가 휴대전화 CM에서 푸시캣 돌스의 노래와 함꼐 춘 춤은? \n",
      "\n",
      "[Ground truth passage]\n",
      "2005년 9월 13일 푸시캣 돌스의 첫 번째 정규 음반 PCD가 발매됐다. 이 앨범은 그들이 활동했던 댄스팀 스타일의 댄스팝 음악들과, 트리뷰트, 커버곡들로 구성되어 있다. 이 앨범은 뉴질랜드에서 1위를 차지했고, 캐나다, 네덜란드, 미국에서 TOP 5에 이름을 올렸으며, 영국, 오스트리아, 독일, 스위스, 아일랜드에서 10위까지 올라갔다. PCD는 전 세계적으로 900만 장 이상의 판매고를 올렸다. 첫 싱글 'Don't Cha'는 영국, 오스트레일리아, 캐나다 등의 나라에서 1위에 올랐고, 빌보드 핫 100 차트 2위에 올랐다. 또한 이 노래는 한국의 휴대전화 기기 제조사 SKY의 휴대전화 CM송으로 쓰여, CM 속 모델들이 추는 춤 이름인 '맷돌춤'의 배경음악으로 유행하기도 하였다. \n",
      "\n",
      "Top-1 passage with score 193.560302734375:.4f\n",
      "('한편, 《영락경》에서는 10신위(十信位)의 보살을 항상 아래의 10심(十心)을 행한다는 뜻에서 10순명자보살(十順名字菩薩)이라 칭하고 '\n",
      " \"있다. '10순명자보살'의 한자어 문자 그대로의 뜻은 '10순(十順)의 이름[名字]만의 보살' 즉 '10가지 수순(隨順: 상응, 계합, \"\n",
      " \"청정)을 행하는 이름만의 보살'로 아직 보살 즉 대승불교의 수행자라고는 할 수 없지만 가칭으로서 보살이라고 부르는 수행자를 뜻한다. 즉, \"\n",
      " \"현성(賢聖) 즉 42현성(四十二賢聖)에 속하지 못한 상태의 수행자를 말한다. 10신위(十信位)의 수행자를 또한 '이름만의 수행자'라는 \"\n",
      " \"뜻에서 간단히 명자보살이라고도 하고, '초주에 오르기 위해 그 전 단계에서 믿음을 일으키는[想] 수행자'라는 뜻에서 \"\n",
      " \"주전신상보살(住前信想菩薩)이라고도 하고 줄여서 신상보살(信想菩薩)이라고도 한다. 또한 '보살이라는 이름만 빌린 수행자'라는 뜻에서 \"\n",
      " '가명보살(假名菩薩)이라고도 한다.')\n",
      "Top-2 passage with score 189.11813354492188:.4f\n",
      "('대개 대모는 잡식성으로 알려져 있지만, 주식은 해면동물인 것으로 알려져 있으며, 특히 카리브해에 분포하는 개체군의 먹이의 70에서 '\n",
      " '95%는 이들로 이루어진다. 비록 이들이 해면동물을 주로 먹는 편식종이지만 먹이가 되는 종은 제한되며, 먹이가 아닌 종은 건들지 않는다. '\n",
      " '카리브해의 개체군은 보통해면강의 종을 먹는 것으로 알려져 있다. 이들이 주로 먹는 종은 Geodia gibberosa 등이다. 해면동물 '\n",
      " '이외에도 해조류, 해파리같은 자포동물 등을 먹는다. 심지어 독성이 강한 해파리인 히드라충강, 고깔해파리(Physalia '\n",
      " 'physalis)같은 종을 먹기도 한다. 대모는 해파리를 먹을 때 평상시는 보호되지 않는 눈을 닫기 때문에 독침이 이들을 해할 수는 '\n",
      " '없다.')\n",
      "Top-3 passage with score 188.91903686523438:.4f\n",
      "('2005년 9월 13일 푸시캣 돌스의 첫 번째 정규 음반 PCD가 발매됐다. 이 앨범은 그들이 활동했던 댄스팀 스타일의 댄스팝 음악들과, '\n",
      " '트리뷰트, 커버곡들로 구성되어 있다. 이 앨범은 뉴질랜드에서 1위를 차지했고, 캐나다, 네덜란드, 미국에서 TOP 5에 이름을 올렸으며, '\n",
      " '영국, 오스트리아, 독일, 스위스, 아일랜드에서 10위까지 올라갔다. PCD는 전 세계적으로 900만 장 이상의 판매고를 올렸다. 첫 '\n",
      " \"싱글 'Don't Cha'는 영국, 오스트레일리아, 캐나다 등의 나라에서 1위에 올랐고, 빌보드 핫 100 차트 2위에 올랐다. 또한 이 \"\n",
      " \"노래는 한국의 휴대전화 기기 제조사 SKY의 휴대전화 CM송으로 쓰여, CM 속 모델들이 추는 춤 이름인 '맷돌춤'의 배경음악으로 \"\n",
      " '유행하기도 하였다.')\n",
      "Top-4 passage with score 183.28305053710938:.4f\n",
      "('전자지급수단과 관련하여 폭넓은 개념으로 디지털화폐(digital money; digital currency)라는 용어가 사용된다. '\n",
      " '디지털화폐는 은행권·동전과 같이 물질인 방식 아니라 디지털방식으로만 사용될 수 있는 유형의 화폐를 가리킨다. 디지털화폐는 금전적 가치를 '\n",
      " '디지털정보로 바꾸고 암호화하여 IC카드에 저장하고 휴대하여 사용하거나 컴퓨터에 보관하고 네트워크상으로 사용하는 것을 모두 포함한다. '\n",
      " '가상화폐와 암호화폐는 디지털화폐에 속한다. 디지털화폐는 전자화폐(electronic money; electronic currency)와 '\n",
      " '같은 뜻으로 사용되기도 하지만, 대한민국의 경우 전자화폐가 되기 위해서는 범용성 요건을 갖추어야 하므로(전자금융거래법 제2조 제15호) '\n",
      " '전자화폐는 디지털화폐보다 좁은 개념이 된다. 가상화폐와 암호화폐는 모두 디지털화폐에 속하지만 아래에서 검토하는 바와 같이 같은 개념이 '\n",
      " '아니다.')\n",
      "Top-5 passage with score 182.78512573242188:.4f\n",
      "('2008년 12월에는 후배들과 함께 과거 히트곡들을 다시 작업한 헌정앨범 격인 스페셜 앨범 《YOONSANG SONGBOOK：Play '\n",
      " 'With Him!》을 발매했다. 2009년 1월 10일에는 경희대학교 평화의 전당에서 〈윤상 콘서트-Play with him〉이라는 '\n",
      " '타이틀로 후배 음악가들과 함께하는 공연을 개최했다. 영국에서 활동하고 있는 Kayip, 베를린에서 활동하고 있는 Superdrive와 '\n",
      " '함께 프로젝트 그룹 ‘모텟’을 결성, 공연을 통해 IDM/electronic 계열의 입지를 다졌다. 이 때 윤상은 Berklee '\n",
      " 'College of Music Music Synthesis학과를 졸업하고 뉴욕 NYU(New York University)대학원 '\n",
      " 'Music Technology학과에서 석사과정을 수학하였다.')\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "print(\"[Search query]\\n\", query, \"\\n\")\n",
    "print(\"[Ground truth passage]\")\n",
    "print(ground_truth, \"\\n\")\n",
    "\n",
    "for i in range(k):\n",
    "  print(f\"Top-{i + 1} passage with score {dot_prod_scores.squeeze()[rank[i]]}:.4f\")\n",
    "  pprint(valid_corpus[rank[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cicYTii7jZZC"
   },
   "source": [
    "Ground Truth와 똑같거나 비슷한 Passage가 출력됐나요? 역시 Pretrained는 강력하군요.\n",
    "### ❓ Dense Retrieval 코드를 class로 합쳐봅시다.\n",
    "현재 구현된 모델은 굉장히 단순히 짜여졌습니다. 이미 Pretrain된 BERT 모델을 활용했는데, 다른 PLM은 어떨까요? 이를 다시 시도해보자니 코드가 너무 셀로 흩어져있어서 재사용이 어렵네요. 우선 클래스화를 진햏한 후에, `BERT`가 아닌 다른 PLM도 활용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on_Ay5_nbqEc"
   },
   "source": [
    "❗ _Hint_   \n",
    "Scratch부터 짜는게 많이 어려우신가요?\n",
    "\n",
    "만들어야하는 기능들의 파이프라인을 나열하고 하나씩 모듈화하는 습관을 들이는 것이 좋습니다. 순서대로 생각해볼까요?\n",
    "\n",
    "1. Setup   \n",
    "    Naive한 `Dataset`을 받아서 이를 In-Batch Negative를 활용한 후 Dataloader로 변경해주는 코드가 있어야겠죠? 클래스 내에서 활용할 수 있도록 속성(attribute)으로 만들어줍시다. 이 코드를 위에서 활용한 `train` 함수에서 조금 차용해볼까요?\n",
    "2. PLM을 주어진 Passage 와 In-batch negative 기법을 활용해서 훈련합니다.   \n",
    "    이는 위에서 만든 `train` 함수를 약간 응용해서 재활용합시다.\n",
    "3. 훈련한 PLM 을 통해 Query 를 Transform 합니다.\n",
    "4. 내적을 통해 유사도를 구하고, 내림차순을 통해 유사한 Passage 를 Retrieval 합니다.\n",
    "\n",
    "위 4단계를 하나씩 함수로 구현한 후 Class로 합치면 훨씬 쉬울 겁니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:47.410939Z",
     "start_time": "2021-09-15T08:06:46.140856Z"
    },
    "id": "994wDYAujsLr"
   },
   "outputs": [],
   "source": [
    "# 코드가 많아보이지만 주석이 더 많지롱\n",
    "class DenseRetrieval:\n",
    "    def __init__(self,\n",
    "        args,\n",
    "        dataset,\n",
    "        num_neg,\n",
    "        tokenizer,\n",
    "        p_encoder,\n",
    "        q_encoder\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            args (Huggingface Arguments):\n",
    "                세팅과 학습에 필요한 설정값을 받습니다.\n",
    "            dataset (datasets.Dataset):\n",
    "                Huggingface의 Dataset을 받아옵니다.\n",
    "            num_neg (int):\n",
    "                In-batch negative 수행시 사용할 negative sample의 수를 받아옵니다.\n",
    "            tokenizer (Callable):\n",
    "                Tokenize할 함수를 받아옵니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "            p_encoder (torch.nn.Module):\n",
    "                Passage를 Dense Representation으로 임베딩시킬 모델입니다.\n",
    "            q_encoder (torhc.nn.Module):\n",
    "                Query를 Dense Representation으로 임베딩시킬 모델입니다.\n",
    "\n",
    "        Summary:\n",
    "            학습과 추론에 필요한 객체들을 받아서 속성으로 저장합니다.\n",
    "            객체가 instantiate될 때 in-batch negative가 생긴 데이터를 만들도록 함수를 수행합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.contexts = np.array(dataset['context'])\n",
    "        self.valid_corpus = list(set(p for p in self.contexts))\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder.to(args.device)\n",
    "        self.q_encoder = q_encoder.to(args.device)\n",
    "        \n",
    "        self.dataloader = None\n",
    "        self.prepare_in_batch_negative(num_neg=num_neg)\n",
    "\n",
    "\n",
    "    def prepare_in_batch_negative(self,\n",
    "        dataset=None,\n",
    "        num_neg=2,\n",
    "        tokenizer=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dataset (datasets.Dataset, default=None):\n",
    "                Huggingface의 Dataset을 받아오면,\n",
    "                in-batch negative를 추가해서 Dataloader를 만들어주세요.\n",
    "            num_neg (int, default=2):\n",
    "                In-batch negative 수행시 사용할 negative sample의 수를 받아옵니다.\n",
    "            tokenizer (Callable, default=None):\n",
    "                Tokenize할 함수를 받아옵니다.\n",
    "                별도로 받아오지 않으면 속성으로 저장된 Tokenizer를 불러올 수 있게 짜주세요.\n",
    "\n",
    "        Note:\n",
    "            모든 Arguments는 사실 이 클래스의 속성으로 보관되어 있기 때문에\n",
    "            별도로 Argument를 직접 받지 않아도 수행할 수 있게 만들어주세요.\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "            \n",
    "        if tokenizer is None:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "            \n",
    "        corpus = self.contexts\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in tqdm(dataset[\"context\"], desc=\"prepare in batch negative\"):\n",
    "\n",
    "            while True:\n",
    "                neg_idxs = np.random.randint(len(corpus), size=self.num_neg)\n",
    "\n",
    "                if not c in corpus[neg_idxs]:\n",
    "                    p_neg = corpus[neg_idxs]\n",
    "\n",
    "                    p_with_neg.append(c)\n",
    "                    p_with_neg.extend(p_neg)\n",
    "                    break\n",
    "        \n",
    "        q_seqs = tokenizer(\n",
    "            dataset[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        p_seqs = tokenizer(\n",
    "            p_with_neg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg + 1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg + 1, max_len)\n",
    "        p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg + 1, max_len)\n",
    "        \n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        \n",
    "        self.dataloader = DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size)\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "        args=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            train을 합니다. 위에 과제에서 이용한 코드를 활용합시다.\n",
    "            encoder들과 dataloader가 속성으로 저장되어있는 점에 유의해주세요.\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "            \n",
    "        batch_size = args.per_device_train_batch_size\n",
    "        num_neg = self.num_neg\n",
    "\n",
    "        # Dataloader\n",
    "        # train_dataloader = DataLoader(self.p_with_neg, batch_size=batch_size)\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        for _ in range(args.num_train_epochs):\n",
    "\n",
    "            # epoch_iterator = tqdm(self.dataloader, desc=\"Iteration\")\n",
    "            with tqdm(self.dataloader, unit=\"batch\") as tepoch:\n",
    "            # for tepoch in epoch_iterator:\n",
    "                for batch in tepoch:\n",
    "\n",
    "                    self.p_encoder.train()\n",
    "                    self.q_encoder.train()\n",
    "                    # print(batch)\n",
    "\n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (num_neg + 1), -1).to(args.device),\n",
    "                        \"token_type_ids\": batch[2].view(batch_size * (num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "\n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[3].to(args.device),\n",
    "                        \"attention_mask\": batch[4].to(args.device),\n",
    "                        \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    del batch\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # (batch_size * (num_neg + 1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)\n",
    "                    # (batch_size, emb_dim)  \n",
    "                    q_outputs = self.q_encoder(**q_inputs)\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    p_outputs = p_outputs.view(batch_size, -1, num_neg + 1)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    # (batch_size, num_neg + 1)\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  \n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.q_encoder.zero_grad()\n",
    "                    self.p_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "        # return self.p_encoder, self.q_encoder\n",
    "\n",
    "\n",
    "    def get_relevant_doc(self,\n",
    "        query,\n",
    "        k=1,\n",
    "        args=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str)\n",
    "                문자열로 주어진 질문입니다.\n",
    "            k (int, default=1)\n",
    "                상위 몇 개의 유사한 passage를 뽑을 것인지 결정합니다.\n",
    "            args (Huggingface Arguments, default=None)\n",
    "                Configuration을 필요한 경우 넣어줍니다.\n",
    "                만약 None이 들어오면 self.args를 쓰도록 짜면 좋을 것 같습니다.\n",
    "\n",
    "        Summary:\n",
    "            1. query를 받아서 embedding을 하고\n",
    "            2. 전체 passage와의 유사도를 구한 후\n",
    "            3. 상위 k개의 문서 index를 반환합니다.\n",
    "        \"\"\"\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            self.p_encoder.eval()\n",
    "            self.q_encoder.eval()\n",
    "\n",
    "            q_seqs_val = self.tokenizer(\n",
    "                [query],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            q_emb = self.q_encoder(**q_seqs_val).to(\"cpu\")  # (num_query, emb_dim)\n",
    "\n",
    "            p_embs = []\n",
    "            for p in tqdm(self.valid_corpus):\n",
    "                p = tokenizer(\n",
    "                    p,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "                p_emb = self.p_encoder(**p).to(\"cpu\").numpy()\n",
    "                p_embs.append(p_emb)\n",
    "\n",
    "        p_embs = torch.Tensor(p_embs).squeeze()  # (num_passage, emb_dim)\n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "        return rank[:k].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.333519Z",
     "start_time": "2021-09-15T08:06:47.419925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1b16d8e4c2d74aab9fdda9ae60c21441",
      "58ca9dbe05d149fcb73677736541c1ce",
      "92cbaf4e42694aca93e327941b1c9422",
      "e4c5ce6351014015aa41d8a43fdca7d8",
      "9407250e75c14524be3fa222b02c4f74",
      "e38d794604bb409bb4d41a5bfef5eb61",
      "0a78e445530c4d93a2ccce5bcda351da",
      "3fe1fbff472c435c94c25c1736081373",
      "430873d31e5c4770afd48183c1a8f1de",
      "3ad885d798e64e35ae6705412494a52e",
      "be78ee5fbd55487fb3dd0da23adbc154"
     ]
    },
    "id": "cBr57rLvkM68",
    "outputId": "1bcc0a3e-90b7-44cb-c772-91a88df16f93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_kor_v1 (/opt/ml/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/31982418accc53b059af090befa81e68880acc667ca5405d30ce6fa7910950a7)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋과 모델은 아래와 같이 불러옵니다.\n",
    "train_dataset = load_dataset(\"squad_kor_v1\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.333519Z",
     "start_time": "2021-09-15T08:06:47.419925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1b16d8e4c2d74aab9fdda9ae60c21441",
      "58ca9dbe05d149fcb73677736541c1ce",
      "92cbaf4e42694aca93e327941b1c9422",
      "e4c5ce6351014015aa41d8a43fdca7d8",
      "9407250e75c14524be3fa222b02c4f74",
      "e38d794604bb409bb4d41a5bfef5eb61",
      "0a78e445530c4d93a2ccce5bcda351da",
      "3fe1fbff472c435c94c25c1736081373",
      "430873d31e5c4770afd48183c1a8f1de",
      "3ad885d798e64e35ae6705412494a52e",
      "be78ee5fbd55487fb3dd0da23adbc154"
     ]
    },
    "id": "cBr57rLvkM68",
    "outputId": "1bcc0a3e-90b7-44cb-c772-91a88df16f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 60407\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.333519Z",
     "start_time": "2021-09-15T08:06:47.419925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1b16d8e4c2d74aab9fdda9ae60c21441",
      "58ca9dbe05d149fcb73677736541c1ce",
      "92cbaf4e42694aca93e327941b1c9422",
      "e4c5ce6351014015aa41d8a43fdca7d8",
      "9407250e75c14524be3fa222b02c4f74",
      "e38d794604bb409bb4d41a5bfef5eb61",
      "0a78e445530c4d93a2ccce5bcda351da",
      "3fe1fbff472c435c94c25c1736081373",
      "430873d31e5c4770afd48183c1a8f1de",
      "3ad885d798e64e35ae6705412494a52e",
      "be78ee5fbd55487fb3dd0da23adbc154"
     ]
    },
    "id": "cBr57rLvkM68",
    "outputId": "1bcc0a3e-90b7-44cb-c772-91a88df16f93"
   },
   "outputs": [],
   "source": [
    "# 메모리가 부족한 경우 일부만 사용하세요 !\n",
    "# num_sample = 4500\n",
    "# sample_idx = np.random.choice(range(len(train_dataset)), num_sample)\n",
    "# train_dataset = train_dataset[sample_idx]\n",
    "# train_dataset['context'][90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.333519Z",
     "start_time": "2021-09-15T08:06:47.419925Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1b16d8e4c2d74aab9fdda9ae60c21441",
      "58ca9dbe05d149fcb73677736541c1ce",
      "92cbaf4e42694aca93e327941b1c9422",
      "e4c5ce6351014015aa41d8a43fdca7d8",
      "9407250e75c14524be3fa222b02c4f74",
      "e38d794604bb409bb4d41a5bfef5eb61",
      "0a78e445530c4d93a2ccce5bcda351da",
      "3fe1fbff472c435c94c25c1736081373",
      "430873d31e5c4770afd48183c1a8f1de",
      "3ad885d798e64e35ae6705412494a52e",
      "be78ee5fbd55487fb3dd0da23adbc154"
     ]
    },
    "id": "cBr57rLvkM68",
    "outputId": "1bcc0a3e-90b7-44cb-c772-91a88df16f93"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model_checkpoint = \"klue/bert-base\"\n",
    "\n",
    "# 혹시 위에서 사용한 encoder가 있다면 주석처리 후 진행해주세요 (CUDA ...)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# p_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)\n",
    "# q_encoder = BertEncoder.from_pretrained(model_checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.335488Z",
     "start_time": "2021-09-15T08:06:50.335488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aUOE8VEyZ1v",
    "outputId": "1b78b56c-a84c-4e5f-8d4a-43afe8dd0c2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare in batch negative: 100%|██████████| 60407/60407 [00:09<00:00, 6612.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-0cb657e988c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m retriever = DenseRetrieval(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-d3280c867860>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, dataset, num_neg, tokenizer, p_encoder, q_encoder)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_in_batch_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-d3280c867860>\u001b[0m in \u001b[0;36mprepare_in_batch_negative\u001b[0;34m(self, dataset, num_neg, tokenizer)\u001b[0m\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         p_seqs = tokenizer(\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mp_with_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2255\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2437\u001b[0m         )\n\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2439\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2440\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m#                    ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         tokens_and_encodings = [\n\u001b[0m\u001b[1;32m    398\u001b[0m             self._convert_encoding(\n\u001b[1;32m    399\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         tokens_and_encodings = [\n\u001b[0;32m--> 398\u001b[0;31m             self._convert_encoding(\n\u001b[0m\u001b[1;32m    399\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mencoding_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retriever는 아래와 같이 사용할 수 있도록 코드를 짜봅시다.\n",
    "retriever = DenseRetrieval(\n",
    "    args=args,\n",
    "    dataset=train_dataset,\n",
    "    num_neg=2,\n",
    "    tokenizer=tokenizer,\n",
    "    p_encoder=p_encoder,\n",
    "    q_encoder=q_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.335488Z",
     "start_time": "2021-09-15T08:06:50.335488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aUOE8VEyZ1v",
    "outputId": "1b78b56c-a84c-4e5f-8d4a-43afe8dd0c2d"
   },
   "outputs": [],
   "source": [
    "retriever.train()\n",
    "# tqdm을 어떻게 처리할까..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T08:06:50.335488Z",
     "start_time": "2021-09-15T08:06:50.335488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aUOE8VEyZ1v",
    "outputId": "1b78b56c-a84c-4e5f-8d4a-43afe8dd0c2d"
   },
   "outputs": [],
   "source": [
    "# query = \"유아인에게 타고난 배우라고 말한 드라마 밀회의 감독은?\"\n",
    "query = \"오바마는 미국의 몇 대 대통령인가?\"\n",
    "results = retriever.get_relevant_doc(query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKKMfYhREui4",
    "outputId": "ef512c8c-f8d6-4b6f-d124-24c438529faf"
   },
   "outputs": [],
   "source": [
    "print(f\"[Search Query] {query}\")\n",
    "\n",
    "# indices = results[1]\n",
    "for i, idx in enumerate(results):\n",
    "    print(f\"Top-{i + 1}th Passage (Index {idx})\")\n",
    "    pprint(retriever.valid_corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwE-zCr2CIiq"
   },
   "source": [
    "#### ➕추가 과제: 다양한 기능 추가하기\n",
    "현재 코드에서 불편한 부분이 있죠. 일단 너무 많은 기능을 한 class에 넣은 것도 문제, 메소드도 한 번에 너무 다양한 기능을 하는 것이 문제입니다. 메소드 내의 기능을 쪼개서 여러 메소드로 나누거나 데이터셋을 만드는 부분 같이 큰 기능을 다른 class로 분리해봅시다. (DPR 추가 과제는 예시 코드가 제공되지 않습니다. 다양하게 활용해보세요 !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU8A2bpXnIOL"
   },
   "source": [
    "### ❓ Sparse Retrieval과 Dense Retrieval을 둘 다 시도해보았습니다. 어떤 차이가 있을까요? 팀원들과 논의해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8KO4AmLhr_G"
   },
   "source": [
    "## ❓ 과제: Wikipedia documents에 대해 Passage Retrieval 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnvVrapPmcCO"
   },
   "source": [
    "위에서 배운 Passage Retrieval을 Wikipedia에 있는 문서들로 진행해봅시다. 사실 위에서 두 클래스를 성공적으로 만들었다면, 데이터를 불러온 후 적용만 시키면 끝이 납니다.   \n",
    "***별도의 예시코드가 제공되지 않습니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC2AVU0m6OoV"
   },
   "outputs": [],
   "source": [
    "# 츄라이 츄라이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74EO3anZnbga"
   },
   "source": [
    "만족스러운 결과가 나왔나요? 그렇지 않았다면 모델의 구조를 바꾸거나, 더 많은 training set으로 학습 시켜보세요 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3j0VhH-kU9e"
   },
   "source": [
    "### ❓ Take-home Question\n",
    "이 QA 모델을 배포한다고 생각해봅시다. 성능이 좋아서 점점 이용자가 많아지는 상황입니다. 만약 동시에 1000명의 이용자가 Query 를 보내면 어떻게 될까요? 전체 Passage가 100,000 개라면, 이 유사도를 계산하고 비교하는 횟수는 ... 점점 많아질 것 같네요. 이 문제는 어떻게 해결하면 좋을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "dCWS-F5haN6_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1012_MRC Mission 3 - Sparse & Dense Passage Retrieval.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e6a65b5bf72b320117a5cfd505c5a1e2290f30898d5727c674807c535145cbf6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "439.071px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03d20af78d7c44159abca802ac082d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d9e100657ac4554974e990951e43387",
      "placeholder": "​",
      "style": "IPY_MODEL_1f2a869e4b404e68b954eb384ef62d68",
      "value": " 38.5M/? [00:01&lt;00:00, 31.1MB/s]"
     }
    },
    "05a482882c4f4e1b9b85150a3eed2293": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec3ad8119f13412f8a7b8b00bf275b8b",
       "IPY_MODEL_a2daef726e6e48219e7c0d9c62ac962c"
      ],
      "layout": "IPY_MODEL_5bfdfadc746c4a2ca44af7da3bfc4ab9"
     }
    },
    "0a78e445530c4d93a2ccce5bcda351da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0be86d05d64240b887902fb7624c474d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d9e100657ac4554974e990951e43387": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f36e44064244c2daf1a9fdeee5e7fe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1111e96e637447439c90016ef4184ba7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "113b4c8b0e264313a5abfc5bc3fe85df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b16d8e4c2d74aab9fdda9ae60c21441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92cbaf4e42694aca93e327941b1c9422",
       "IPY_MODEL_e4c5ce6351014015aa41d8a43fdca7d8",
       "IPY_MODEL_9407250e75c14524be3fa222b02c4f74"
      ],
      "layout": "IPY_MODEL_58ca9dbe05d149fcb73677736541c1ce"
     }
    },
    "1f2a869e4b404e68b954eb384ef62d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2553f14880194fe3b1a4a5c611de59f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_925386f42c2b40a09990ade0181559ba",
       "IPY_MODEL_03d20af78d7c44159abca802ac082d1a"
      ],
      "layout": "IPY_MODEL_9d5de35252fd4404a474054f999e950b"
     }
    },
    "3ad885d798e64e35ae6705412494a52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d4b7f7914fb42f39c364a92538eea2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fe1fbff472c435c94c25c1736081373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "430873d31e5c4770afd48183c1a8f1de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46d61012f6ac4ca189d8890826a27fe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48f64c7b8d164ce7ac7efb8ae7aaaffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "493174df299e451c95241a6fa0f21b59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49e6ffd8000644eda4157dcb48829984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c832fc7a4ca4965bd37b68a1ae2c736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be86d05d64240b887902fb7624c474d",
      "placeholder": "​",
      "style": "IPY_MODEL_49e6ffd8000644eda4157dcb48829984",
      "value": " 428/428 [00:01&lt;00:00, 320B/s]"
     }
    },
    "5357ac4465374fc79ec8dd5a9c532a49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58ca9dbe05d149fcb73677736541c1ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bfdfadc746c4a2ca44af7da3bfc4ab9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "697798699f354b3bb87a4265f545dcbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f85b679b2add466ba5230481ffcfa3ab",
       "IPY_MODEL_4c832fc7a4ca4965bd37b68a1ae2c736"
      ],
      "layout": "IPY_MODEL_9ebc6cb8cbdc4675869035f04166b9d7"
     }
    },
    "69b6b6a01c034bf78478509a09ac3513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc2727ff9c2d401a8008e617ba9ad747",
       "IPY_MODEL_fb079b9a51a248d9ad0de418ab0476c8"
      ],
      "layout": "IPY_MODEL_9da62ff244874ecb94b771df40dd7b64"
     }
    },
    "7260a5df06ff44a18e4890109b9b0be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7b64d95da47f46c4800e0bba13283693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "925386f42c2b40a09990ade0181559ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46d61012f6ac4ca189d8890826a27fe5",
      "max": 7568316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9e10b47884943488ce8fdbaf2f1e9b9",
      "value": 7568316
     }
    },
    "92cbaf4e42694aca93e327941b1c9422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a78e445530c4d93a2ccce5bcda351da",
      "placeholder": "​",
      "style": "IPY_MODEL_e38d794604bb409bb4d41a5bfef5eb61",
      "value": "100%"
     }
    },
    "9407250e75c14524be3fa222b02c4f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be78ee5fbd55487fb3dd0da23adbc154",
      "placeholder": "​",
      "style": "IPY_MODEL_3ad885d798e64e35ae6705412494a52e",
      "value": " 2/2 [00:00&lt;00:00, 36.73it/s]"
     }
    },
    "9a28b6e8626841f2a23fd6f31160a2f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f36e44064244c2daf1a9fdeee5e7fe4",
      "placeholder": "​",
      "style": "IPY_MODEL_b973d350bcfd4d75b3c4235b1624f798",
      "value": " 4.56k/? [00:01&lt;00:00, 2.72kB/s]"
     }
    },
    "9abf651d6f80423ea448ab06acfac005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d5de35252fd4404a474054f999e950b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da62ff244874ecb94b771df40dd7b64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebc6cb8cbdc4675869035f04166b9d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2daef726e6e48219e7c0d9c62ac962c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d4b7f7914fb42f39c364a92538eea2e",
      "placeholder": "​",
      "style": "IPY_MODEL_113b4c8b0e264313a5abfc5bc3fe85df",
      "value": " 248k/248k [00:00&lt;00:00, 252kB/s]"
     }
    },
    "b17c04a5d62542f1b51ed9a0b32e77bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b973d350bcfd4d75b3c4235b1624f798": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be78ee5fbd55487fb3dd0da23adbc154": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc2727ff9c2d401a8008e617ba9ad747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff74f9f5d22b498aaf8f6c3d41c5270c",
      "max": 962,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd1c037bd10a4894a4fc213294dc3a13",
      "value": 962
     }
    },
    "cd1c037bd10a4894a4fc213294dc3a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cd22331b8ece475080b0b102be3522eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5357ac4465374fc79ec8dd5a9c532a49",
      "max": 1745,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d863b911b833435dbb32db1761884539",
      "value": 1745
     }
    },
    "d863b911b833435dbb32db1761884539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ddb9baa5e79c4e5dbfc4cc2a23e79987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd22331b8ece475080b0b102be3522eb",
       "IPY_MODEL_9a28b6e8626841f2a23fd6f31160a2f7"
      ],
      "layout": "IPY_MODEL_7b64d95da47f46c4800e0bba13283693"
     }
    },
    "e38d794604bb409bb4d41a5bfef5eb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4c5ce6351014015aa41d8a43fdca7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430873d31e5c4770afd48183c1a8f1de",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fe1fbff472c435c94c25c1736081373",
      "value": 2
     }
    },
    "e9e10b47884943488ce8fdbaf2f1e9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec3ad8119f13412f8a7b8b00bf275b8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_493174df299e451c95241a6fa0f21b59",
      "max": 248477,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7260a5df06ff44a18e4890109b9b0be9",
      "value": 248477
     }
    },
    "f85b679b2add466ba5230481ffcfa3ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1111e96e637447439c90016ef4184ba7",
      "max": 428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48f64c7b8d164ce7ac7efb8ae7aaaffe",
      "value": 428
     }
    },
    "fb079b9a51a248d9ad0de418ab0476c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b17c04a5d62542f1b51ed9a0b32e77bf",
      "placeholder": "​",
      "style": "IPY_MODEL_9abf651d6f80423ea448ab06acfac005",
      "value": " 2.24k/? [00:00&lt;00:00, 28.6kB/s]"
     }
    },
    "ff74f9f5d22b498aaf8f6c3d41c5270c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
