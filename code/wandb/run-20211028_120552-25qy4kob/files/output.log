
 80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 11977/14970 [00:01<00:00, 9617.88it/s]
































































































































































































 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 12498/14970 [06:46<31:15,  1.32it/s]
































































































































































































 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 12999/14970 [13:16<25:02,  1.31it/s]

























































































































































































 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 13478/14970 [19:33<20:18,  1.22it/s]Traceback (most recent call last):
  File "train.py", line 378, in <module>
    main()
  File "train.py", line 109, in main
    run_mrc(data_args, training_args, model_args, datasets, tokenizer, model)
  File "train.py", line 343, in run_mrc
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1162, in train
    self.optimizer.step()
  File "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 67, in wrapper
    return wrapped(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py", line 345, in step
    exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)
KeyboardInterrupt