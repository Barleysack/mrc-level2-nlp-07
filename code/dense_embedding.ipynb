{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116de6d1-b38a-48f2-b17c-294cc9511770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, AutoModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41485e01-0a23-41e5-9dbf-b69b6d3c1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f934f77-828f-451e-87be-c4de4b7c35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우왕은 어린 시절을 신돈의 집에서 보내야 했다. 공민왕은 원래 자식이 없어 고민하였는데 신돈이 자신의 여종인 반야를 바쳐 아이를 얻으라고 권유하였다. 이에 공민왕은 반야와 동침했고 얼마 뒤에 반야는 임신하였다. 반야가 만삭이 되자 신돈은 자신의 친구인 승려 능우(能禑)의 어머니에게 반야를 맡겼다 능우의 어머니 집에서 아이를 출산한 반야는 일년 후에 신돈의 집에 가서 기거하였다. 신돈은 동지밀직 김횡이 보낸 여종 김장을 유모로 삼아 아이를 돌보게 했다\\n\\n반야는 신돈의 여종이었고 공민왕은 반야의 아이를 신돈의 아이라고 할까 봐 근심하고 고민하였다. 1371년 신돈이 역모죄로 몰려 수원부(水原府)로 유배되자 공민왕은 자신에게 아들이 있다고 백관들에게 밝히고 반야의 아들 모니노(牟尼奴)를 궁궐로 데려오라고 하였다 공민왕은 자신이 살해당하던 달인 1374년 9월 초, 모니노가 반야의 소생이라고 하면 사람들이 모니노를 신돈의 자식이라고 의심할 것이라 염려한 공민왕은 이미 사망하고 없는 궁인 한씨를 우왕의 생모라고 말하고서 한씨의 삼대(三代) 조상과 그 여자의 외조에게 벼슬을 추증한다 우왕 즉위 후 한씨에게는 순정왕후라는 시호가 내려진다\n",
      "우왕은 어디서 태어났는가?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 616, 3778, 3640, 3662, 1172,  849, 1507, 2192,  137, 2480,  454,\n",
       "        940,  173,  925, 3741, 2540, 3077, 3889,  433, 1558,  137, 1393,\n",
       "       3658, 1847, 3777, 2696,   29, 1150, 3787, 2092, 2789, 1603,  909,\n",
       "       3280,  369, 3549, 2848,  385, 3918, 2341,  107, 3837, 3247, 1092,\n",
       "        413, 2479, 3615,  376,  407,    6,  804, 3402, 2479, 2590,  607,\n",
       "       1864, 1953, 2747, 2676,    1, 1378, 1011,  763, 2479,  902,  148,\n",
       "       3312,  968,  600, 3832, 3748, 1192, 1851, 1521, 2896, 3276, 1122,\n",
       "       2803, 2826, 3742,  389, 3145, 3478,  375, 1840, 2358, 2633,  869,\n",
       "       1188, 3451, 1570, 2852,  460,  547, 1317, 2623, 1704, 2625,  697,\n",
       "       3498, 2739,  440, 3083, 1777, 1499, 3794,  904, 3325, 1676, 1078,\n",
       "       3453, 1771,  307, 2205, 3468,   51, 1419, 2976,  777, 3572, 1015,\n",
       "         66, 2034, 2852, 1411, 2094, 1658, 2901, 2802,  142,  298,  545,\n",
       "        883, 3665, 2294,  348, 3589, 2927,  781, 3076,  888,  611, 1244,\n",
       "       1415, 2424,  257, 2533, 3885, 1020, 1509, 1824, 3049, 3109, 3833,\n",
       "       2347,  264, 3486, 2624, 2121, 2017,  896,  906, 2816, 1402, 3532,\n",
       "       3585,  961, 3734, 3029, 3787, 2820, 3605, 2460, 1007, 2090, 2670,\n",
       "       3790,  622, 2924, 2588, 3865, 2588, 3574, 2336, 2587, 2947, 3388,\n",
       "       2007, 3755, 2863,  366, 2964,  276,  530, 3488,  600, 1477, 1208,\n",
       "       2700, 3813,  838,  756,  648,  420, 2799,   42, 3368, 2434, 1691,\n",
       "       1939,  991, 3210,  320, 3221,  331, 2690, 3851,  574, 2997,  144,\n",
       "       2438,  485,  608, 3731, 3354, 3196,  454, 3654,   65, 1119,  500,\n",
       "       3147,  385, 1566,  552, 3705, 2170, 2128, 3530,   90, 2317, 3389,\n",
       "        839, 2972, 3519, 2995,  320, 1242, 1863, 3921,  263, 1560, 1923,\n",
       "       2026, 3481,  605, 1196, 2473, 3817,  865,  691, 1143, 3075, 3173,\n",
       "       2323, 2456, 3565,  688, 2244, 3336, 3745, 1054,  522, 2714, 1472,\n",
       "        635, 1619,  443,  588, 2956, 2502, 2665, 2678,  363, 2547, 1820,\n",
       "        868, 1707,   76, 2526,   15, 2800, 3022, 2157, 3025, 1795,  751,\n",
       "       1175,  855,  691, 1774, 3928, 1366, 3411,  915, 2041, 2613, 1902,\n",
       "       3909, 3914, 2817, 3602, 1170,  117,  116, 1094, 2691, 3221, 3360,\n",
       "       2261,  797,  695, 1997,  666, 3240, 3537,  352,  545,  788, 1961,\n",
       "       3547, 1817,  257, 2898,  581,  969,   72, 1545, 2226, 1220, 3462,\n",
       "        762, 2332, 1089, 2154, 1673, 1231, 2390, 2574, 3470, 2697, 2818,\n",
       "        759, 2806, 2223, 1037, 1363, 1767,   26, 3424, 3241,  928,  119,\n",
       "       2015,  297,  740,  439, 2984, 3612, 2145, 3910, 3354,  598, 3016,\n",
       "       2659,  424, 1787,  455, 3043,  551, 3944,  205, 2325,  735,  165,\n",
       "       2810, 3125,  367, 3871, 2371, 3498, 2112,  897, 3712,  838,  416,\n",
       "       2265, 2153, 1978,  117, 3356, 1936, 1947,  935,  397, 3559, 1650,\n",
       "        185, 2512, 3694,   11, 3025,  316, 1527, 2002, 2784, 2400, 3791,\n",
       "       1528,  956,  782, 1433, 1894, 2224, 3428, 1820,  742, 1721,  880,\n",
       "       1300, 2079, 1531, 1821, 3889, 3257,  837, 1772,  621, 1722, 3593,\n",
       "       1096, 3382, 1685, 3092,  664, 2667, 2547, 2221, 3942, 2370, 1866,\n",
       "       1087, 3926, 2903, 2685,  945, 2899,  168,  339, 3887, 3307,  157,\n",
       "       1535, 2698, 3436, 3338,  933, 1220, 3530, 2566, 3032,  132, 1872,\n",
       "        667, 3197, 1653,  990,   20, 2403,  214, 2297, 1374, 3857,  403,\n",
       "       3188, 1044, 3688, 2097,  726,   97, 2329, 1521,  618, 1575, 1289,\n",
       "       1743, 1828, 1124,  935, 1478, 1402, 3065, 2281, 1543, 2289,  822,\n",
       "       1426, 3866, 2723, 2880, 2082, 2134, 3675, 2002, 2554, 3059,  207,\n",
       "       2621, 2653, 1174, 3096, 1175, 1750, 3125,  547, 3241, 1689, 3139,\n",
       "       2887, 1151, 1991,  475, 3831, 3128, 3170,  414, 1640, 1430, 3155,\n",
       "       2558, 2666,  725, 3009, 3472, 2080, 2053, 2898, 1267, 2185, 2293,\n",
       "       3567, 2399, 2408, 3417, 2339, 3629, 1857, 3511,  667, 2512, 3937,\n",
       "        920, 3262, 3129, 1831, 3254,  988, 1730,  758, 3540, 2988, 2765,\n",
       "       2736, 3017, 2518, 2078, 1367,  431, 3041,  330, 1732, 1057, 3541,\n",
       "       1918,  577,  582, 2915, 2805,  664,  217, 3389,  351, 3657, 3570,\n",
       "         34,  510, 3484, 2156, 3201, 2002, 3786, 2906,  763, 1838, 2931,\n",
       "       3217, 1451, 2396, 2391,  295, 1970, 3942, 3052,  600,  687, 3648,\n",
       "       3867, 3747,  532, 2675, 2647, 1724,  850, 2198, 2683, 2372, 2024,\n",
       "       1929, 3947, 3692, 2658, 1341, 2361,  512, 1228, 1490, 2709, 3030,\n",
       "       1241, 1347, 2758, 2532, 2058,  219, 2187, 2366, 2214, 3926, 2910,\n",
       "       1403,  429, 1966,  445, 1832, 3236,  161,  427, 3775,  285, 3059,\n",
       "       1774, 3372, 2848, 1727,  364, 1093,  962, 1279,  191, 1603, 3948,\n",
       "       1988, 2509, 3566, 2266, 2381, 2548, 3033,  366, 1185, 1672, 2792,\n",
       "       3497, 1024, 2435, 2269, 2506, 3513, 2040, 1592, 1906, 2935,  167,\n",
       "        630, 1728,  545, 1858, 3454, 1496, 3489, 2662,  142,  447, 3165,\n",
       "       3132, 3183,  824, 3862, 1914, 2038,  830, 3936, 2346, 3684,   59,\n",
       "        181, 2237,  942, 2343, 3395,  729, 1874,  364,   98, 3527,  225,\n",
       "       1848, 1066, 1191, 3947,  908, 3764, 3492, 1413, 3410, 1946, 3330,\n",
       "        563, 1275,  703, 2198, 1761, 1570, 1313, 2132, 3634, 3811, 3220,\n",
       "       2854,  494,  593,  259,  533, 3922, 1626, 1831, 2112, 3381, 3148,\n",
       "        653, 1599, 2931,  458, 1850, 2336, 2881, 3852, 3637, 3159, 1090,\n",
       "       3317, 1495, 2736,  622, 2640, 1122, 1277, 2739, 1150,   81, 1191,\n",
       "       1281,  799, 3257,  263, 1369, 3490, 3613, 3515, 1826,  196,  195,\n",
       "       3506,  858,  414, 1813, 3644,  430, 3553, 3554, 3693, 2257,  974,\n",
       "       3878, 2137,  760, 3638, 3921, 3374, 1909, 2967,  925, 2225,  194,\n",
       "       1661, 3764, 1360, 3683, 3781, 2377, 2233,  975, 3474, 2329, 3595,\n",
       "       2016, 1965, 1164, 3105,  990, 3686,  233, 3611, 3897, 3909,  181,\n",
       "       1823, 2718, 1433, 1004, 3869, 2674,  992, 3331, 2674,  769, 2416,\n",
       "       1041,  905, 2342,  151, 2086,  776, 3257, 3230, 2329, 3018,  794,\n",
       "       2546, 3154, 3629, 3854,   85, 3687, 1341, 1710, 1574,  326, 3524,\n",
       "       2800, 2850,  495,  599, 3710, 2736,  366, 1240,  339, 3636, 3690,\n",
       "        673, 1523,  466,  124, 2970, 2625, 3099,  675,  273, 2843, 1659,\n",
       "       1793, 2695, 3414, 2187,  595,  340,  444, 1458, 1734,  879, 2206,\n",
       "        233,  235, 1946,  401, 2876, 1265,  524,  482, 2881, 2711, 2138,\n",
       "       2654, 3453, 1532, 3302, 3829, 3593,  218, 2486, 2772, 1493, 2318,\n",
       "        442, 1034, 3354, 2098, 3639, 3552, 2650, 1809, 1141, 1006, 1687,\n",
       "       2344,  548, 3547, 2154, 1942, 3718, 3482, 2706, 1640, 1251, 3926,\n",
       "       3890, 2845, 3530,  547, 2155, 1526, 1512, 2828, 3323,   44, 1960,\n",
       "       2828, 2009, 1537, 1623, 2599, 3638, 2263,  483,  790, 2152,  845,\n",
       "       1291, 1876, 3725, 1264, 1884,  865, 1384, 2432, 1459, 2591, 1627,\n",
       "         77, 3297, 2294, 3413, 3350,  312, 3569,   74, 2302, 1902, 2044,\n",
       "       1968,  608, 3766, 1352,  145, 2904, 3047,  714, 3888, 1120])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_from_disk(\"../data/train_dataset/train\")\n",
    "sample_idx = np.random.choice(range(len(train_dataset)), 1000)\n",
    "training_dataset = train_dataset[sample_idx]\n",
    "print(training_dataset['context'][0])\n",
    "print(training_dataset['question'][0])\n",
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6743c0c-76fc-451d-b63a-1dbb06ca50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(AutoModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = AutoModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(self, input_ids,  attention_mask=None): \n",
    "\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0261854e-2a5b-458a-8e03-4b5ddbb3217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "p_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(device)\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_checkpoint).to(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     p_encoder.cuda()\n",
    "#     q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720f4765-e790-42e2-a7a6-3a6fd8b10996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef0c9261-c342-4858-ae09-a4318103e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval:\n",
    "    def __init__(self, args, dataset, num_neg, tokenizer, p_encoder, q_encoder):\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "        self.prepare_in_batch_negative(num_neg=num_neg)\n",
    "        \n",
    "        # self.eval_dataset = eval_dataset\n",
    "        # self.prepare_eval_dataset\n",
    "\n",
    "    def prepare_in_batch_negative(self, dataset=None, num_neg=2, tokenizer=None):\n",
    "\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # 1. In-Batch-Negative 만들기\n",
    "        # CORPUS를 np.array로 변환해줍니다.\n",
    "        corpus = np.array(list(set([example for example in dataset[\"context\"]])))\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in dataset[\"context\"]:\n",
    "            while True:\n",
    "                neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "                if not c in corpus[neg_idxs]:\n",
    "                    p_neg = corpus[neg_idxs]\n",
    "\n",
    "                    p_with_neg.append(c)\n",
    "                    p_with_neg.extend(p_neg)\n",
    "                    break\n",
    "\n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(dataset[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        p_seqs = tokenizer(p_with_neg, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg+1, max_len)\n",
    "        # p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg+1, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], # p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], # q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "        valid_seqs = tokenizer(\n",
    "            dataset[\"context\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        passage_dataset = TensorDataset(\n",
    "            valid_seqs[\"input_ids\"],\n",
    "            valid_seqs[\"attention_mask\"],\n",
    "            # valid_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        self.passage_dataloader = DataLoader(\n",
    "            passage_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, args=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "        print(args.device)\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "        # for _ in range(int(args.num_train_epochs)):\n",
    "        for _ in train_iterator:\n",
    "\n",
    "            with tqdm(self.train_dataloader, unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "\n",
    "                    p_encoder.train()\n",
    "                    q_encoder.train()\n",
    "            \n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        # \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "            \n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[2].to(args.device),\n",
    "                        \"attention_mask\": batch[3].to(args.device),\n",
    "                        # \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    # (batch_size*(num_neg+1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)[1]\n",
    "                    # (batch_size, emb_dim)\n",
    "                    q_outputs = self.q_encoder(**q_inputs)[1]\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    p_outputs = p_outputs.view(batch_size, -1, self.num_neg+1)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  #(batch_size, num_neg + 1)\n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.p_encoder.zero_grad()\n",
    "                    self.q_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1, args=None, p_encoder=None, q_encoder=None):\n",
    "    \n",
    "        if args is None:\n",
    "            args = self.args\n",
    "\n",
    "        if p_encoder is None:\n",
    "            p_encoder = self.p_encoder\n",
    "\n",
    "        if q_encoder is None:\n",
    "            q_encoder = self.q_encoder\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_encoder.eval()\n",
    "            q_encoder.eval()\n",
    "\n",
    "            q_seqs_val = self.tokenizer(\n",
    "                [query],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(args.device)\n",
    "            q_emb = q_encoder(**q_seqs_val)[1].to(\"cpu\")  # (num_query=1, emb_dim)\n",
    "\n",
    "            p_embs = []\n",
    "            for batch in tqdm(self.passage_dataloader):\n",
    "\n",
    "                batch = tuple(t.to(args.device) for t in batch)\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    # \"token_type_ids\": batch[2]\n",
    "                }\n",
    "                p_emb = p_encoder(**p_inputs)[1].to(\"cpu\")\n",
    "                p_embs.append(p_emb)\n",
    "\n",
    "        # (num_passage, emb_dim)\n",
    "        p_embs = torch.stack(p_embs, dim=0).view(len(self.passage_dataloader.dataset), -1)\n",
    "\n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "        print(dot_prod_scores)\n",
    "        print(rank)\n",
    "\n",
    "        return rank[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fe7bf4d-b4bb-4ae3-aa01-bc7651aa698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c653dfd1-8567-4c9a-b7fb-f4a61d062df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = DenseRetrieval(args=args, \n",
    "                           dataset=training_dataset,\n",
    "                           # eval_dataset=None, \n",
    "                           num_neg=2, \n",
    "                           tokenizer=tokenizer, \n",
    "                           p_encoder=p_encoder, \n",
    "                           q_encoder=q_encoder\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac2f4ba0-b81f-4dc5-9d6e-06157eba1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df244fc87a244582a097189196252c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178bb0e264204a839ace36477ea235df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cdfb8385364015b1b0261e3976a3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b11e3e8b164c33b4a6ca348d84cd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470a8144959b490c8ea4f46b61716cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb495e78e53485abab49c0e5d1cd511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d099ef3-848f-409d-baea-7020e7e292ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "링컨이 조지 워싱턴 장군에 합세하여 우익을 통솔했던 전투는?\n",
      "1776년, 링컨은 준장에 이어 소장으로 승격되어, 보스턴 지역의 모든 매사추세츠 부대를 지휘했다. 영국군이 보스턴에서 철수한 후 링컨은 뉴욕에서 조지 워싱턴 장군에 합류, 화이트 플래인스 전투에서는 우익을 지휘했다. 인디펜던스 요새 전투 직후에 링컨은 대륙군 소장이 되었다.\\n\\n1777년 9월, 링컨은 새러토가 근처에 있던 호레이쇼 게이츠의 부대에 합류하여 새러토가 전투에 참전했다. 그러나 뒤꿈치에 머스켓 총알을 맞은 상처 때문에 새러토가 전투에서는 큰 역할을 하지 못했다. 이때의 상처로 한쪽 다리가 짧은 상태가 되었다. 이 상처가 아물자 링컨은 1778년 9월에 남부 방면 군의 지휘관에 임명되었다. 링컨은 1779년 10월 9일의 조지아의 사바나 공격에 참가, 사우스캐롤라이나의 찰스턴으로 철수를 하도록 만들었다. 링컨에게는 운이 나빴지만, 찰스턴 시내로 몰려 포위를 당했다.\\n\\n1780년 5월 12일에는 영국군의 헨리 클린턴 장군에게 항복하게 되었다. 이것은 독립 전쟁에서 대륙군 최대의 패배였다. 링컨은 항복 있어서, 전사의 명예를 부정당하게 되고 마음 속으로 화가 치밀었다. 링컨은 포로 교환 식으로 석방되었지만, 해제됐지만, 조사위원회에서는 어떤 허물도 문제되지 않았다. 링컨은 곧바로 워싱턴의 주력 부대로 복귀하여, 남부 버지니아로 가서 1781년 10월 19일의 요크타운에서 영국군의 항복까지 큰 역할을 했다. 이 항복은 콘월리스 경이 패배의 굴욕을 느끼고 워싱턴 장군에 직접 군도를 전달하는 항복 의식을 거부했다. 결국 부관인 찰스 오하라가 콘월리스 대신 서게 되었으며, 워싱턴의 부관이었던 링컨이 콘월리스의 군도를 받았다.\n"
     ]
    }
   ],
   "source": [
    "IDX = 10\n",
    "print(training_dataset['question'][IDX])\n",
    "print(training_dataset['context'][IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff2d5f35-d5a7-4e74-a5d9-3f4f97935278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "링컨이 조지 워싱턴 장군에 합세하여 우익을 통솔했던 전투는?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a1fb7691bc45c1bcfb6d0d8cc26405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[86.1941, 86.1700, 86.1245, 86.2547, 86.1179, 86.2094, 86.0226, 86.0526,\n",
      "         86.0990, 86.0593, 86.2248, 86.0264, 85.9630, 86.3734, 86.0847, 86.0439,\n",
      "         86.3819, 86.1201, 86.0742, 86.1609, 86.0990, 86.0352, 86.1289, 86.1771,\n",
      "         86.1498, 86.1031, 86.0030, 86.1216, 86.0489, 86.2452, 86.1868, 86.3122,\n",
      "         86.0979, 85.9766, 86.0921, 86.1825, 86.0275, 86.2494, 85.9991, 86.0983,\n",
      "         86.2463, 86.1144, 85.9928, 86.1971, 86.0700, 86.0330, 86.1281, 86.2053,\n",
      "         86.1489, 86.1334, 86.3268, 86.0768, 86.0330, 85.9015, 86.2053, 86.1359,\n",
      "         85.9843, 86.1666, 86.0735, 86.3388, 86.1852, 86.2069, 86.2173, 86.0330,\n",
      "         86.1078, 86.1198, 86.0115, 86.1644, 86.0768, 86.0023, 86.1617, 86.1920,\n",
      "         85.9641, 86.0306, 86.1577, 86.0604, 86.3583, 85.9388, 85.9913, 86.2101,\n",
      "         86.2272, 86.2305, 86.0813, 85.9851, 86.3177, 86.0586, 86.0588, 86.1189,\n",
      "         86.3018, 86.1953, 86.1147, 86.3724, 86.0908, 85.8900, 86.1059, 86.0357,\n",
      "         86.2109, 86.3115, 85.9824, 86.0272, 86.1489, 86.3163, 86.2042, 86.1430,\n",
      "         86.1744, 86.2765, 86.1761, 86.4204, 86.2362, 86.1648, 86.1340, 86.2923,\n",
      "         86.0732, 86.0429, 85.9822, 86.1364, 86.1116, 85.8215, 86.2086, 86.3475,\n",
      "         85.9832, 86.1988, 85.9537, 86.3724, 86.0513, 86.0957, 86.0481, 85.9677,\n",
      "         86.2236, 86.3029, 86.2688, 86.2210, 86.0973, 86.1483, 86.0144, 86.0966,\n",
      "         85.8954, 86.0841, 86.0066, 85.9501, 86.2602, 86.1615, 85.9864, 86.0116,\n",
      "         86.2768, 86.2462, 86.1572, 86.2659, 86.1323, 86.1525, 86.0998, 86.1368,\n",
      "         86.2496, 86.1501, 86.2003, 86.2399, 86.2922, 86.0921, 86.1704, 86.2216,\n",
      "         86.1480, 86.1972, 86.2659, 86.1059, 86.1940, 86.2521, 86.0879, 86.0735,\n",
      "         86.1846, 86.0489, 86.2623, 86.2614, 86.0606, 86.3228, 86.1861, 86.0631,\n",
      "         86.4044, 85.9976, 86.0603, 86.1927, 86.1450, 86.1927, 86.1774, 86.0726,\n",
      "         86.0078, 86.1218, 86.1953, 86.1389, 86.0557, 86.1783, 86.5024, 86.2051,\n",
      "         86.1559, 86.2331, 86.1351, 86.0768, 86.2036, 85.9652, 86.1697, 86.3010,\n",
      "         86.0428, 86.1621, 86.2424, 86.0664, 86.1445, 85.8904, 86.0865, 86.1253,\n",
      "         86.1038, 86.2063, 86.1712, 86.0520, 86.2722, 86.2256, 86.0555, 85.9633,\n",
      "         85.9271, 86.2044, 86.1241, 86.3992, 85.9890, 86.1654, 86.2423, 86.1167,\n",
      "         86.1177, 86.1658, 86.2248, 86.2655, 86.1930, 86.2009, 86.2135, 86.0569,\n",
      "         86.2494, 86.3495, 86.1412, 85.9676, 86.2532, 86.1163, 86.1533, 86.2603,\n",
      "         86.2468, 86.3789, 86.2549, 86.1179, 86.0997, 86.2721, 86.2722, 86.2026,\n",
      "         86.1301, 86.4563, 86.1522, 86.0120, 86.3589, 85.9551, 86.2895, 86.1359,\n",
      "         86.2249, 86.1678, 86.1131, 86.1781, 86.0290, 86.3984, 86.0513, 86.0608,\n",
      "         85.9509, 86.1558, 86.1290, 86.1230, 86.1404, 86.0713, 85.9902, 86.2708,\n",
      "         86.1877, 86.0811, 85.9962, 85.9738, 86.2640, 86.1581, 86.2555, 86.1052,\n",
      "         86.1044, 86.1327, 86.2416, 86.1990, 86.1982, 86.2167, 86.1299, 86.0611,\n",
      "         86.1001, 86.1381, 86.2085, 86.0955, 86.0733, 85.8725, 86.1929, 86.1638,\n",
      "         85.9315, 85.8646, 86.0447, 86.0290, 86.3384, 86.1405, 85.9951, 86.0837,\n",
      "         86.2009, 86.2528, 85.9940, 86.2143, 86.0493, 86.1448, 86.0939, 86.0968,\n",
      "         86.2907, 86.2113, 86.0117, 86.2204, 86.3966, 86.2256, 86.1126, 86.2264,\n",
      "         86.1270, 85.9348, 86.1177, 86.0324, 86.0833, 86.2132, 86.4016, 86.2210,\n",
      "         86.1774, 86.0805, 86.2848, 86.1122, 86.2462, 86.2926, 86.0983, 86.4502,\n",
      "         85.9991, 86.0996, 86.2919, 86.0545, 86.2360, 86.0507, 86.0049, 86.2374,\n",
      "         86.2010, 86.1116, 86.2375, 86.3172, 86.0749, 86.1105, 86.2226, 85.7548,\n",
      "         86.2318, 85.9986, 86.2147, 86.0407, 86.1442, 86.3058, 86.3482, 86.0819,\n",
      "         86.0844, 86.1119, 85.8849, 86.1727, 86.2855, 86.0913, 86.2052, 85.9738,\n",
      "         86.2706, 86.0664, 85.8386, 86.1177, 86.3854, 86.2577, 85.8963, 86.1828,\n",
      "         86.2031, 86.0925, 86.0131, 86.3386, 86.2885, 85.9152, 85.9689, 86.2645,\n",
      "         86.4492, 86.1266, 86.3269, 86.2271, 86.0576, 86.0045, 86.0272, 86.1471,\n",
      "         86.1653, 85.9895, 86.0428, 86.2351, 86.1060, 86.1893, 86.0956, 86.2113,\n",
      "         85.9220, 86.2146, 86.1804, 86.1519, 86.2065, 86.1703, 86.1443, 86.1694,\n",
      "         86.0800, 85.9133, 86.1562, 86.1929, 86.1361, 86.0009, 86.0508, 86.0461,\n",
      "         86.2028, 86.2031, 86.2969, 86.0106, 86.1920, 86.1880, 86.2649, 86.1708,\n",
      "         85.8051, 86.2167, 86.1168, 86.0038, 86.1637, 86.1827, 86.2621, 86.1581,\n",
      "         86.2372, 86.1201, 86.2580, 86.2420, 86.2453, 86.0882, 86.0124, 85.7213,\n",
      "         86.1965, 86.1797, 86.2068, 86.1144, 86.1634, 86.4465, 86.1982, 86.0601,\n",
      "         86.1723, 86.2224, 86.0601, 86.0254, 85.9438, 86.2258, 86.1579, 86.3845,\n",
      "         85.9359, 86.0547, 86.0984, 85.8717, 86.0664, 86.1367, 86.1693, 85.8777,\n",
      "         85.9539, 85.9843, 85.9769, 86.0545, 86.1533, 86.0112, 86.0396, 85.8227,\n",
      "         86.0466, 86.2192, 86.2360, 86.0684, 86.2655, 86.1634, 86.3226, 86.0231,\n",
      "         86.0759, 85.9010, 86.1694, 85.9968, 86.0744, 86.1707, 85.9605, 86.1795,\n",
      "         86.0699, 86.2379, 86.1921, 86.0306, 86.1510, 86.2221, 86.1366, 86.0229,\n",
      "         86.1283, 86.0342, 86.1519, 86.2029, 86.1059, 85.9079, 86.0299, 86.1599,\n",
      "         86.0781, 86.1423, 86.1822, 86.1375, 86.2661, 86.1477, 86.3932, 85.7833,\n",
      "         86.0670, 86.0508, 85.8227, 86.3068, 86.2615, 86.2706, 86.1986, 85.9264,\n",
      "         85.9981, 85.8646, 86.1127, 86.3269, 85.8900, 86.0844, 86.0552, 86.2706,\n",
      "         86.0911, 86.2027, 86.1046, 85.8614, 86.1432, 85.9262, 86.1567, 86.3978,\n",
      "         86.2599, 86.1609, 86.3299, 85.9741, 85.9685, 85.9868, 85.9529, 86.3509,\n",
      "         86.2092, 86.1510, 86.2926, 86.2528, 86.3202, 86.2904, 86.0541, 86.1167,\n",
      "         86.2085, 86.2298, 85.8717, 85.8904, 86.1381, 86.2659, 86.2192, 86.0800,\n",
      "         86.0748, 86.2050, 86.0502, 85.8413, 86.1243, 86.3623, 86.3043, 86.2374,\n",
      "         86.0090, 86.1389, 86.0595, 86.0329, 86.1159, 86.0449, 86.1176, 86.0001,\n",
      "         86.0007, 86.1142, 86.0935, 86.2229, 86.1786, 86.3470, 86.1126, 86.1807,\n",
      "         86.0662, 86.1846, 86.3058, 86.3374, 86.1634, 86.3086, 86.3789, 86.1301,\n",
      "         86.3219, 85.9928, 86.0863, 86.0669, 86.0576, 86.0767, 86.0594, 86.0508,\n",
      "         86.4059, 86.1739, 86.2173, 86.1079, 86.1360, 86.2221, 86.0117, 86.2410,\n",
      "         86.1822, 86.2792, 86.1187, 86.1723, 86.3248, 86.0768, 86.0123, 86.1162,\n",
      "         86.1868, 86.0886, 86.3780, 86.2639, 86.1233, 86.2496, 86.2302, 86.2376,\n",
      "         86.2166, 86.5409, 85.9840, 86.0555, 86.1228, 86.1385, 86.1302, 86.1418,\n",
      "         85.9153, 86.1106, 86.2098, 86.3240, 85.9330, 86.2980, 86.0176, 86.3319,\n",
      "         86.2187, 85.9248, 86.1991, 86.2025, 85.9595, 86.1448, 85.9685, 85.9438,\n",
      "         86.1015, 86.0804, 85.9153, 86.1012, 85.9690, 86.0859, 86.0439, 86.0770,\n",
      "         86.2280, 86.3286, 86.0247, 86.3068, 86.3384, 86.2656, 86.0275, 85.9629,\n",
      "         86.1125, 86.2348, 86.1458, 86.1158, 86.2036, 86.3122, 86.2469, 86.2363,\n",
      "         86.1511, 86.2086, 85.9955, 86.0376, 86.1243, 86.2165, 86.5024, 86.2124,\n",
      "         86.1419, 86.2267, 86.1120, 86.3101, 86.2316, 86.1398, 86.0016, 86.2032,\n",
      "         85.8833, 86.0419, 86.0045, 86.1968, 85.9398, 85.9799, 86.0460, 86.2210,\n",
      "         86.1855, 85.9751, 86.0002, 86.2962, 86.1134, 86.3029, 86.0617, 86.2438,\n",
      "         86.2113, 86.1882, 86.1364, 86.0624, 86.1858, 86.1513, 86.2361, 86.1320,\n",
      "         85.7234, 86.0879, 86.2022, 86.1129, 85.8462, 86.2987, 86.2840, 86.2238,\n",
      "         86.2225, 86.2204, 86.1125, 86.2438, 86.3277, 86.3485, 86.2870, 86.0374,\n",
      "         86.1592, 86.1228, 86.4568, 85.8203, 86.2825, 85.9765, 86.1783, 85.9440,\n",
      "         86.3669, 86.2594, 86.2232, 86.0308, 86.2376, 86.1140, 86.1147, 85.9382,\n",
      "         86.2395, 86.2208, 86.2194, 86.4317, 86.1347, 85.8137, 86.1245, 85.9071,\n",
      "         86.2329, 86.0871, 85.8390, 86.1243, 86.1471, 86.2159, 86.2295, 86.0726,\n",
      "         86.0401, 86.1360, 85.9938, 86.1598, 86.0726, 86.0956, 86.0030, 86.1312,\n",
      "         86.0739, 86.2064, 86.1622, 86.1260, 86.1159, 85.9976, 85.9016, 86.3583,\n",
      "         86.0172, 86.1489, 86.1216, 86.1245, 86.1592, 86.0880, 86.1663, 86.2580,\n",
      "         86.1522, 86.1890, 86.0295, 86.2240, 86.0639, 86.1653, 86.1335, 86.0426,\n",
      "         86.1572, 86.2838, 86.3978, 86.2027, 86.1985, 86.0860, 85.9539, 86.3398,\n",
      "         86.1956, 86.1008, 86.1316, 86.2441, 86.2361, 86.0290, 86.1030, 86.4563,\n",
      "         86.2712, 86.2241, 86.2486, 86.3734, 86.0672, 86.0284, 86.2265, 85.8203,\n",
      "         86.1487, 86.0392, 86.1307, 86.2527, 86.0408, 86.0977, 86.2975, 86.1921,\n",
      "         85.8972, 86.1409, 86.2510, 86.1646, 86.2494, 86.2655, 86.2452, 86.2402,\n",
      "         86.0380, 86.3232, 86.0493, 86.1129, 86.1214, 86.1830, 86.1880, 85.9513,\n",
      "         86.3312, 86.0727, 86.1569, 86.2139, 86.0727, 86.1050, 86.0572, 86.2115,\n",
      "         86.3117, 86.3536, 86.0870, 86.1027, 86.0126, 86.2580, 86.1266, 86.1921,\n",
      "         86.0633, 86.2570, 86.3304, 86.1090, 85.8904, 86.1173, 86.2854, 86.2341,\n",
      "         86.1418, 86.2629, 86.0254, 86.0203, 86.2721, 86.0955, 86.2496, 86.1493,\n",
      "         86.1939, 86.2489, 86.1159, 86.5024, 86.1933, 86.0984, 85.9359, 86.1797,\n",
      "         85.8920, 85.9603, 86.1422, 86.1915, 86.3232, 86.3115, 86.2316, 86.1894,\n",
      "         86.3638, 85.9629, 86.0278, 86.1654, 86.1140, 86.1374, 85.9595, 86.0302,\n",
      "         86.1803, 85.8618, 86.2291, 86.0987, 86.1457, 86.2464, 86.2402, 86.1331,\n",
      "         85.9440, 86.1049, 86.1765, 86.1367, 86.0127, 86.1349, 86.0956, 86.2075,\n",
      "         86.3014, 86.3150, 86.1340, 86.0276, 86.2322, 86.0919, 85.7213, 85.8278,\n",
      "         85.9932, 86.1264, 86.2610, 86.0942, 86.2389, 86.0893, 86.1177, 86.3218,\n",
      "         86.2008, 86.1942, 86.0975, 86.2625, 86.2507, 86.2027, 86.1169, 86.1143,\n",
      "         86.1341, 86.2848, 86.2010, 86.2134, 86.0466, 86.1900, 86.0786, 86.2599,\n",
      "         86.2059, 85.9438, 86.1124, 86.3844, 86.1533, 85.8900, 86.2259, 86.2308,\n",
      "         86.1537, 85.9064, 86.4866, 85.9991, 86.3671, 85.9064, 86.3304, 86.1752,\n",
      "         86.0302, 86.2215, 86.1030, 86.3668, 86.2160, 86.1181, 86.3108, 86.1943,\n",
      "         86.2979, 86.1968, 86.1287, 86.1137, 86.2192, 86.1781, 86.1927, 86.0472,\n",
      "         86.2348, 86.1119, 86.0225, 86.0569, 86.0305, 86.0144, 86.0518, 86.2431,\n",
      "         86.0489, 86.0072, 86.0802, 86.2897, 86.2143, 86.4060, 86.0774, 86.2423,\n",
      "         86.0048, 86.0911, 85.9884, 86.0769, 86.1764, 85.9579, 85.9650, 86.0316]])\n",
      "tensor([625, 875, 190, 678, 954, 730, 807, 249, 335, 384, 445, 747, 107, 989,\n",
      "        600, 176, 326, 219, 261, 794, 535, 316, 510, 372, 455, 947,  16, 241,\n",
      "        590, 618,  13, 811,  91, 123, 956, 736, 963, 888, 565, 252, 775,  76,\n",
      "        849, 543, 233, 725, 358, 119, 581, 799,  59, 379, 660, 300, 587, 639,\n",
      "        840, 958, 858, 538, 657, 724, 523, 386,  50, 612, 635, 833, 884, 173,\n",
      "        478, 592, 927, 548,  84, 347, 101, 913,  31, 669, 848,  97, 885, 966,\n",
      "        683, 589, 659, 515, 357, 586, 566, 701, 129,  88, 912, 199, 717, 637,\n",
      "        968, 822, 418, 699, 546, 333, 111, 156, 338, 312, 549, 987, 254, 380,\n",
      "        726, 364, 862, 330, 937, 718, 793, 732, 609, 144, 105, 246, 212, 868,\n",
      "        245, 808, 271, 368, 517, 527, 130, 508, 557, 162, 147, 661, 476, 227,\n",
      "        829, 422, 383, 276, 619, 865, 931, 170, 430, 516, 171, 922, 239, 140,\n",
      "        536, 943, 737, 434, 853, 783, 373, 857, 278, 242,   3, 236, 547, 305,\n",
      "        819, 165, 826, 932, 870, 621, 152, 232, 828,  37, 873, 810, 670, 240,\n",
      "        901,  40, 332, 145, 436,  29, 830, 803, 723, 703, 983, 202, 222, 991,\n",
      "        435, 282, 607, 831, 902, 155, 744, 924, 489, 740, 623, 346, 343, 567,\n",
      "        432, 671, 108, 804, 710, 340, 474, 395, 976, 665, 863, 193, 752, 916,\n",
      "        352, 684, 886, 951,  81, 622, 553, 758, 898, 656,  80, 387, 681, 814,\n",
      "        319, 950, 453, 213, 317, 256,  10, 226, 809, 787, 719, 128, 738, 579,\n",
      "        350, 720, 449, 605, 493, 159, 961, 131, 327, 695, 745, 315, 721, 746,\n",
      "        473, 558, 972, 640,  62, 602, 285, 425, 624, 677, 964, 757, 354, 401,\n",
      "        988, 307, 843, 230, 939, 325, 679, 847, 313, 704, 399,  96,  79, 634,\n",
      "          5, 544, 118, 673, 290, 552, 911,  61, 442, 404, 769, 209, 944,  47,\n",
      "         54, 366, 191, 561, 217, 102, 668, 196, 687, 417, 376, 499, 416, 933,\n",
      "        529, 795, 247, 643, 714, 344, 938, 229, 304, 928, 154, 642, 283, 121,\n",
      "        518, 796, 284, 446, 161,  43, 691, 969, 440, 800,  89, 186, 967, 929,\n",
      "          0, 164, 872, 876, 228, 294, 411, 179, 974, 181, 490, 855, 823,  71,\n",
      "        420, 883, 941, 887, 397, 785, 705, 421, 838, 272, 616,  30, 174, 708,\n",
      "        696,  60, 168, 585, 837, 375, 429,  35, 506, 608, 583, 402, 896, 441,\n",
      "        879, 487, 580, 189, 734, 259, 973, 182, 328,  23, 906, 996, 106, 959,\n",
      "        104, 601, 363, 611, 448, 210, 423, 485, 158, 405,   1, 198, 407, 482,\n",
      "        462, 257,  57, 782, 225, 891, 221, 789, 392, 109, 827,  67, 295, 428,\n",
      "        588, 444, 477, 770, 201,  70, 141, 537,  19, 503, 763, 728, 780, 277,\n",
      "        431, 454,  74, 792, 146, 842, 534, 410, 192, 265, 952, 238, 948, 468,\n",
      "        149, 784, 250, 498, 403, 709, 672, 492, 545, 153,  24, 871,  48, 100,\n",
      "        777, 816, 133, 160, 509, 391, 756, 666, 900, 180, 309, 645, 204, 406,\n",
      "        356, 532, 103, 505, 882, 680, 864, 631, 234, 825, 301, 268, 685, 569,\n",
      "        187, 629, 289, 556, 507, 893, 151, 907, 461, 494, 706, 115, 412, 604,\n",
      "        761,  55, 255, 194, 909, 748, 936, 914, 110, 790,  49, 903, 281, 148,\n",
      "        711, 802, 767, 818, 630, 591, 248, 286, 266,  22, 970, 496,  46, 320,\n",
      "        385, 854, 921, 771, 207,   2, 750, 779, 676, 755, 564, 218, 620, 267,\n",
      "        628, 729, 185,  27, 778, 836,  17, 433,  65,  87, 610, 965, 243,   4,\n",
      "        322, 371, 926, 224, 574, 861, 934, 426, 223, 551, 237, 615, 874, 572,\n",
      "        772, 667, 742,  90,  41, 443, 935, 577, 892, 741, 971, 700, 258, 715,\n",
      "        835, 522, 582, 318, 722, 664, 946, 331, 682, 977, 361, 116, 345, 633,\n",
      "        349, 859, 603,  64, 396, 500, 163,  94, 279, 845, 905, 530, 280, 208,\n",
      "         25, 806, 962, 851, 648, 651, 801, 288, 150, 244, 337,  20,   8, 899,\n",
      "        877, 458,  39, 334,  32, 821, 930, 132, 311, 135, 125, 765, 910, 398,\n",
      "        869, 291, 923, 310, 578, 377, 157,  34, 917, 365, 528, 993,  92, 925,\n",
      "        617, 437, 781, 713, 166, 753, 850, 206, 594, 797, 653,  14, 360, 525,\n",
      "        137, 303, 324, 359,  82, 273, 329, 649, 986, 559, 408, 942, 504, 990,\n",
      "        655, 995, 613,  68, 195,  51, 597, 480, 348, 560, 484,  18, 768,  58,\n",
      "        167, 292, 112, 841, 844, 764, 759, 183, 269,  44, 488, 475, 812, 512,\n",
      "        595, 460, 369, 203, 584, 788, 856, 175, 707, 702, 287, 263, 172,  75,\n",
      "        178, 447, 450, 570, 598,   9,  86,  85, 596, 388, 846, 979, 231, 188,\n",
      "        214, 627, 526, 457, 467, 339, 550,   7, 211, 982, 124, 262, 599, 513,\n",
      "        414, 341, 562, 834, 308,  28, 169, 984, 126, 975, 940, 472, 415, 694,\n",
      "        573, 298,  15, 654, 113, 200, 394, 791, 689, 820, 355, 760, 470, 817,\n",
      "        832, 675, 727,  95,  21, 497,  63,  52,  45, 571, 323, 999, 739, 491,\n",
      "         73, 980, 895, 960, 502, 786, 299, 260, 805, 813, 890, 915,  36, 662,\n",
      "        390,  99,  11, 451, 866, 658, 479, 495,   6, 978, 867, 638, 776, 134,\n",
      "        981, 378, 908, 852, 438, 614, 251, 314, 606, 143,  66, 469, 419, 568,\n",
      "        184, 985, 138, 342, 992, 389, 690, 427,  26, 766,  69, 686, 413, 576,\n",
      "        698, 575,  38, 336, 955, 353, 520, 773, 177, 483, 274, 674, 302, 306,\n",
      "        762, 920,  42, 593,  78, 270, 393, 220, 994, 541, 142,  83, 465,  56,\n",
      "        626, 120,  98, 114, 693, 466,  33, 733, 697, 539, 367, 275, 652, 382,\n",
      "        540, 646, 127, 235, 197, 998,  72, 215,  12, 889, 663, 486, 881, 894,\n",
      "        644, 997, 253, 464, 798, 122, 542, 839, 264, 139, 904, 735, 647, 452,\n",
      "        945, 692,  77, 743, 456, 878, 321, 636, 296, 216, 519, 533, 641, 400,\n",
      "        650, 632, 381, 409, 501, 751, 953, 957, 774,  53, 481, 824, 374, 136,\n",
      "        880, 205, 555, 860, 524,  93, 949, 362, 688, 463, 293, 554, 459, 521,\n",
      "        297, 897, 531, 716, 563, 754, 370, 919, 514, 471, 117, 815, 731, 749,\n",
      "        424, 511, 351, 712, 918, 439])\n",
      "영동 방언(嶺東方言)은 강원도 영동 지방(강릉시, 동해시, 속초시, 삼척시, 태백시, 고성군, 양양군, 통천군)과 영동 남부 지역이자 지리상의 영서 지방 일부(영월군, 평창군, 정선군), 경상북도 울진군, 함경남도의 금야군 이남 지역에서 사용되는 한국어의 방언(사투리)이다. 강원도는 대관령을 경계로 영서(嶺西)와 영동 지역으로 나뉘는데, 백두대간의 높은 산맥 때문에 두 지역 사이에는 문화적, 언어적 차이가 나타나게 되었다. 다만, 영서 남부의 영월·평창·정선 지역은 지리적으로 경상도와 가깝고 역사·문화적으로 영동 지방과 교류가 활발하여 영동 방언을 쓰게 된 것으로 추정된다.\\n\\n영동 방언은 말소리의 높이(성조)와 길이(음장)가 모두 뜻을 구별하는 데에 이용되며, 발음할 수 있는 음소의 가짓수도 한국어의 여러 방언들 가운데에 가장 많다. 특히, 서울 등 다른 지역의 방언과 달리 'ㅚ'와 'ㅟ'를 단순모음으로 발음한다는 점이 특기할 만하다. 뜻을 구별하는 데에 성조(聲調)를 쓴다는 점 뿐만 아니라 어휘면에 있어서도 동남 방언(경상 방언)과 닮은 특질들이 상당수 발견된다. 특이한 점은 ~지라우와 ~하라우라는 말이 간혹 쓰이는데, 북한의 함경 방언 영향을 받은 것으로 추정된다. 이들은 서남 방언에서도 사용된다.\n",
      "옌바이는 면적이 6899.5km²이고, 홍강은 지방을 통과한다. 옌바이는 험준한 산악 풍경과 계곡의 푸른 논밭으로 특징지어지는 산악 지방이다. 호앙리엔선 산맥은 지방을 관통한다. 홍강(혹은 타오 강)과 짜이강이 그 지방을 흐른다. 이 강들은 중국 윈난에서 발원한다. 옌바이성의 이 두 강줄기에 의해 만들어진 계곡은 울퉁불퉁한 영토지만 비옥하다. 무옹로 평원은 옌바이성의 곡창지대이다. \\n\\n지형은 동쪽에서 서쪽으로, 남쪽에서 북으로 솟아 있어 가파르다. 평균 고도는 해발 약 600m이다. 홍강 유역의 좌안과 홍강의 높은 우안에 있는 저지대와 홍강과 다강 사이의 고원에는 산이 많다. 두 개의 주요 강인 홍강과 다강 외에, 이 지방에는 약 200개의 운하, 작은 개울, 큰 호수, 늪이 있다. 탁바 호수는 234km²의 면적과 1,331개의 섬과 언덕을 가진 인공호수도 옌바이성에 위치해 있다. 30억~39억 입방미터의 용수를 보유하고 있으며, 당초의 의도는 베트남 최초의 대규모 수력 발전 사업 중 하나인 탁바 수력발전소를 가동하는 것이었다. 수천 개의 언덕과 섬에는 훔, 꺼우꾸오이, 박싸를 포함한 많은 동굴이 있다. 탁바 사원은 호수 지역에 위치해 있다. 탁바호는 그 지방의 서부 지역의 기후 패턴을 덥고 건조한 상태에서 적당한 상태로 바꾸었다.\n",
      "옌바이는 면적이 6899.5km²이고, 홍강은 지방을 통과한다. 옌바이는 험준한 산악 풍경과 계곡의 푸른 논밭으로 특징지어지는 산악 지방이다. 호앙리엔선 산맥은 지방을 관통한다. 홍강(혹은 타오 강)과 짜이강이 그 지방을 흐른다. 이 강들은 중국 윈난에서 발원한다. 옌바이성의 이 두 강줄기에 의해 만들어진 계곡은 울퉁불퉁한 영토지만 비옥하다. 무옹로 평원은 옌바이성의 곡창지대이다. \\n\\n지형은 동쪽에서 서쪽으로, 남쪽에서 북으로 솟아 있어 가파르다. 평균 고도는 해발 약 600m이다. 홍강 유역의 좌안과 홍강의 높은 우안에 있는 저지대와 홍강과 다강 사이의 고원에는 산이 많다. 두 개의 주요 강인 홍강과 다강 외에, 이 지방에는 약 200개의 운하, 작은 개울, 큰 호수, 늪이 있다. 탁바 호수는 234km²의 면적과 1,331개의 섬과 언덕을 가진 인공호수도 옌바이성에 위치해 있다. 30억~39억 입방미터의 용수를 보유하고 있으며, 당초의 의도는 베트남 최초의 대규모 수력 발전 사업 중 하나인 탁바 수력발전소를 가동하는 것이었다. 수천 개의 언덕과 섬에는 훔, 꺼우꾸오이, 박싸를 포함한 많은 동굴이 있다. 탁바 사원은 호수 지역에 위치해 있다. 탁바호는 그 지방의 서부 지역의 기후 패턴을 덥고 건조한 상태에서 적당한 상태로 바꾸었다.\n",
      "옌바이는 면적이 6899.5km²이고, 홍강은 지방을 통과한다. 옌바이는 험준한 산악 풍경과 계곡의 푸른 논밭으로 특징지어지는 산악 지방이다. 호앙리엔선 산맥은 지방을 관통한다. 홍강(혹은 타오 강)과 짜이강이 그 지방을 흐른다. 이 강들은 중국 윈난에서 발원한다. 옌바이성의 이 두 강줄기에 의해 만들어진 계곡은 울퉁불퉁한 영토지만 비옥하다. 무옹로 평원은 옌바이성의 곡창지대이다. \\n\\n지형은 동쪽에서 서쪽으로, 남쪽에서 북으로 솟아 있어 가파르다. 평균 고도는 해발 약 600m이다. 홍강 유역의 좌안과 홍강의 높은 우안에 있는 저지대와 홍강과 다강 사이의 고원에는 산이 많다. 두 개의 주요 강인 홍강과 다강 외에, 이 지방에는 약 200개의 운하, 작은 개울, 큰 호수, 늪이 있다. 탁바 호수는 234km²의 면적과 1,331개의 섬과 언덕을 가진 인공호수도 옌바이성에 위치해 있다. 30억~39억 입방미터의 용수를 보유하고 있으며, 당초의 의도는 베트남 최초의 대규모 수력 발전 사업 중 하나인 탁바 수력발전소를 가동하는 것이었다. 수천 개의 언덕과 섬에는 훔, 꺼우꾸오이, 박싸를 포함한 많은 동굴이 있다. 탁바 사원은 호수 지역에 위치해 있다. 탁바호는 그 지방의 서부 지역의 기후 패턴을 덥고 건조한 상태에서 적당한 상태로 바꾸었다.\n",
      "반주 (스위스)\\n스위스의 26개 주 가운데 6개의 반주(半州)가 존재하며, 반주의 기원은 주마다 각각 다르다. 옵발덴주와 니트발덴주는 \"운터발덴 주\"로 존속하였지만, 지리적인 문제 및 나폴레옹 시대에 존재했던 헬베티아 공화국 연방 가입 문제 등으로 갈라져 반주가 되었으며, 바젤슈타트주와 바젤란트주는 이전에 \"바젤 주\"로 존속하였으나 도시와 농촌 간의 문제, 바젤란트주의 독립 요구로 갈라져 반주가 되었다. 아펜첼이너로덴주와 아펜첼아우서로덴주는 \"아펜첼 주\"로 존속하였다가 1597년 스위스의 종교개혁 때 갈라져 가톨릭교도가 많은 아펜첼이너로덴주와 개신교도가 많은 아펜첼아우서로덴으로 나뉘어 반주가 되었다.\\n\\n반주(半州)는 독자적인 발언권을 가지지 못했으나, 1999년 스위스 연방 헌법 개정과 함께 반주(半州)법이 폐지되면서 주의 독자성을 인정받게 되었다. 다만, 기존 주들과의 형평을 고려해서 몇가지 점에서는 차별이 있으며, 제일 주요한 구분점은 연방평의회에 보내는 각주의 대표를 다른 주들은 2명씩 보내는 반면에, 반주의 경우 1명만 보낼 수 있으며, 이외에 투표상에서도 약간의 차이점이 있다.\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset['question'][IDX])\n",
    "# print(retrieval.get_relevant_doc(training_dataset['question'][IDX]))\n",
    "retrieval_list = retrieval.get_relevant_doc(training_dataset['question'][IDX], k=5)\n",
    "for i in retrieval_list.tolist():\n",
    "    print(training_dataset['context'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141064bb-9631-4fad-9631-f341d2250742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거 에폭마다 eval 진행하게 함수 추가하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
