{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116de6d1-b38a-48f2-b17c-294cc9511770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertModel, BertPreTrainedModel,\n",
    "    AdamW, get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, AutoModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41485e01-0a23-41e5-9dbf-b69b6d3c1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f934f77-828f-451e-87be-c4de4b7c35b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문서는 브르타뉴 공국의 통치자 목록이다. 브르타뉴 군주들은 왕, 제후, 공작 등 시대 별로 달랐다. 브르타뉴 통치자들은 어쩔 때는 선출되기도 하고, 어쩔 때는 정복이나 계략, 상속권을 통해 자리에 올랐다. 세습 공작 중에는 때로는 여성 통치자가 있기도 했으며, 브르타뉴 여공작이라는 작위를 지녔다. 주요 도시들과 지역들은 브르타뉴 통치자들과 자주 분쟁을 벌이거나 브르타뉴 통치자가 된 백작들의 지배를 받았다.\\n\\n로마 제국이 쇠퇴해가던 시절에, 갈리아내 초기 브르타뉴인은 코르누아이와 돔노니아 등의 작은 왕국들의 왕이라고 자청했다. 이런 왕들 중에 일부는 아르모리카 반도내에 브리튼족들에 대한 헤게모니를 형성했을 것이고, 요르다네스에 대해 연대기 작가 요르다네스는 브리튼족의 왕이라 칭해다. 그럼에도 브르타뉴 전체에 대한 통치자는 없었으며, 브르타뉴는 지역 백작들의 영지들로 나누어졌다.\\n\\n브르타뉴 공국은 939년 트랑라포레 전투에서 기원을 했으며, 브르타뉴와 노르망디 간에 경계인 쿠에농강에 세워졌다. 942년, 알랑 2세는 루이 4세에게 충성을 서약했으나, 브르타뉴 공국은 루이 6세가 낭트 주교가 된 1123년까지 프랑스 왕가의 관심을 갖기 못했다. 다른 어떤 브르타뉴 공작들도 아르튀르 1세가 1202년에 필리프 2세를 주군이라 인정할 때까지 알랑 2세의 충성 맹세를 되풀이하지 않았다. \\n\\n브르타뉴 지역은 흔히 공국이라 불렸고, 브르타뉴의 통치자도 독립 주권을 지닌 공작으로서 여겨졌다. 하지만 한 역사적 관점은 12세기 중반 이전에 당시 프랑스 왕국은 브르타뉴를 더 이상 영지라고 보지 않았기에 브르타뉴 공작들을 프랑스 왕들이 백작이라 불렀다고도 본다.날짜=2018-10-30 1297년에 브르타뉴는 프랑스의 대귀족제에서 공국으로 승격되었다. 이런 태도는 샤를 8세, 그 다음에는 루이 12세가 브르타뉴와 상속을 위해 각각 결혼한 안 드 브르타뉴의 상속권에 접근했던 방식과는 모순된다.\n",
      "브르타뉴와 노르망디를 구분짓는 것은?\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk(\"../data/train_dataset/train\")\n",
    "sample_idx = np.random.choice(range(len(train_dataset)), 30)\n",
    "training_dataset = train_dataset[sample_idx]\n",
    "print(training_dataset['context'][0])\n",
    "print(training_dataset['question'][0])\n",
    "# sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6743c0c-76fc-451d-b63a-1dbb06ca50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEncoder(AutoModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaEncoder, self).__init__(config)\n",
    "\n",
    "        self.roberta = AutoModel(config)\n",
    "        self.init_weights()\n",
    "      \n",
    "    def forward(self, input_ids,  attention_mask=None): \n",
    "\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs[1]\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0261854e-2a5b-458a-8e03-4b5ddbb3217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "p_encoder = RobertaEncoder.from_pretrained(model_checkpoint)\n",
    "q_encoder = RobertaEncoder.from_pretrained(model_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    p_encoder.cuda()\n",
    "    q_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720f4765-e790-42e2-a7a6-3a6fd8b10996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0c9261-c342-4858-ae09-a4318103e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrieval:\n",
    "    def __init__(self, args, dataset, num_neg, tokenizer, p_encoder, q_encoder):\n",
    "\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "        self.num_neg = num_neg\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_encoder = p_encoder\n",
    "        self.q_encoder = q_encoder\n",
    "\n",
    "        self.prepare_in_batch_negative(num_neg=num_neg)\n",
    "        \n",
    "        # self.eval_dataset = eval_dataset\n",
    "        # self.prepare_eval_dataset\n",
    "\n",
    "    def prepare_in_batch_negative(self, dataset=None, num_neg=2, tokenizer=None):\n",
    "\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = self.tokenizer\n",
    "\n",
    "        # 1. In-Batch-Negative 만들기\n",
    "        # CORPUS를 np.array로 변환해줍니다.\n",
    "        corpus = np.array(list(set([example for example in dataset[\"context\"]])))\n",
    "        p_with_neg = []\n",
    "\n",
    "        for c in dataset[\"context\"]:\n",
    "            while True:\n",
    "                neg_idxs = np.random.randint(len(corpus), size=num_neg)\n",
    "\n",
    "                if not c in corpus[neg_idxs]:\n",
    "                    p_neg = corpus[neg_idxs]\n",
    "\n",
    "                    p_with_neg.append(c)\n",
    "                    p_with_neg.extend(p_neg)\n",
    "                    break\n",
    "\n",
    "        # 2. (Question, Passage) 데이터셋 만들어주기\n",
    "        q_seqs = tokenizer(dataset[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        p_seqs = tokenizer(p_with_neg, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        max_len = p_seqs[\"input_ids\"].size(-1)\n",
    "        p_seqs[\"input_ids\"] = p_seqs[\"input_ids\"].view(-1, num_neg+1, max_len)\n",
    "        p_seqs[\"attention_mask\"] = p_seqs[\"attention_mask\"].view(-1, num_neg+1, max_len)\n",
    "        # p_seqs[\"token_type_ids\"] = p_seqs[\"token_type_ids\"].view(-1, num_neg+1, max_len)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            p_seqs[\"input_ids\"], p_seqs[\"attention_mask\"], # p_seqs[\"token_type_ids\"], \n",
    "            q_seqs[\"input_ids\"], q_seqs[\"attention_mask\"], # q_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "        valid_seqs = tokenizer(\n",
    "            dataset[\"context\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        passage_dataset = TensorDataset(\n",
    "            valid_seqs[\"input_ids\"],\n",
    "            valid_seqs[\"attention_mask\"],\n",
    "            # valid_seqs[\"token_type_ids\"]\n",
    "        )\n",
    "        self.passage_dataloader = DataLoader(\n",
    "            passage_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, args=None):\n",
    "        if args is None:\n",
    "            args = self.args\n",
    "        batch_size = args.per_device_train_batch_size\n",
    "        print(args.device)\n",
    "\n",
    "        # Optimizer\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.p_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": args.weight_decay},\n",
    "            {\"params\": [p for n, p in self.q_encoder.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon\n",
    "        )\n",
    "        t_total = len(self.train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        # Start training!\n",
    "        global_step = 0\n",
    "\n",
    "        self.p_encoder.zero_grad()\n",
    "        self.q_encoder.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train_iterator = tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\")\n",
    "        # for _ in range(int(args.num_train_epochs)):\n",
    "        for _ in train_iterator:\n",
    "\n",
    "            with tqdm(self.train_dataloader, unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "\n",
    "                    p_encoder.train()\n",
    "                    q_encoder.train()\n",
    "            \n",
    "                    targets = torch.zeros(batch_size).long() # positive example은 전부 첫 번째에 위치하므로\n",
    "                    targets = targets.to(args.device)\n",
    "\n",
    "                    p_inputs = {\n",
    "                        \"input_ids\": batch[0].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        \"attention_mask\": batch[1].view(batch_size * (self.num_neg + 1), -1).to(args.device),\n",
    "                        # \"token_type_ids\": batch[2].view(batch_size * (self.num_neg + 1), -1).to(args.device)\n",
    "                    }\n",
    "            \n",
    "                    q_inputs = {\n",
    "                        \"input_ids\": batch[2].to(args.device),\n",
    "                        \"attention_mask\": batch[3].to(args.device),\n",
    "                        # \"token_type_ids\": batch[5].to(args.device)\n",
    "                    }\n",
    "\n",
    "                    # (batch_size*(num_neg+1), emb_dim)\n",
    "                    p_outputs = self.p_encoder(**p_inputs)[1]\n",
    "                    # (batch_size, emb_dim)\n",
    "                    q_outputs = self.q_encoder(**q_inputs)[1]\n",
    "\n",
    "                    # Calculate similarity score & loss\n",
    "                    # p_outputs = p_outputs.view(batch_size, -1, self.num_neg+1)\n",
    "                    p_outputs = torch.transpose(p_outputs.view(batch_size, self.num_neg+1, -1), 1, 2)\n",
    "                    q_outputs = q_outputs.view(batch_size, 1, -1)\n",
    "\n",
    "                    sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()  #(batch_size, num_neg + 1)\n",
    "                    sim_scores = sim_scores.view(batch_size, -1)\n",
    "                    sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "\n",
    "                    loss = F.nll_loss(sim_scores, targets)\n",
    "                    tepoch.set_postfix(loss=f\"{str(loss.item())}\")\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    self.p_encoder.zero_grad()\n",
    "                    self.q_encoder.zero_grad()\n",
    "\n",
    "                    global_step += 1\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    del p_inputs, q_inputs\n",
    "\n",
    "\n",
    "    def get_relevant_doc(self, query, k=1, args=None, p_encoder=None, q_encoder=None):\n",
    "    \n",
    "        if args is None:\n",
    "            args = self.args\n",
    "\n",
    "        if p_encoder is None:\n",
    "            p_encoder = self.p_encoder\n",
    "\n",
    "        if q_encoder is None:\n",
    "            q_encoder = self.q_encoder\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_encoder.eval()\n",
    "            q_encoder.eval()\n",
    "\n",
    "            q_seqs_val = self.tokenizer(\n",
    "                [query],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(args.device)\n",
    "            q_emb = q_encoder(**q_seqs_val)[1].to(\"cpu\")  # (num_query=1, emb_dim)\n",
    "\n",
    "            p_embs = []\n",
    "            for batch in tqdm(self.passage_dataloader):\n",
    "\n",
    "                batch = tuple(t.to(args.device) for t in batch)\n",
    "                p_inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    # \"token_type_ids\": batch[2]\n",
    "                }\n",
    "                p_emb = p_encoder(**p_inputs)[1].to(\"cpu\")\n",
    "                p_embs.append(p_emb)\n",
    "\n",
    "        # (num_passage, emb_dim)\n",
    "        p_embs = torch.stack(p_embs, dim=0).view(len(self.passage_dataloader.dataset), -1)\n",
    "\n",
    "        dot_prod_scores = torch.matmul(q_emb, torch.transpose(p_embs, 0, 1))\n",
    "        rank = torch.argsort(dot_prod_scores, dim=1, descending=True).squeeze()\n",
    "        # print(dot_prod_scores)\n",
    "        # print(rank)\n",
    "\n",
    "        return rank[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe7bf4d-b4bb-4ae3-aa01-bc7651aa698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"dense_retireval\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c653dfd1-8567-4c9a-b7fb-f4a61d062df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = DenseRetrieval(args=args, \n",
    "                           dataset=training_dataset,\n",
    "                           # eval_dataset=None, \n",
    "                           num_neg=2, \n",
    "                           tokenizer=tokenizer, \n",
    "                           p_encoder=p_encoder, \n",
    "                           q_encoder=q_encoder\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2f4ba0-b81f-4dc5-9d6e-06157eba1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f518dc1b74ad477b848d789549d7e7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf30d02ae49f4bd8bf0e65adb30d09d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4d6eaf9b16499099110be65944f94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3d784e91194e9db708564b4d6a1e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120329dfe9c345a7adf2b7ea45eea124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed007ab6893e4e84bc5d265af74c674c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a63eefeb474e02af0897eb835a68bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c526ec581c4a358257d9fbf8765aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab78b3b9e1c4f1bab74f5ee75be88bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4a8772e25145749ff9d8311d90bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3e6da3c3044b2a9558e87c27ddab1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d099ef3-848f-409d-baea-7020e7e292ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "러일 전쟁의 승전국은?\n",
      "전라남도 광주 출신이다. 출생 연도나 성장 과정에 대해서는 알려지지 않았다.\\n\\n러일 전쟁이 일본 제국의 승리로 끝나고 을사조약이 체결되자, 전국적으로 을사조약에 반대하는 을사의병이 일어났다. 임창모는 전남 화순군 능주 출신인 양회일과 함께 의병을 일으켰다. 1907년에 화순에서 공격을 당하여 포위되었고, 의병을 이끌던 양회일과 임창모가 모두 체포되었다. \\n\\n체포된 두 사람은 광주감옥에 갇혔다. 양회일은 단식투쟁을 하다가 곧 옥사하였고, 임창모는 신안군 지도읍 지도로 유배되었다. 이듬해인 1908년에 유배가 해제되어 풀렸났다. \\n\\n임창모는 유배에서 풀려나자마자 당시 세력을 떨치고 있던 전남 보성군의 안규홍 의병대에 합류하여 다시 의병 운동에 뛰어들었다. 1908년 6월 이후에는 임창모의 부대가 안규홍의 의병대에서 따로 분리되어 독립하였으며, 보성에 거점을 두고 활동하였다.\\n\\n1909년 10월에 호남 지역에서 군과 경찰이 투입되는 대대적인 토벌 작전이 펼쳐졌다. 임창모는 전남 해남군과 강진군, 영암군에 걸쳐 있는 흑석산에서 첫째아들 임학규와 함께 있다가 정체가 발각되었고, 병부대는 기습을 받아 전멸하고 임창모도 전사하였다.\\n\\n1963년에 건국훈장 독립장이 추서되었다.\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "print(training_dataset['question'][IDX])\n",
    "print(training_dataset['context'][IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2d5f35-d5a7-4e74-a5d9-3f4f97935278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "러일 전쟁의 승전국은?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8192415d5b74cc3aa70b9b8c8b04e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "이 밴드의 이전 음반인 1989년 《Mother's Milk》는 빌보드 200에 진입한 두 번째 음반이 되었으며, 이 음반은 52위에 그 당시 그들의 생애에서 가장 큰 음반이었다. 비록 음반은 다소 성공적이었지만, 제작은 프로듀서 마이클 베인혼에 의해 짓눌렸다. 그는 프루시안테에게 전체적으로 더 무거운 음색으로 연주하도록 설득했고, 앤서니 키디스에게 더 라디오가 가능한 가사를 쓰라고 지시하여 밴드가 창작적으로 제약을 느끼게 했다. \\n\\nEMI와 밴드의 계약이 끝나면서 그들은 또 다른 음반사를 찾기 시작했다. 이 그룹은 소니 BMG/에픽과 함께 EMI로부터 마지막 음반을 구입한다는 단서로 함께 가기로 합의했다. 레이블은 며칠밖에 걸리지 않을 것이라고 약속했지만, 그 과정은 수개월로 연장되었다 비록 소니/에픽과 거래가 이루어졌지만, 워너 브라더스 레코드의 모 오스틴은 키디스에게 전화를 걸어 성공적인 계약을 축하했고, 라이벌 음반사를 칭찬했다. 키디스는 당시 상황을 회상하며 \"우리가 이 모든 협상 중에 만났던 가장 멋지고, 가장 실제적인 인물은 제가 경쟁 회사에 훌륭한 기록을 세우도록 격려하기 위해 방금 개인적으로 전화를 했습니다. 그런 사람이면 내가 일하고 싶은 사람이었어요.\" 이 그룹은 이 아이디어를 추구했고, 결국 워너 브라더스와의 계약에 찬성하여 소니와의 계약을 포기했다. 오스틴은 EMI의 옛 친구에게 전화를 걸었는데, EMI는 즉시 레이블 이전을 허락했다\n",
      "통킹 사건\\n3월 28일, 브리에르 드 리즐이 파리에 보낸 비관적인 ‘랑선 전보’는 막대한 정치적 파장을 낳았다. 페리의 즉각적인 반응은 통킹에서 군대를 증원한다는 것이었다. 실제로 브리에르 드 리즐은 상황에 대한 그의 추정치를 빠르게 수정하고 정부에게 전선을 곧 안정화시킬 수 있다고 조언했다. 그러나 그의 두 번째 생각은 너무 늦었다. 그의 첫 전보가 파리에 공개되자, 하원에서는 소란이 일었다. 조르주 클레망소가 이끄는 정부의 반대파들이 공격을 시작했다. 불신임 동의안이 상정되었고, 페리 정부는 3월 30일 전례가 없는 소동 속에 붕괴되었다. “페리를 세느강에 던져라! 통킹 놈들을 죽이자!”며 군중들이 외쳤다. 페리는 통킹 사건으로부터 회복되지 못했다. 그는 다시는 수상이 되지 못했고, 나머지 생애 동안 그의 정치적 영향력은 심각하게 줄어들었다. 그의 후임인 샤를 드 프레이시네는 정전과 잠정적인 강화 조약을 협상하기 위해 빠르게 움직였다.\\n\\n청나라는 3월 31일에 프랑스 해군에게 펑후 해전에서 큰 타격을 입었고 (정작 프랑스에서는 별다른 관심을 받지 못했던 진전이었지만), 조선에 대한 경쟁적인 이해관계를 놓고 일본과의 전쟁 가능성을 경계했다. 따라서 청나라 정부는 이례적으로 합의를 성사시키고자 했다. 청나라 협상가들은 즉시 톈진 협약(통킹에 대한 프랑스 보호령을 인정)를 이행하기로 합의함)에 동의했으며, 프랑스 정부는 박레 매복에 대한 배상 요구를 철회했다. 적대행위를 종식시키는 평화협약이 4월 4일에 서명되었다. 그리고 6월 9일 이홍장와 프랑스의 쥘 파트노트르 장관이 실질적인 평화 조약에 서명했다.\n",
      "스태튼아일랜드(Staten Island, Staaten Eylandt|스타턴 에일란트)는 미국 뉴욕주의 뉴욕에 속하는 구역으로 도시의 5개 자치구 중 하나를 형성한다. 도시의 남서부에서 스태트아일랜드는 섬과 주의 남쪽 첨단에 컨퍼런스하우스 공원과 함께 도시와 주 양쪽의 가장 남부에 있다. 자치구는 아서힐과 킬밴컬에 의하여 뉴저지주로부터, 그리고 뉴욕만에 의하여 뉴욕의 나머지로부터 갈라졌다. 2015년 미국 인구조사국이 측정한 474,558명의 인구와 함께 스태튼아일랜드는 자치구들 중에 가장 적은 인구이나 58 스퀘어 마일에 지역에서 3번째로 가장 크다. 자치구는 리치먼드군과 공동 확장이며 1975년까지 리치먼드 자치구였다. 그 깃발은 후에 이것을 반사하는 데 바뀌었다. 스태튼아일랜드는 도시 정부에 의하여 무시를 당한 느낌이 드는 주민들에 의하여 어쩌다 \"잊혀진 자치구\"로 불려왔다.\\n\\n노스쇼어 - 특히 세인트조지, 톰킨스빌, 클리프턴과 스테이플턴의 이웃들은 섬의 가장 도시 지역이며 큰 빅토리아 왕조의 집들을 나타내는 데 지정된 세인트조지 역사 구역과 세인트폴스 애비뉴-스테이플턴하이츠 역사 구역을 품고 있다. 이스트쇼어는 세계에서 4번째로 가장 긴 2.5 마일 (4km)의 F. D. R. 브로드워크의 고향이다. 17세기 네덜란드인과 프랑스 위그노들의 정착의 현장 사우스쇼어는 1960년대 초창기와 1970년대에 빠르게 개발되었고, 현재 주로 교외 성격이다. 웨스트쇼어는 적은 인구를 가졌고 섬의 가장 산업적 부분이다.\\n\\n대부분의 교통은 베라자노 내로스 교를 통하여 브루클린으로부터, 그리고 아우터브리지 크로싱, 가설스 교와 베이욘 교를 통하여 뉴저지주로부터 자치구에 도달할 수 있다. 스태튼아일랜드는 MTA 버스 노선과 세인트조지에서 페리 터미널로부터 토턴빌로 달리는 MTA 빠른 대중 교통 노선인 스태튼아일랜드 철도를 가지고 있다. 스태튼아일랜드는 뉴욕 지하철 시스템에 연결되지 않은 단 하나의 자치구이다. 자유스러운 스태튼아일랜드 페리는 자치구를 맨해튼으로 연결하고, 자유의 여신상, 엘리스섬과 로어 맨해튼의 광경들을 마련하는 인기있는 관광객의 목적지이다.\\n\\n스태튼아일랜드는 911 테러 공격으로부터 파편을 받는 데 그해를 일시적으로 재개장 하였어도 2001년 폐문하기 전에 세계에서 가장 큰 매립이었던 프레쉬킬스 매립을 가졌다. 매립은 서식지 복원에 전념 한 지역 프레쉬킬스 공원으로 재개발되었고, 공원은 완공 후 뉴욕의 2번째로 가장 큰 공공 공원이 될 것이다.\\n\\n뉴욕의 자치구 중 유일하게 주민 중 백인이 다수이며, 그 중에서도 이탈리아계와 아일랜드계가 많다. 뉴욕의 한적한 교외 중산층 거주지로 알려져 있다.\n",
      "경리는 2012년 1월 11일 나인뮤지스의 싱글 〈News〉를 통해 가수로서 첫 음반을 발매하였다. 이후 다음 날인 1월 12일 Mnet 《엠카운트다운》을 통해 공식 데뷔하였다. 데뷔 초기에는 나인뮤지스 내 경리의 존재감은 다소 미비하였다. 그러나 2013년 나인뮤지스가 싱글 《DOLLS》를 통해 본격적인 인기를 얻은 해, 경리 역시 대중의 관심을 받기 시작했다. 특히 이 시기 경리는 퍼포먼스와 특유의 눈빛, LG 트윈스의 시구 등을 통해 알려지며 각종 예능 프로그램에 단독으로 출연하는 등 이름을 알리기 시작했다. 또한 싱글 《GLUE》부터 나인뮤지스의 팀 내 센터로서 많은 파트를 분배받고 퍼포먼스의 중심을 맡고 있다. 중간 영입 멤버로서 팀 내 중심으로 자리잡은 경우는 다소 이례적이다. \\n\\n그러나 2014년 개인 인지도와 팀 인지도를 모두 쌓아가는 중 멤버 세라, 이샘, 은지가 연속적으로 나인뮤지스를 탈퇴하자 경리는 개인 활동을 중심으로 연예 활동을 이어갔다. 또한 이 시기 2014년 9월 23일 경리는 제국의아이들의 케빈과 당시 연습생이던 동갑내기 소진과 함께 혼성 유닛 그룹인 네스티네스티를 결성하여 가수 활동을 시작했다. 2015년 경리는 약 2년간의 공백기를 깨고 나인뮤지스로서 다시 활동하였다. 특히 이 시기 과거 멤버 세라와 함께 맡았던 센터의 역할을 홀로 수행하며 나인뮤지스의 명실상부한 센터로 자리매김하였다. 이 시기 경리는 데뷔 후 처음으로 예능 프로그램의 고정 패널로 출연하였다. 또한 다수의 웹드라마, 웹예능 등에 출연하며 활동 범위를 넓혔다.\n",
      "1989년 8월 11일 정무원에서 발표한 민주조선에서 가내작업반, 부업반, 가내편의서비스업 관리운영규정이 발표가 되어 큰파장을 낳고 있으며 특히 가내작업반 관리운영규정을 세심히 살피자면 가내작업반을 사실상 사적기업설립권으로 인정을 하게 되었다.\\n\\n특히 가내작업반, 부업반, 가내편의서비스업 등은 각급 기관, 기업소, 협동단체와 노동자구, 동의 인민반에서 가정주부와 노인을 동원하여 조직한 노동조직으로 생필품 및 식료품의 생산과 판매에 종사한다고 되어 있으며 규정의 적용대상이 기관, 기업소, 협동단체와 노동자구, 동의 인민반과 같은 데에 조직된 가내작업반, 부업반, 가내편의서비스업, 협동조합식당, 가내축산반이라고 되어 있다.\\n\\n가내편의서비스업은 가내편의서비스관리소에 등록되어 하는 것을 기본으로 하며 관리소는 가내편의서비스업의 등록 및 영업 허가를 하며 가내편의서비스활동을 감독 및 통제한다.\\n\\n가내작업반과 부업반은 노동자, 사무원의 부양가족인 가정부인과 연로자들로 구성하며 규모는 3명 이상이 원칙이며 가내작업반, 부업반을 조직하거나 가내편의서비스직원으로 등록하려는 기관, 기업소, 협동단체와 공민은 소속에 관계없이, 읍, 노동자구, 동, 리 사무소의 합의를 받아야 한다.\\n\\n해당 도, 시, 군 행정경제위원회는 이것을 검토하고 승인한 다음 허가증을 내주고 해당 가내작업반, 부업반의 성원과 가내편의서비스직원에게 증명서를 발급하도록 되어 있다.\\n\\n이미 승인받은 가내편의서비스업종을 변경하려고 할 경우에는 도, 시, 군 행정경제위원회에 가내편의서비스업변경신청서를 내어 승인을 받아야 하며 가내편의서비스업과 같은 조직에서 중요한 것은 가내편의서비스업 성원이 적기에 등록하는 것이다.\\n\\n자원성에 의거하여 집에서 쉬고 있는 사람으로 조직된 경리형태로써 생산 및 과제수행에 대한 의무성이 법적 성격을 띠지 않으며 작업반원이 공동출자금으로 낸 출자금이 자체경영운영자금이고 국가예산 밖에서 생산활동이 진행되며 원료, 자재를 자체로 해결함으로 국가 계획에 의해 보장되지 않는다.\\n\\n작업반원이 공동출자금으로 낸 출자금이 자체경영운영자금이고 국가예산 밖에서 생산활동이 진행되며 원료, 자재를 자체로 해결함으로 국가 계획에 의해 보장되지 않는다고 밝혔다.\\n\\n가내편의서비스업장에는 가내편의서비스업종을 표시한 간판과 영업허가증을 붙이거나 걸어놓아야 하며 간판의 형식과 규격은 광고와 관련한 법규에 맞게 하여야 한다.\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset['question'][IDX])\n",
    "# print(retrieval.get_relevant_doc(training_dataset['question'][IDX]))\n",
    "retrieval_list = retrieval.get_relevant_doc(training_dataset['question'][IDX], k=5)\n",
    "for i in retrieval_list.tolist():\n",
    "    print(training_dataset['context'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141064bb-9631-4fad-9631-f341d2250742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거 에폭마다 eval 진행하게 함수 추가하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
